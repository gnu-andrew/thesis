\begin{abstract}
The free ride for software developers is over.  In the past, computer
programs have increased in performance simply by running on new
hardware with ever increasing clock speeds.  Now, however, this line
of development has reached its end and chip designers are producing
new processors, not with faster clocks, but with more \emph{cores}.
To take advantage of the speed increases offered by these new
products, applications need to be redesigned with parallel processing
firmly in mind.

The problem is that mainstream designs are still \emph{inherently
  sequential}.  Concurrency tends to be an afterthought that may be
useful to gain a performance boost, not an essential part of the
design process.  The current vogue for object-oriented designs tends
to also have the side-effect of making them heavily
\emph{data-oriented} which doesn't scale well; each shared element of
data has to be protected from simultaneous access, resulting in
operations becoming sequential again.  In addition, the usual method
for protecting data, using \emph{semaphores} or \emph{monitors}, tends
to be very error-prone.

In this thesis, we propose a new design method whereby applications
are constructed from small sequential \emph{tasks} connected by
intercommunication primitives.  This in itself is not new -- UNIX
pipelines, for example, followed this approach back in the 1970s --
but it has now become almost a \emph{necessity}; the only practical
and maintainable way to write large concurrent applications is to go
back to the design stage and completely rethink our ideas from this
perspective.  This is painstakingly clear when we consider the
alternative of debugging unreproducible race conditions and deadlocks.

What we envisage is a two-stage design process.  First, the individual
tasks are created as independent entities and tested with appropriate
inputs.  Secondly, the communication infrastructure between them is
developed.  We provide support for the latter via the DynamiTE
framework, which allows the interactions to be defined using the terms
of a process calculus.  Depending on the developer's background, they
can treat this as just another API, as a design pattern or as an
algebraic expression which can be property checked for issues such as
deadlocks.  Either way, the communication layer can be developed,
tested and evaluated separately from the tasks once it is known how
the tasks will interface with one another.

To supplement DynamiTE, we propose our own process calculus, Nomadic
Time, which contains a carefully chosen selection of constructs based
on our survey of the field contained within.  Among the features of
the calculus are the ability to perform communication both locally
(\emph{one-to-one}) and globally (\emph{one-to-many}), and the
flexibility to change the location of tasks during execution.
Security is paramount to the design of Nomadic Time and migratory
operations can be limited in two ways; by simple enumeration of
possibilities or by the optional typing of constructs to allow
restriction on a task-by-task basis.

While we can't eradicate all the problems inherent in designing
concurrent applications, we hope that DynamiTE can make things easier
by reducing the dependency on shared resources and enhancing the
reusability of concurrent components.

\end{abstract}
