\chapter{Introduction}
\label{introduction}

\section{Rationale}

Recent changes in the direction of computer hardware development have
created an impasse in the domain of software engineering.  Over the
past few years, new microprocessors have not seen the same increase in
clock speed that has prevailed over previous decades.  Instead, the
use of multiple `cores' has become common, due largely to physical
limitations which prevent the individual elements of a single
processor core becoming any smaller.  As a result, the performance
benefits of these new processors arise not from being able to execute
a single task faster than before, but from the parallel execution of
many such tasks.

However, this leads to a problem.  The existing dominant methods for
designing software systems are inherently sequential.  Current
imperative and object-oriented programming languages are still founded
on the principles of early computational models, such as the Turing
machine \cite{turing:36}.  These take a idealised view of events
whereby they always occur sequentially and in isolation.  Programs are
thus still effectively written as a sequence of reads and writes to a
form of memory.  The problem with this approach is that it runs into
major issues when the execution of other programs may cause changes to
memory outside the remit of the program.  Imagine Turing's model but
with multiple heads, each running separate programs yet still sharing
the same tape -- what happens if more than one head writes to the same
area of the tape?

In this thesis, we advocate a move towards systems where the focus is
on interaction between minimal sequential subsystems.  Rather than
building huge monolithic structures, the same result can be achieved
using a number of smaller components, running in parallel.  Such a
design has been suggested in varying forms over the years, but due to
the perceived future evolution of the microprocessor, this is now an
essential requirement, rather than a design ideal or optimisation.  We
also provide a formal grounding for such designs, based on academic
research which has been largely overlooked in the industrial sector.
Security also forms an inherent part of both the design and formal
model by allowing restrictions to be imposed on the communication
between individual components.

In the remainder of this chapter, we provide a brief overview of the
evolution of concurrent processing, highlighting current issues
arising from the flawed approach of maintaining a sequential design
which is becoming more and more distant from reality.  We also look at
how restricted mutability and an emphasis on intercommunication
between smaller, more specific processes can provide a better
solution, and how this approach has been adopted in the past with
varying results.  We close with a summary of the novelty of this work,
and an overview of how this will be covered in the later chapters of
this thesis.

\subsection{The Current Status Quo}

\subsubsection{Multiprogramming}

Concurrency is nothing new.  The concept of executing multiple
programs at once has been in use since \emph{multiprogramming} was
first introduced back in the 1960s.  But the same underlying model has
remained.  Parallelism is still seen as an optimisation, beholden to
the maintenance of the sequential standard.  Utilising concurrency
within a program remains relegated to study as an advanced feature,
seldom taught and even less well practiced.  If parallelism is to
become the dominant means of exploiting the power of future hardware,
this needs to change.

Multiprogramming was introduced as an efficiency measure.  At the
time, machines were available only on a per-institution rather than
per-user level, so a batch of \emph{jobs} were submitted to the
machine, each consisting of the program to run and any associated data
it needed to do so.  The machine ran a relatively simple
\emph{operating system}, which would take each job in turn and execute
a specified series of commands written in a batch job language.  Such
jobs would usually consist of reading in the program, compiling it if
necessary, and then executing it.  During execution, data was read in
and the results of computation output for the user to later digest.

It soon become clear that having an expensive processor sit idle while
input/output (I/O) operations took place was wasteful.  To solve this
problem, a new generation of machines were introduced which provided a
\emph{scheduler} as part of the operating system.  Instead of running
each job to completion before attempting the next, the system read in
multiple jobs to begin with, each forming a \emph{process} in memory.
These processes consisted not only of the code being executed, but
also included contextual information, such as the current instruction
being executed (the \emph{program counter}) and environmental data
(e.g. open file handles).

If a process being run by the system reached a point where it had to
wait for an I/O operation, the scheduler would move the process into a
\emph{blocked} state and perform a \emph{context switch} to begin the
execution of another.  Once the I/O operation was complete, the
blocked process would be reassigned to a \emph{ready} state, making it
again eligible for execution.

All this remained completely invisible to the running processes, each
of which appeared to be running in complete isolation.  The hardware
provided memory protection, which prevented a process from accessing
data outside its own memory space and they remained largely oblivious
to the fact that their execution was effectively being paused and then
resumed later.  The effect of such operation was only noticeable if
the running time of the process was recorded, as such results were now
dependent on factors such as system load and the arbitrary choices of
the scheduler.

Over time, schedulers have been extended so as to also switch when a
quantum of time allocated to a process has been depleted.  This
ensures a greater degree of fairness; a processor-intensive task which
rarely blocks can no longer become overly dominant.  This wasn't of
great importance for batch systems, as users submitted a job and then
collected the results later on.  In this context, just utilising the
time when a process was blocked had a significant impact on perceived
performance.  However, with a move towards first time sharing and then
personal computer systems, it became necessary to ensure that each
process was given time to execute on a regular basis, so the system
remained responsive.  This concept is referred to as
\emph{preemption}.

Finally, further performance enhancements were made possible by
allowing processes to have multiple threads of control and extending
the scheduler to enable switching between the individual threads
within a process.  The advantage of such threading is that the threads
share the same memory space and thus may interoperate more easily and
more efficiently.  The disadvantage is that it makes the possibility
of contention much more likely.

\subsubsection{Resource Contention}

Concurrency issues arise when a resource is accessed by multiple
processes or threads at the same time.  With threads, this is a
frequent occurrence as they run the same code and access the same
variables.  It also occurs with processes; although they have their
own memory space in which to operate, the resources provided by the
underlying operating system are shared by them all.  An obvious
example is the filesystem.  What happens if more than one process
tries to access a file at the same time?  Unless only reads occur, the
possibility of data corruption arises.

Such bugs, known as \emph{race conditions}, are difficult to reproduce
as they are heavily dependent on timing.  This is especially true of
single processor systems, where concurrency is merely simulated by the
scheduler switching between processes.  Whether or not file corruption
occurs depends on the choices made by the scheduler, which in turn
depends on a number of factors, such as system load.  If many
processes are competing for the processor, then there is less chance
of one which accesses the same file being picked.

A print spooler is a program which allows a printer (another shared
resource) to be used by many processes while maintaining separation
between individual jobs.  Without such a mechanism, one program may
write a few lines to the printer, and then be suspended by the
scheduler.  The program which is allowed to run next may then also
write to the printer, causing the user to end up with output from
different jobs mixed together.

Instead, the spooler tackles this concurrency problem by acting as an
mediator between the programs and the printer.  However, such an
application must be carefully designed to ensure it doesn't fall foul
to the same issue.  Imagine the spooler operates by reading a list of
files to print from a shared file.  When a process wants to add a new
job to the queue, it writes the filename as a new entry at the end of
the file:

\begin{verbatim}
int fd = open("/var/spool/print_jobs");
seek(fd, END_OF_FILE);
write(fd, "my_print_job");
close(fd);
\end{verbatim}

Problems arise because such an operation is \emph{non-atomic}; it is
possible that the process may be stopped by the scheduler while adding
a job to the list (e.g. after the \texttt{seek} function above), just
as it may be stopped while writing to the printer.  If this happens,
there is a possibility that whichever other process is scheduled in
its place could also choose to alter the queue.  The result of such a
collision depends on the timing:

\begin{enumerate}
\item If the first program only opened the file, or was just about to
  close it, then there will be no consequence.  In the first case, the
  first program will move to the end of the (now longer) file when it
  resumes and write its entry.  In the latter case, closing the file
  is just a matter of freeing resources and has no effect on the file
  itself.
\item If the first program seeked to the current end of the file, then
  on resumption, it will overwrite any data added in the meantime.  If
  the new data is longer than the older data, then the old data will
  simply be lost.  If it is shorter, the file will be corrupted.
\end{enumerate}

The solution to these sort of problems is to limit access to a
resource, so that a process is forced to wait its turn.

\subsubsubsection{Semaphores}

Such access limitations can be imposed by a \emph{semaphore}, a
solution first proposed by Dijkstra\cite{semaphore}.  A semaphore
maintains an integer count which is manipulated by two operations:
\emph{up} and \emph{down}.  The count can be used to limit the number
of threads of control active in a particular region.  In effect, this
is akin to the scenario where a gate requires a token in order to
allow someone (a thread) to pass through, but the number of such
tokens is limited.  When a thread wants to pass through the gate, it
attempts to acquire a token by executing the down operation.  If the
count maintained by the semaphore is greater than zero, then it will
be decremented and the thread can proceed through the gate.  However,
if it is zero, there are no tokens left so the thread is forced to
wait until one of the existing tokens is returned.  Tokens are
returned by executing the up operation.

The up and down operations must be atomic; it should not be possible
for such an operation to be interrupted.  If they can, then the whole
purpose of the semaphore is defeated; a further solution would be
needed to resolve the possible concurrency issues that may occur
inside the semaphore itself.  Most operating systems provide such
atomicity by using support available at the processor level.

An example of where semaphores are necessary is in implementing a
bounded buffer, which is fed by a number of producer threads and
emptied by a number of consumers.  A semaphore with a count of one can
be used to make modifications to the buffer appear atomic; a thread
wanting to operate on the buffer needs to first acquire the token and
will be unable to do so if another thread has already taken it.  Such
\emph{binary semaphores} are very common, and are available in a more
simplified form known as a \emph{mutex}.  Rather than maintaining a
count, a mutex is simply locked or unlocked.  Just as with a binary
semaphore, a thread first locks the mutex, does its work and then
unlocks the mutex to allow others access.

Such locks can also be implemented at the file level, and thus provide
a solution to the problem we encountered in the previous section:

\begin{verbatim}
int fd = open("/var/spool/print_jobs");
flock(fd, LOCK_EX);
seek(fd, END_OF_FILE);
write(fd, "my_print_job");
flock(fd, LOCK_UN);
\end{verbatim}

The first call to \texttt{flock} acquires an exclusive lock
(\texttt{LOCK\_EX}) on the file referenced by \texttt{fd} (the file
descriptor returned by the operation which opens the file).  Let's
assume that this process is stopped by the scheduler after the
\texttt{seek} function executes and another process is allowed to run.
This second process executes the same program.  While it can
successfully acquire a file descriptor for the file through the
\texttt{open} function, the \texttt{flock} function will block trying
to obtain an exclusive lock.  This is because the lock is still held
by the original process which has been descheduled but has not yet
relinquished the lock.  When the original process is chosen again by
the scheduler, it can continue to write to the file, safe in the
knowledge that no other process has altered its contents in the
interim.  The final call to \texttt{flock} releases the lock so the
second process may now proceed.

The advantage of semaphores is that they also have signalling
capabilities (waiting threads are awoken when an up occurs) and so, in
our buffer scenario, we can make further use of them to monitor the
state of the buffer.

  ensure a process has exclusive access to a resource.  This has the
  effect of making the action seemingly atomic; although the process
  can still be interrupted, no other process can interfere by
  beginning an equivalent transaction.  Instead, such a process is
  blocked until the original process relinquishes control.

The usual method for implementing exclusive access is via a lock of
some form.  The process of acquiring such a luck must itself be
atomic, so as to avoid the same kind of problems with acquiring the
lock that the addition of the lock itself was designed to avoid.  If
acquistion can not be interrupted, a lock will always be in one of two
known states: held by a process or unheld.  Our above example would be
modified as follows:


But there are inherent problems with locking itself.  A successful
strategy requires placing acquisition and release calls in all
affected locations and is extremely prone to error.  Suppose the first
process above never relinquishes the lock.  The second process (and
any future processes running the same program) will be blocked
indefinitely waiting for the lock.  Similarly, it takes only one
program accessing the file without first acquiring the lock to make
the whole process of locking redundant.

Locks don't scale well either.  The above example deals with only
acquiring one resource, but most programs will need to make use of
several.  In such a situation, ordering also becomes important.  If
locks are acquired in the wrong order, deadlock can occur.  Processes
deadlock when each process is trying to obtain a lock on a resource
held by the other process.  Take two processes, A and B, both of whom
are trying to acquire a lock on the two files, \texttt{/etc/passwd}
and \texttt{/etc/shadow} in order to add a new user to the system.  If
both processes acquire the locks in the same order, then all is well.
If they don't, a deadlock may occur.

Let's assume process A runs first.  It acquires a lock on
\texttt{/etc/passwd}.  At this point, A has used its allocated quantum
of CPU time and so is descheduled.  A context switch occurs and
process B begins to run.  If B begins by trying to acquire a lock on
\texttt{/etc/passwd}, then it will simply block as A already holds
this lock.  If, however, it tries to acquire a lock on
\texttt{/etc/shadow} first, this will succeed.  We then get stuck; B
blocks trying to acquire the lock on \texttt{/etc/passwd} held by A,
which will never be relinquished because A will be blocked trying to
acquire the lock on \texttt{/etc/shadow} held by B.  Such problems
occur simply through two program statements occurring in the wrong
order.

Such examples are frequent.  Take the classic readers and writers
problem.  Here, we again have a scenario with a single shared resource
(a database) and contention from multiple processes, which can result
in data corruption.  The simplest solution is to do the same as we did
above; have each reader or writer acquire a lock on the database when
it wants to use it and relinquish it afterwards.  The problem with
this approach is that it makes use of the database sequential.
Multiple processes in parallel gives us the possibility of a
substantial performance gain, but we sacrifice this to retain the same
design and behaviour we are used to when working with a single process
and no possibility of interference.

Things can be made a little better by exploiting the fact that
processes either read from the database (\emph{readers}) or write to
it (\emph{writers}).  It clearly doesn't matter if several readers are
accessing the database at once, but a writer needs exclusive access.
If two or more writers are altering the database at the same time,
corruption may result.  Similarly, if readers are trying to use the
database while it is being used, they may obtain inconsistent results.

This results in the following conditions for access to the database:
\begin{itemize}
\item For a reader to access the database, there must be no writers
  using the database.
\item For a writer to alter the database, there must be no readers or
  writers using the database.
\end{itemize}
One possible solution uses two controlled resources.  The first of
these is a count, which enumerates the number of readers currently
accessing the database.  Only when this count is zero may a writer
gain access to the database.  The second resource is simply a binary
flag which is used to indicate when a writer is active, making
modifications.  Both readers and writers may not access the database
if this flag is set.

Both resources must be modified atomically.  \emph{Semaphores} provide
an apt solution for this. TODO

Clearly, this process can quickly become complex and even more error
prone when multiple resources are involved.  One attempt at
simplifying this process is the use of \emph{monitors}.  TODO

Ideally, we need to limit the need for such locks altogether by
reducing the number of shared resources and the amount of mutability
inherent in our designs.  In the future, we want to be able to utilise
the advantages of massively parallel systems and this can only be
achieved by reducing the need for resource contention.

\subsubsection{Interprocess Communication}

To achieve this, we need to focus on more short-lived processes which
interact directly with each other, rather than via the means of shared
resources.  This is nothing new.  However, it has never achieved
universal usage as a design paradigm because having to deal with
concurrency issues, as outlined above, has been avoidable while the
majority of systems have remained based on a single thread of control.
This is no longer the case.

For a long time, UNIX systems have included the notion of pipelines
between processes.  For example, the command \texttt{du}, which
calculates disk usage, doesn't include an option to sort the results.
This is because there also exists a command, \texttt{sort} which can
order an arbitrary block of text in a number of ways.  As such, there
is no point adding duplicate functionality to \texttt{du} when its
output can just be fed in as input to \texttt{sort} for those who want
sorting.

A pipeline is created in the shell by separating the two programs with
a \texttt{|} symbol.  For our example, \texttt{du -h | sort -n} would
do the job of outputting disk usage in human-readable form
(\texttt{-h}) and then sorting it numerically (\texttt{-n}).  A simple
solution can be applied programatically using system calls such as
\texttt{pipe}, \texttt{fork} and \texttt{execve}.  The pipe allows the
output of one program (\texttt{du}) to become the input of another
(\texttt{sort}).  Neither of the individual programs needs to be aware
that this is happening.  As far as \texttt{du} is concerned, it is
still sending output on its standard output channel.  The difference
is that this channel has been changed externally so as to instead feed
into a pipe, the other end of which forms \texttt{sort}'s standard
input channel.

This is a very simple solution, yet it elegantly solves the problem of
sharing the data between the two processes.  If a pipe was not used,
\texttt{du} would have to store its results somewhere for \texttt{sort}
to access.  This could then result in contention between the two
processes for access to the resource.  Instead, here the two are
working together rather than against each other by synchronising the
passage of data between them.

A similar design tactic has been used in numerous areas.
\emph{Microkernels} such as Mach\cite{mach}, MINIX 3\cite{minix3} and
the GNU HURD\cite{hurd} also utilise this idea of synchronous
communication over a monolithic design based around shared resources.
In this context, it provides an essential stability and security
advantage; many services, such as drivers, file systems and network
protocols, can operate at the same level (or close to) as user-level
processes.

Some elements of the kernel must interact with the CPU at a very low
level.  This can only be achieved in the assembly language of the
processor and in a special \emph{protected} mode of operation.
However, it is not necessary for the entire kernel to function under
these conditions.  Device drivers are particularly notorious for
causing system instability by doing so.  Through running as part of a
monolithic kernel and assuming complete control, a buggy device driver
can cause the entire system to crash.

Instead, in MINIX 3, device drivers operate as separate privileged
processes.  Unlike normal user-level processes, they have the ability
to request direct access to hardware but such access is achieved
through message passing to the underlying core of the kernel.  The
majority of the driver's operation takes place in userspace and any
low-level access can be monitored and potentially prohibited.

The Mach and GNU HURD kernels (the former currently forming the basis
for the latter) take a similar approach.  The central mantra behind
this design is one of multiple servers, which provide different
functionality such as a file system or network service.  Apple also
adopted this design for XNU, the Mac OS X kernel, but, while basing it
on Mach, they largely reduced the design to a single server running a
monolithic BSD-based kernel.

The traditional objection against such designs has been performance.
Designs based on intercommunication have always tended to be more
elegant, but their usage has tended to be restricted to distributed
systems such as web services.  In these circumstances, any design
approach nesisitates utilising a potentially slow connection to
another system and having a central resource upon which all others
rely becomes disadvantageous, due to the potential for failure.

We believe it is time to reevaluate the benefits of systems focused on
intercommunication between specialised components.  With modern
systems, the potential performance disadvantage is becoming outweighed
by the benefits of a cleaner and more sustainable design.  With the
increasing prevalance of truly concurrent systems, more monolithic
systems will face a clear disadvantage, as the potential for
parallelism is severly reduced by contention for shared resources.

\section{Contributions to Knowledge}

Through this thesis, we present the following contributions to
knowledge which we believe to be novel:

\begin{enumerate}
\item The development of an algebraic process calculus (\ref{apc})
  with compositional global synchronisation (\ref{globsync}), mobility
  (\ref{mobility}) and security provision via the notion of `bouncers'
  -- see \ref{nt}.
\item The addition of a type system to the calculus which provides
  movement restriction via the group membership of processes -- see
  \ref{tnt}.
\item The addition of explicit data provision to the calculus via the
  use of typed channels -- see \ref{tnted}.
\item The realisation of the calculus as a design metholodogy through
  the implementation of its constructs as programmatic elements in the
  Java programming language -- see \ref{dynamite}.  This will allow
  the specification of system interactions to be shifted directly from
  the theoretical domain into an implementation backed by a formal
  methodology, helping in turn to improve industrial adoption of
  concurrent techniques.
\end{enumerate}

\section{Structure of the Thesis}

In the next chapter, we introduce existing research in to the area of
algrebriac process calculi through an exploration of the Calculus of
Concurrent Systems (CCS) \cite{milner:ccs}.  The following two
chapters focus on specific issues: global synchronisation
(\ref{globsync}) and mobility (\ref{mobility}).  In chapter \ref{nt},
we introduce our own research in the form of the Nomadic Time process
calculus.  The following chapter extends this with a type system to
create TNT (Typed Nomadic Time).  Chapter \ref{tnted} describes an
optional data extension to the calculus which allows channels to also
be typed.  The second part of our research is covered in chapter
\ref{dynamite} with the development of the DynamiTE (Dynamic Theory
Execution) framework.  We close with suggestions for future work in
chapter \ref{futurework}.
