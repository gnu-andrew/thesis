\chapter{Introduction}
\label{introduction}

\section{Rationale}

Recent changes in the direction of computer hardware development have
created an impasse in the domain of software engineering.  Over the
past few years, new microprocessors have not seen the same increase in
clock speed that has prevailed over previous decades.  Instead, the
use of multiple `cores' has become common, due largely to physical
limitations which prevent the individual elements of a single
processor core becoming any smaller.  As a result, the performance
benefits of these new processors arise not from being able to execute
a single task faster than before, but from the parallel execution of
many tasks.

However, this leads to a problem.  The existing dominant methods for
designing software systems are inherently sequential.  Current
imperative and object-oriented programming languages are still founded
on the principles of early computational models, such as the Turing
machine \cite{turing:36}.  These take a idealised view of events
whereby they always occur sequentially and in isolation.  Programs are
thus still effectively written as a sequence of reads and writes to a
form of memory.  The problem with this approach is that it runs into
major issues when the execution of other programs may cause changes to
memory outside the remit of the program.  Imagine Turing's model but
with multiple heads, each running separate programs yet still sharing
the same tape -- what happens if more than one head writes to the same
area of the tape?

In this thesis, we advocate a move towards systems where the focus is
on interaction between minimal sequential subsystems.  Rather than
building huge monolithic structures, development should result in a
number of smaller components, running in parallel.  Such a design has
been suggested in varying forms of the years, but due to the perceived
future evolution of the microprocessor, this is now an essential
requirement, rather than a design ideal or optimisation.  We also
provide a formal grounding for such designs, based on academic
research which has been largely overlooked in the industrial sector.
Security also forms an inherent part of both the design and formal
model, allowing restrictions to be established between individual
components.

In the remainder of this chapter, we provide a brief overview of the
evolution of concurrent processing, highlighting current issues
arising from the flawed approach of maintaining a sequential design
which is becoming more and more distant from reality.  We also look at
how restricted mutability and an emphasis on intercommunication better
smaller, more specific processes can provide a better solution, and
how this approach has been adopted in the past with varying results.
We close with a summary of the novelty of this work, and an overview
of how this will be covered in the later chapters of this thesis.

\subsection{The Current Status Quo}

\subsubsection{Multiprogramming}

Concurrency is nothing new.  The concept of executing multiple
programs at once has been in use since \emph{multiprogramming} was
first introduced back in the 1960s.  But the same underlying model has
remained.  Parallelism is still seen as an optimisation, beholden to
the maintenance of the sequential standard.  Utilising concurrency
within a program remains relegated to study as an advanced feature,
seldom taught and even less well practiced.  If parallelism is to
become the dominant means of exploiting the power of future hardware,
this needs to change.

Multiprogramming was introduced as an efficiency measure.  At the
time, machines were available only on a per-institution rather than
per-user level, so a batch of \emph{jobs} were submitted to the
machine, each consisting of the program to run and any associated data
it needed to do so.  The machine would take each job in turn and
execute a series of commands written in a batch job language.  Such
jobs would usually consist of reading in the program, compiling it if
necessary, and then executing it, which would probably involve reading
in some data and outputting the results.

It soon become clear that having an expensive processor sit idle while
I/O operations took place was a waste.  To solve this problem, a new
generation of machines were introduced which provided a
\emph{scheduler} as part of the operating system.  Instead of running
each job to completion before attempting the next, the system read in
multiple jobs to begin with, each forming a \emph{process} in memory.
These processes consisted not only of the code being executed, but
also included contextual information, such as the current instruction
being executed (the \emph{program counter}) and environmental data
(e.g. open file handles).

If a process being run by the system reached a point where it had to
wait for an I/O operation, the scheduler would move the process into a
\emph{blocked} state and begin the execution of another.  Once the I/O
operation was complete, the blocked process would be reassigned to a
\emph{ready} state, making it again eligible for execution.

All this remained completely invisible to the running processes, each
of which appeared to be running in complete isolation.  The hardware
provided memory protection, which prevented a process from accessing
data outside its own memory space and they remained largely oblivious
to the fact that their execution was effectively being paused and then
resumed later.  The effect of such operation was only noticeable if
the running time of the process was recorded, as such results were now
dependent on factors such as system load and the arbitrary choices of
the scheduler.

Over time, schedulers have been extended so as to also switch when a
quantum of time allocated to a process has been depleted.  This
ensures a greater degree of fairness; a processor-intensive task which
rarely blocks can no longer become overly dominant.  This wasn't of
great important for batch systems, as users submitted a job and then
collected the results later on.  In this context, just utilising the
time when a process was blocked had a significant impact on perceived
performance.  However, with a move towards first time sharing and then
personal computer systems, it became necessary to ensure that each
process was given time to execute on a regular basis, so the system
remained responsive.  This concept is referred to as \emph{preemption}.

\subsubsection{Interprocess Communication}

Concurrency issues arise on such systems as, although the processes
have their own memory space in which to operate, the functionality
provided by the underlying operating system is shared by them all.  An
obvious example is the filesystem.  What happens if more than one
process tries to access a file at the same time?  The result is
inconsistent data, either being read from the file or written to it,
and the reason for this is \emph{race conditions}.

Imagine that a print spooler operates by reading a list of files to
print from a shared file.  When a process wants to add a new job to
the queue, it writes its name as a new entry at the end of the file:

\begin{verbatim}
int fd = open("/var/spool/print_jobs");
seek(fd, END_OF_FILE);
write(fd, "my_print_job");
\end{verbatim}

Problems occur because such an operation is \emph{non-atomic}; it is
possible that the process may be stopped by the scheduler while adding
a job to the list (e.g. after the \texttt{seek} function above).  If
this happens, there is a possibility that whichever other process is
scheduled in its place could also choose to alter the queue.  The
result of such a collision is that one of the jobs will be scheduled
and the other will be lost, depending on which process writes to the
file last.

The solution to these sort of problems is to ensure a process has
exclusive access to a resource.  This has the effect of making the
action seemingly atomic; although the process can still be
interrupted, no other process can interfere by beginning an equivalent
transaction.  Instead, such a process is blocked until the original
process relinquishes control.

The usual method for implementing exclusive access is via a lock of
some form.  The process of acquiring such a luck must itself be
atomic, so as to avoid the same kind of problems with acquiring the
lock that the addition of the lock itself was designed to avoid.  Our
above example would be modified as follows:

\begin{verbatim}
int fd = open("/var/spool/print_jobs");
flock(fd, LOCK_EX);
seek(fd, END_OF_FILE);
write(fd, "my_print_job");
flock(fd, LOCK_UN);
\end{verbatim}

The first call to \texttt{flock} acquires an exclusive lock
(\texttt{LOCK_EX}) on the file referenced by \texttt{fd} (the file
descriptor returned by the operation which opens the file).  Let's
assume that this process is stopped by the scheduler after the
\texttt{seek} function executes and another process is allowed to run.
This second process executes the same program.  While it can
successfully acquire a file descriptor for the file through the
\texttt{open} function, the \texttt{flock} function will block trying
to obtain an exclusive lock.  This is because the lock is still held
by the original process which has been descheduled but has not yet
relinquished the lock.  When the original process is chosen again by
the scheduler, it can continue to write to the file, safe in the
knowledge that no other process has altered its contents in the
interim.  The final call to \texttt{flock} releases the lock so the
second process may now proceed.

But there are inherent problems with locking itself.  A successful
strategy requires placing acquisition and release calls in all
affected locations and is extremely prone to error.  Suppose the first
process above never relinquishes the lock.  The second process (and
any future processes running the same program) will be blocked
indefinitely waiting for the lock.

\section{Purpose of the Thesis}

\section{Contributions to Knowledge}

\section{Structure of the Thesis}

A standard feature of concurrent systems modelling is the use of
\emph{synchronisation} to model \emph{interaction}. Indeed, this is the
defining concept of CCS \cite{milner:ccs}, a process algebra commonly
used for modelling synchronous communication between two processes,
where one sends a signal and the other receives it at the same time (a
concept referred to as \emph{local synchronisation}).  However, it
cannot directly represent systems involving synchronisation of a sender
with an arbitrary number of recipient processes (known as \emph{global
synchronisation}) in a \emph{compositional} manner.  

For example, consider an environment inhabited by the members of a
factory team. The team receives a series of tasks to execute, which
require the team members to communicate with one another. At some point,
the first task is completed and the team moves on to the next. Modelling
this in a setting where the number of workers cannot be predicted
typically requires the use, either of \emph{global} synchronisation, or
else (which we reject) of an infinite set of defining equations.
Crucially, this means that the semantics of a broadcast agent cannot
suitably be represented using CCS.  If the agent is defined as
transmitting a signal to each of the recipients sequentially, through
multiple local synchronisations, then its semantics will become
non-compositional, because such behaviour depends upon the number of
recipients.  Each time a new recipient is introduced, or one of the
existing ones is removed, the semantics will have to be changed.

A solution to this deficiency lies in determining when all possible
synchronisations have taken place.  With this facility available, the
broadcast agent can recurse, transmitting signals, until this
condition holds. The family of abstract timed process calculi
(including TPL\cite{hennessy:tpl} and CaSE\cite{CaSE}) allow this by
extending CCS with \emph{abstract clocks}.  These don't represent real
time, with units such as minutes and seconds, but are instead used to
form synchronous cycles of internal actions followed by clock ticks.
A concept known as \emph{maximal progress} enforces the precedence of
internal actions over clock ticks, allowing the possible
synchronisations to be monitored.  When a synchronisation takes place,
it appears to the system as an internal action.  Thus, with maximal
progress, synchronisations prevent the clock from ticking, and as a
result, the occurrence of a clock tick also indicates that there are
no possible synchronisations.

However, the timed calculi mentioned above lack any notion of spatial
distribution or mobility. Thus, while they can adequately represent
large static systems, involving both local and global synchronisation,
they fail to model the ability of a system to change its topological
structure unilaterally. In contrast, the ambient calculus \cite{amb} and
its analogues are well positioned to model distributed systems (via
structures known as \emph{ambients}) in which resources can migrate from
one location to another. But, it suffers from similar deficiencies to
CCS when modelling global synchronisation.

This report presents the formal theory of the calculus of \emph{Typed
Nomadic Time} (TNT) \cite{hughes:nt}, which combines the abstract timed
calculus, CaSE, with notions of distribution and mobility from the
ambient calculus and its variants
(\cite{sangiorgi:mobsafeambients,controlledamb02}).  This allows the
creation of a compositional semantics for mobile component-based
systems, which utilise the notion of communication between arbitrary
numbers of processes within a mobile framework.  To extend the example
of a broadcast agent given above, this extension allow broadcasts to be
localised to a particular group of processes, which can change during
execution.  Current work on TNT is discussed in chapter
\ref{currentwork}, while chapter \ref{literaturereview} contains a
review of the existing literature in this area.  

Our long term aim is to leverage TNT as a foundation for creating
concurrent systems within a programmatic context. This will allow the
specification of system interactions to be shifted directly from the
theoretical domain into an implementation backed by a formal
methodology, helping in turn to improve industrial adoption of
concurrent techniques.  This is discussed along with future development
of the calculus in chapter \ref{futurework}.
