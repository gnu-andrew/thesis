% Thesis: DynamiTE
% Author: Andrew Hughes

\chapter{The DynamiTE Framework}
\label{dynamite}

\section{Introduction}

In this chapter, we introduce DynamiTE, the Dynamic Theory Execution
framework.  This provides the solution we first proposed in
\ref{solution}, using the Nomadic Time (NT) calculus introduced in
\ref{nt} and \ref{tnt} as a foundation for application development.
When using DynamiTE, programmers compose NT processes, realised as
Java objects, to create a working system.  The framework handles
running these processes, in parallel if necessary, and negotiates the
communication between them.  Both features are provided by leveraging
existing facilities in the underlying Java virtual machine and class
library.

Over the course of this chapter, we will describe how NT processes are
mapped onto Java objects (see \ref{dyn:maptheory}) and then show how
DynamiTE can be used to create an implementation of the application we
introduced in \ref{app:req} (see \ref{app:dynamite}).  But first, we
discuss why we chose Java as the host language for DynamiTE and what
advantages and disadvantages this brings to its implementation.

\section{Why Java?}

The first implementation of the Java programming language was released
in 1995 by Sun Microsystems.  It takes the form of a block structured
language with a syntax akin to C or C++.  However, unlike programs
written in those languages, Java applications tend to be compiled to
platform-independent Java bytecodes which are then executed by a Java
Virtual Machine or JVM.  This allows the same Java program to be
executed on multiple platforms without the need for recompilation.
With this new operating environment comes the removal of a number of
features found in Java's predecessors and the restriction of others,
with the aim of creating a safer and more portable language:

\begin{itemize}
\item \textbf{No pointer manipulation}.  All primitive types in Java
  (integer, floating point numbers, booleans and single characters)
  are passed by value.  All objects are stored and passed as pointers
  or references to their location in memory.  These pointers are
  immutable, removing the ability to perform pointer arithmetic
  (e.g. for iterating over arrays) and with it, a host of problems
  inherent with inappropriate memory access.  For example, attempts to
  use a \texttt{null} pointer are caught by the virtual machine and
  produce a checked exception, rather than causing a segmentation
  fault which brings down the entire process.
\item \textbf{All arrays are bounds checked}.  A major cause of errors
  and security issues in C and C++ programs is the possibility of
  buffer overflows, where programs write to memory beyond the end of
  an array.  In Java, such errors are prevented by the virtual
  machine; any attempt to access an index outside the bounds of an
  array causes a checked exception to be thrown and direct access to
  the array's memory is forbidden by the lack of pointer manipulation.
\item \textbf{All memory management is performed by a garbage
  collector}.  While allowing manual memory management allows the
  programmer greater control, it leads to an equivalent to the issue
  we saw with semaphores in \ref{semaphores}; every allocation must be
  paired with a later deallocation to avoid the possibility of an
  application leaking memory.  The problem is even more pronounced
  with regard to memory management as, while the acquisition and
  release of a lock tend to occur in close proximity to one another,
  allocation and deallocation can occur in quite disparate parts of
  the application.  In Java, memory is instead managed by a
  \emph{garbage collector} which allocates memory for objects as
  needed and periodically reclaims those that are no longer
  referenced.  The downside of this is that the garbage collector has
  to use processor time to perform its scans which would otherwise be
  used by the application.  However, as new garbage collection
  techniques, such as concurrent and generational collectors, become
  prevalent, this disadvantage is further outweighed by the prospect
  of chasing memory leaks.
\item \textbf{Lack of unsigned types}.  All integer types in Java use
  a bit to store the sign of the value, with no equivalent unsigned
  types that instead use this bit to store larger values.  This makes
  bitwise operations (and (\texttt{\&}), or(\texttt{|}) and
  not(\texttt{\~})) more inefficient as they need to operate on the
  type one size above (bytes ($2^8$) on shorts ($2^{16}$), shorts on
  ints ($2^{32}$), etc.).  Indeed, section 15.22.1 of the Java
  language specification \cite{javaspec} states that \emph{binary
    numeric promotion} (as defined in 5.6.2) should be applied to the
  operands, causing them to be converted to integer or long integer
  levels of precision before the operation is performed.  Thus, it
  logically follows that it is impossible to work with unsigned long
  integers ($2^64$) without resorting to the overhead of a class which
  implements arbitrary precision integers, such as
  \texttt{java.lang.BigInteger}.  Unsigned types continue to be
  proposed for addition to the Java language, but no such extension is
  scheduled for the next release (Java 7) in early 2010.
\end{itemize}

Although these changes are made at the expense of flexibility for the
programmer and possible efficency gains, they save time overall in
chasing bugs caused by memory allocation errors, buffer overflows or
leaks.  Besides, the Java Native Interface (JNI) can be used to
implement certain methods in C, should the need arise.  Many of the
methods provided by the Java class library do just that, usually to
make use of a platform-specific application programming interface
(API).  Doing so has some overhead and means losing the safety and
memory management benefits of Java, but is possible when a native
library needs to be used to facilitate reuse.

Performance has been a common criticism of Java, not just because of
these features but also because the Java bytecodes it uses must be
either interpreted or compiled into native code at run time.  This is
much less of an issue than it once was, due to advances in virtual
machine design and Just-In-Time (JIT) compilation techniques.
Theoretically, JIT compilation should eventually exceed the
performance of code compiled Ahead-Of-Time (AOT) as it can take
advantage of information only available at runtime.  This includes
knowing the exact platform on which the code will execute and being
able to make better optimisations based on statistics gathered through
execution (e.g. better branch prediction).  For example, HotSpot, the
virtual machine used by Sun's implementation of Java, only uses the
JIT compiler to create native code when it believes the code is used
enough (`hot' enough) to make doing so worthwhile.

Of these changes, the absence of unsigned types is the only one that
seems to have no advantage, other than simplifing the language.  Many
file formats and network protocols include unsigned types, so working
with them in Java becomes harder than is necessary.  Although their
absence may have made sense in earlier versions of the language, the
complexity of understanding unsigned arithmetic now seems trivial when
compared with the existential type system and its lack of reification
which was introduced by the addition of `\emph{generics}' in Java 5.
We thus hope that they may make an appearance in Java 8.

\subsection{Concurrency Provision}
\label{java:concurrency}

From the perspective of implementing DynamiTE, one advantage of Java
is its broad support for concurrency.  Java is one of the few
languages to have an implementation of \emph{monitors}, a feature we
demonstrated in \ref{semaphores}.  It has also had support for threads
from the very beginning, with support as a core part of the virtual
machine rather than as an auxillary library (the approach used for C).
Java's platform independence means that the same threading constructs
and semantics, as mandated by the VM specification\cite{vmspec}, can
be used across all operating systems supported by a Java virtual
machine.  The actual implementation is provided by the virtual machine
and class library, which may either map them on to native threads or
provide \emph{green} threads, where the virtual machine itself creates
and schedules threads.  The main disadvantage of the latter is that
blocking calls to the operating system performed by one thread will
cause the virtual machine and all its threads to be blocked; as the
system is unaware of the presence of the threads, its only option is
to block the entire VM process.  Green threads are however much faster
to create and synchronise, as everything takes place within the VM.
They can also match the required thread semantics exactly, rather than
having to map those provided by the operating system's threads.  While
earlier versions of Sun's implementation used green threads, native
threads are now used on all supported platforms.

The result of this early adoption of multithreading is that its
implementation in Java is reasonably mature and well-tested.  With
Java 5, this support was greatly expanded as a result of research led
by Doug Lea and incorporated into the Java platform via JSR166 and the
\texttt{java.util.concurrent} packages \cite{concpractice,jsr166}.

The extensions provided by JSR166 take the form of a host of new
classes, backed by support in the virtual machine.  The Java language
itself is not altered.  It provides support for:

\begin{itemize}
\item \textbf{Atomic Variables}.  These provide replacements for
  integer, long integer and reference fields which can be updated in
  an atomic fashion, which are safer than \texttt{volatile} variables
  and more efficient than locking.  The Java memory model allows
  operations which alter the value of normal fields to be reordered by
  the VM as a form of optimisation, as long as this reordering is not
  visible from within the same thread.  However, this means that other
  threads may see the changes in the wrong order or not at all.
  Marking a field as \texttt{volatile} makes the VM aware that it may
  be accessed by multiple threads, causing updates to be made visible
  to all threads immediately.  However, \texttt{volatile} fields are
  still prone to race conditions when used in non-atomic operations
  such as incrementing a value or performing a conditional
  update\footnote{e.g. in \texttt{if (x == 4) x = 5}, it is possible
    for \texttt{x}'s value to have been changed by another thread
    before the assignment but after the comparison}.  The usual
  solution is to obtain a lock on the class every time the variable is
  altered; this provides both the update guarantees of a
  \texttt{volatile} variable and blocks other threads trying to obtain
  the same lock.  Atomic variables provide an alternate solution by
  allowing the processor's CAS operation (see \ref{semaphores}) to be
  used.  This is usually more efficient than locking the entire class,
  which will involve the VM performing a CAS operation on the lock at
  some point anyway.  While locking takes a pessimistic approach to
  thread safety by blocking all other threads, CAS operations are
  optimistic; the update is attempted, and if it fails, we try again
  until it succeeds.  Implementing such a check successfully is even
  more prone to error than locking, as the programmer has to ensure
  they check the result of the CAS and loop accordingly, but it is
  usually much more efficient when contention is low.  With the
  addition of atomic variables to Java, programmers now have the
  choice of using either.
  \item \textbf{Explicit Locks}.  As we saw in \ref{semaphores}, Java
    has implicit reentrant locking via the \texttt{synchronised}
    keyword.  Their use, however, is limited; there is only one lock
    per class, so all its variables must be protected by the same
    lock, and threads are always blocked until they either acquire the
    lock or the thread is interrupted by \texttt{Thread.interrupt()}.
    The \texttt{ReentrantLock} class provides a more advanced version
    with the following additional features:
    \begin{itemize}
      \item \texttt{tryLock()} can be called to perform a non-blocking
        acquisition of the lock.  It immediately returns with
        \texttt{true} if the lock was acquired, and \texttt{false} if
        it wasn't.
      \item \texttt{tryLock(long, TimeUnit)} can be called to perform
        a timed acquisition.  If the lock is available, it acquires it
        and returns immediately.  Otherwise, it blocks.  However,
        unlike the implicit lock provision and the \texttt{lock()}
        method, it will become unblocked after the given timeout and
        return \texttt{false}.
        \item The lock can operate in a fair mode, where threads
          acquire the lock in the order they requested it.  Both
          implicit and explicit locks default to unfair behaviour,
          which permits \emph{barging} (jumping the queue) if a new
          thread happens to request a lock when it is unheld.  Unfair
          locks are much faster\footnote{If threads are not allowed to
            jump the queue, then we end up blocking and descheduling a
            thread which could have quite happily acquired the lock
            but isn't allowed to do so because of the fairness
            policy}, but fairness is sometimes needed to ensure
          correctness.
    \end{itemize}
    A class can have multiple instances of an explicit lock, just like
    any other variable, and this benefit is utilised by
    \texttt{ReentrantReadWriteLock}.  This class provides both a
    shared (\textbf{read}) lock and an exclusive (\textbf{write})
    lock.  Multiple threads can acquire the read lock, but to acquire
    the write lock, both locks must be unheld.  This can be used to
    make classes more efficient when compared with the brute force
    approach of enforcing mutual exclusion for all operations.  For
    example, a collection class can allow multiple threads to read
    values as long as there is no thread altering the collection.
    Locks, and other implementations such as \texttt{Semaphore}, are
    based on \texttt{AbstractQueuedSynchronizer}\cite{aqs} which
    provides a common framework thread queues.
    \item \textbf{Explicit Condition Queues}.  As in the case of
      locks, Java already has its own implicit condition queues,
      accessible via the \texttt{wait}, \texttt{notify} and
      \texttt{notifyAll} methods.  These also have similar limitations
      to the implicit locks; only one queue is available per class and
      either one or all threads must be notified.  With only one
      condition queue, the usability of \texttt{notify} to alert a
      single thread is extremely limited; using it is dangerous if
      there is more than one condition as the wrong thread may be
      awoken, and it is inefficient unless a change in the condition
      means that one and only one thread may proceed.  The former can
      be observed in the buffer example of \ref{semaphores} where
      there are two conditions: \texttt{used == BUFFER\_SIZE} and
      \texttt{used == 0}. The latter is observable in a `gate'
      scenario where multiple threads queue up waiting for a condition
      to hold, and then all proceed when it does.  Explicit condition
      queues address these issues by allowing a class to have multiple
      condition queues.  Each \texttt{Condition} is obtained from a
      \texttt{Lock} by a call to \texttt{Lock.newCondition} and that
      same lock must be held when calling its methods.  In the buffer
      example, the synchronized blocks would be replaced by the use of
      explicit locks and the calls to \texttt{wait} and
      \texttt{notifyAll} by \texttt{await} and \texttt{signal} calls
      on one of two \texttt{Condition} objects.  The \texttt{signal}
      method can now be used rather than \texttt{signalAll}, awakening
      just one thread, as we know the thread will be waiting for the
      condition whose state has changed and no other.  This avoids
      waking all threads and having all but one go back to sleep.
    \item \textbf{Executors and Thread Pools}.  The new classes
      provide a framework for executing tasks in the form of the
      \texttt{Executor} interface.  This decouples the process of
      submitting a task from how it is executed.  Tasks (in the form
      of an object which implements the \texttt{Runnable} or
      \texttt{Callable} interface) are submitted to an
      \texttt{Executor} instance, and then performed in a manner
      determined by the \texttt{Executor} implementation.  The
      \texttt{Executors} class provides a number of pre-defined
      instances:
      \begin{itemize}
        \item A single thread executor, which performs tasks
          sequentially.
        \item An executor with a fixed size pool of threads.
        \item An executor with an unbounded pool that grows and
          shrinks as demand allows.
        \item An executor with a fixed size pool of threads and
          delayed or periodic task execution.
      \end{itemize}
      The programmer is also, of course, free to define their own
      implementation.  This feature is very useful for implementing
      parallel composition in DynamiTE as each process may be
      submitted to an executor, the choice of which is left up to the
      user of the framework.
    \item \textbf{New Collections}.  The standard Java collections
      apply an all-or-nothing approach to thread safety; either the
      instance is unsafe for multithreaded use (as with instances of
      the Java 1.2 classes -- \texttt{HashMap}, \texttt{ArrayList},
      etc.) or every method call locks the class (as with the legacy
      classes such as \texttt{Vector} and \texttt{Hashtable} or the
      1.2 classes when wrapped by the \texttt{synchronizedX} methods
      in \texttt{Collections}).  The JSR166 extensions provide a new
      set of collections which utilise the features listed above.  For
      example, \texttt{ConcurrentHashMap} provides a hash map which
      utilises \emph{lock striping}; the map is protected by multiple
      read and write locks which protect only a segment of the whole
      map each.  Thus, not only can multiple readers access the map
      concurrently, but it may be possible to perform multiple writes
      concurrently if they effect different areas of the map.  The new
      collections also include various \texttt{BlockingQueue}
      implementations, which implement the producer-consumer model we
      demonstrated with the buffer example in \ref{semaphores}.  One
      such implementation is \texttt{SynchronousQueue} which closely
      matches the semantics of synchronous channels in Nomadic Time;
      it has no storage so a thread performing a \texttt{put} blocks
      until a receiving thread calls \texttt{take}.
\end{itemize}

With these additions, the programmer is given a lot of control and
flexibility when implementing concurrent programs in Java, and we will
leverage many of these features when implementing DynamiTE.  Having
essential components such as locks and concurrent collections already
available and well tested makes it much easier to meet the
requirements of the framework.

Other languages are not so lucky.  In C and C++, threads are provided
by an operating system library and thus vary depending on platform.
The POSIX standard for threads attempts to overcome this by providing
a standard threading interface and semantics for POSIX systems.  While
POSIX-based systems including GNU/Linux, Solaris, FreeBSD and Mac OS X
all provide implementations. the problem remains with systems that do
not provide such by default, notably Microsoft Windows.

Haskell has been slow to introduce threading support.  Although the
Concurrent Haskell \cite{conchaskell} extension was originally
proposed in 1996, it does not form part of the Haskell 98 standard and
the GHC documentation still lists it as experimental.  Both Hugs and
the Glasgow Haskell Compiler (GHC), the two main implementations of
Haskell, provide an implementation of Concurrent Haskell's
\texttt{Control.Concurrent} module, they do so using green threads.
As mentioned above, while these are faster than native threads,
blocking calls to the operating system, such as I/O, will cause all
threads to be blocked.  A workaround is provided in GHC when it is
built with the \texttt{-threaded} option; it uses a pool of worker
threads to execute Haskell code and switches to a new one when a
\texttt{safe} foreign call is made.  It also allows native threads via
\texttt{forkOS} when built in this manner.  As with C, this makes
Haskell's thread behaviour dependent on the underlying system as
opposed to providing a standard set of operations and semantics;
whether threads are provided and how well they perform depends
entirely on which implementation of Haskell is being used.

However, functional languages in general should be a good basis for
concurrency.  They already operate in a task-oriented manner through
\emph{pure functions}; data is fed in, manipulated as desired and the
result output without altering memory.  Those that do alter memory,
and thus could lead to concurrency issues, are clearly denoted
(e.g. by monads in Haskell), reducing the amount of code that has to
be checked for race conditions.

It is thus a pity that they are not more widely used and their
concurrency facilities not more well developed.  This is changing,
however.  GHC has recently been extended with support for Software
Transactional Memory (STM) \cite{haskellstm}, which provides a new
\texttt{atomic} function and \texttt{STM} monad for implementing
transactions.  The STM logs all actions and then performs a single
atomic commit, provided there are no conflicts with other updates.
This allows Haskell programmers to compose new atomic transactions
from others, and moves the need to ensure atomicity away from each
individual function to the caller, who can only invoke them from
within an atomic environment.

Erlang \cite{erlang} is another interesting case, as both it and
DynamiTE focus on message passing between processes as opposed to
shared data and locking.  Erlang differs in that it operates
asynchronously, collecting messages in a mailbox on a per-process
basis and filtering which ones are received in any one operation.
However, synchronous delivery can be implemented by requiring messages
to be acknowledged.  The main limitation of current Erlang
implementations is that they use green processes; unlike green
threads, these don't share state but they do have the same downside
that a blocking system call from one will cause them all to become
blocked by the system.

Both Erlang and Haskell provide an interesting environment in which to
implement a framework like DynamiTE.  Indeed, we hope that the
majority of the design explained here in \ref{dyn:maptheory} can be
applied to most languages with sufficient threading support.  However,
there is another reason for our choice of Java as the initial
prototype language.

\subsection{Popularity}

Popularity is rarely a good reason to do anything but, in combatting
developer inertia, it is a good weapon to have.  The simple fact is
that most of today's developers know Java and sometimes little else;
it (or its close relative, C\#) is taught as part of most computer
science degrees and is used as the language of choice for many
applications, especially in the area of enterprise web applications.

As we discussed in \ref{solution}, easing the barriers for adoption is
an essential aspect in influencing developers to try something new.
With DynamiTE, we are already advocating the idea of using message
passing rather than state manipulation to Java developers, a body of
programmers who will generally be more familiar with object-oriented
design techniques which focus on manipulating data.  Adding the
prospect of learning an entirely new language is not going to help our
case, and we believe this to be the main reason other solutions have
not moved far beyond their academic roots.  Instead, DynamiTE is
developed as a Java class library like any other, which leverages
standard features of the Java platform and which can be further
developed by the very people that use it.

We will be the first to admit that Java has issues; its age means that
with hindsight many design decisions can now be seen as flawed and
attempting to change this leads us to consider the bane of all
programming languages -- backwards compatibility.  Most features, good
or bad, are now enshrined in the language and further development
rightly takes a conservative attitude to avoid breaking the huge body
of existing code already in use.  This means that APIs are deprecated
rather than removed, causing the class library to become more bloated
than ever, and new language features such as generics take years to
appear and even then have to be limited.  No consensus has yet been
reached on how closures should be implemented, so they will not appear
in Java 7 either.  These issues are here to stay; Java is unlikely to
ever have a type system as advanced as that of most functional
languages or a separation between pure and impure functions.  But with
these come maturity and a vast body of developers which we believe to
be far more useful in achieving our goal than the possibilities of a
perfect but niche language.  Java is also the language with which we
are most familiar, and thus it makes sense to experiment first using
Java and then turn to other languages.

\section{Mapping Theory to Practicality}
\label{dyn:maptheory}

In this section, we show how the syntactic constructs of NT introduced
in chapter \ref{nt} are mapped on to Java classes by the DynamiTE
framework.  Within DynamiTE, developers can create concurrent
applications simply by implementing the specific behaviour they
require in appropriate subclasses.  Recall the syntax of NT from
\ref{eqn:nt:syntax}:

\begin{equation}
  \begin{aligned}
    \expr, \exprb \quad \mathrel{::=} \quad &
      \nil  \mid
      \Omega \mid
      \Delta \mid
      \Delta_{\sigma} \mid
      \alpha . \expr  \mid
      \expr + \exprb \mid
      \expr \mathrel{\!|\!} \exprb \mid
      \timeout{\expr}{\sigma}{\exprb} \mid \\
    & \stimeout{\expr}{\sigma}{\exprb} \mid 
      \mu X . \expr \mid
      X \mid 
      \expr \res{A} \mid
      \locv{m}{\expr}{\exprb}{\vec{\sigma}} \mid
      \ambop . \expr \\
   \ambop \quad \mathrel{::=} \quad & \tntin{m} \mid \tntout{m} \mid \tntopen{m} \mid
      \procin{\beta}{m} \mid \procout{\beta}{m} \mid \bin \mid
      \bout \mid \bopen
   \end{aligned}
\end{equation}

Each process term ($\expr, \exprb$) above becomes a class that implements \texttt{Process}:

\begin{verbatim}
public interface Process
  extends State
{
  Set<Transition> getPossibleTransitions();
  Process substitute(String var, Process proc);
}
\end{verbatim}

Operation follows a top-down approach; the complete system is
represented by a single instance of one of these classes which, in
most cases, will be an operator that composes together further
instances as appropriate.

The \texttt{Process} interface itself extends the marker interface,
\texttt{State}.  This, along with \texttt{Transition}, forms part of
our implementation of a labelled transition system, found under the
\texttt{lts} subpackage.  By making implementations of
\texttt{Process} also implement \texttt{State}, they can be used as
the start and finish state in the transitions represented by the
\texttt{Transition} class.  The label used by the transition is
provided by a subclass of \texttt{Action}, which also allows for the
possibility of side effects, which we cover in \ref{dyn:evolvers}.

Each \texttt{Process} is required to implement
\texttt{getPossibleTransitions()} and it is in this method that the
operational semantics found in tables \ref{tab:core} and
\ref{tab:mobsubset} are realised in Java code.  The simplest
implementation is found in the representation of $\Delta$, realised as
the class \texttt{Delta}, as it has no transitions.

\begin{verbatim}
public Set<Transition> getPossibleTransitions()
{
  return Collections.emptySet();
}
\end{verbatim}

The other method in \texttt{Process},
\texttt{substitute(String,Process)} is primarily used to implement
recursion.  The arguments passed to \texttt{substitute} are the
variable name and the process bound to that name respectively, and the
implementation is expected to return the same process with this
substitution applied.

We implement substitution in this manner so that it is independent of
the syntax of the calculus.  Nomadic Time is implemented in DynamiTE
by deriving from classes which implement CaSE, which in turn derive
from those implementing CCS.  Of these, only CCS has an implementation
of recursion:

\begin{verbatim}
public Set<Transition> getPossibleTransitions()
{
  Set<Transition> trans = new HashSet<Transition>();
  for (Transition t : proc.getPossibleTransitions())
  {
    Process end = (Process) t.getFinish();
    trans.add(new Transition(this,
                             end.substitute(var, this),
                             t.getAction()));
  }
  return trans;
}
\end{verbatim}

\noindent as the same rule is used in all three calculi.  Because the
new constructs in CaSE and Nomadic Time all provide an implementation
of \texttt{substitute}, the call to \texttt{substitute} in the CCS
implementation of recursion (provided by a class called \texttt{Rec}
with variables \texttt{proc} and \texttt{var}) will still work, even
when one of these forms the final state, \texttt{end}.

The above implementation of recursion highlights a common pattern in
the semantics, which is visible both in their formal representation
and the Java version; the rules reference one or more component
processes, and create new transitions based on the transitions of
these processes.  As a result, most of the implementations of
\texttt{getPossibleTransitions()} in DynamiTE operate by looping over
the set of transitions from each component process, checking if they
meet the prerequisties for one of the rules and then creating new
transitions\footnote{It is possible to make the implementations shown
  here more efficient, firstly by retrieving the transitions of the
  subprocesses simultaneously using separate threads and secondly by
  caching the result so that future calls don't recompute the
  transitions.}.

Recursion is probably one of the simplest examples of this.  From its
semantics,
\begin{center}
      \Rule{Rec}
      {E \derives{\gamma} E'}
      {\mu X.E \derives{\gamma} E' \{ \mu X.E / X\}}
      {}
\end{center}
\noindent we can see that it applies its transformation to all
transitions ($\gamma$ ranges over all possible labels), and the only
change it makes is to apply substitution to $E'$, represented in Java
as the final state of the \texttt{Transition} object
(\texttt{t.getFinish()}).  Thus, all the new transitions returned
perform the same action (\texttt{t.getAction()}), have the current
instance of \texttt{Rec} as the start state and a final state derived
from the original via substitution.

Implementing the $\mid$ operator, via the \texttt{Par} class, is a
more involved task.  In CCS alone, $\mid$ features in three of its
operational rules: $Par1$, $Par2$ and $Par3$ (see table
\ref{tab:ccssemantics}).  CaSE adds a further rule, $Par4$, to deal
with the passage of time over the operator (see table
\ref{tab:casesemantics}).  With Nomadic Time, the first two rules are
combined due to structural congruence\footnote{This makes no
  difference to the implementation; it merely cuts down on the number
  of rules that need to be listed in the semantics.  The case of $F
  \derives{\alpha}$, which is missing in the rules for Nomadic Time,
  is handled by a combination of $StrPar1$ ($E \pc F \equiv F \pc E$)
  and $SCong$}, but a further five are introduced ($InEnv$, $OutEnv$,
$Open$, $ProcIn$ and $ProcOut$) to handle mobility.  All of these are
handled in much the same way as $Par3$ (inspect the composed processes
and their transitions, and apply as required) so we will just look at
the CCS implementation here for brevity:

\begin{verbatim}
public Set<Transition> getPossibleTransitions()
{
  Set<Transition> trans = new HashSet<Transition>();
  // Par1
  for (Transition t : left.getPossibleTransitions())
  {
    Process nextLeft = (Process) t.getFinish();
    trans.add(new Transition(this,
                             new Par(nextLeft, right),
                             t.getAction()));
  }
  // Par2
  for (Transition t : right.getPossibleTransitions())
  {
    Process nextRight = (Process) t.getFinish();
    trans.add(new Transition(this,
                             new Par(left, nextRight),
                             t.getAction()));
  }
  // Now find pairs for synchronisation (Par3)
  Set<Transition> syncTrans = new HashSet<Transition>();
  for (Transition t : trans)
  {
    String label = t.getAction().getLabel().getText();
    if (Context.getContext().isRegisteredName(label))
    {
      for (Transition t2 : trans)
      {
        String label2 = t2.getAction().getLabel().getText();
        if (Context.isConame(label2) &&
            label.equals(Context.convertLabelToName(label2)))
        {
          Par finish1 = (Par) t.getFinish();
          Par finish2 = (Par) t2.getFinish();
          Action sync = new Sync(t, t2);
          if (!finish1.left.equals(left) &&
              !finish2.right.equals(right))
            syncTrans.add(new Transition(this,
                          new Par(finish1.left, finish2.right),
                          sync));
          else if (!finish1.right.equals(right) &&
                   !finish2.left.equals(left))
            syncTrans.add(new Transition(this,
                          new Par(finish2.left, finish1.right),
                          sync));
        }
      }
    }
  }
  trans.addAll(syncTrans);
  return trans;
}
\end{verbatim}

The class \texttt{Par} maintains references to the two composed
processes as \texttt{left} and \texttt{right}.  Thus, $Par1$ and
$Par2$ are implemented by iterating over the transitions of these
processes, as with recursion.  For each original transition, each
iteration produces a new transition, with the \texttt{Par} instance as
the start state, the same transition action as the original
(\texttt{t.getAction()} and a new \texttt{Par} instance as the final
state, where one argument is the unchanged process (either
\texttt{left} or \texttt{right}) and the other is the final state of
the original transition.  Looking at $Par1$,

\begin{center}
     \Rule{Par1}
     {E \derives{\alpha} E^\prime}
     {E \;|\; F \derives{\alpha} E^\prime \;|\; F}
     {}
\end{center}

\noindent it should be clear how this corresponds to the behaviour
described there, if $E$ is \texttt{left} and $F$ is \texttt{right}.

The majority of the method is spent handling $Par3$:

\begin{center}
      \Rule{Par3}
      {E \derives{a} E^\prime,
        F \derives{\overline{a}} F^\prime}
      {E \;|\; F \derives{\tau} E^\prime \;|\; F^\prime}
      {}
\end{center}

\noindent which represents synchronisation.  The implementation loops
over the set of new transitions\footnote{We could equally loop over
  the transitions of \texttt{left} and \texttt{right}, but we'd then
  need to store these together in another new set.  It seems slightly
  more efficient to iterate over the new transitions and decompose
  them as needed, adding any resulting transitions to a new set which
  is then added to the other new transitions at the end.}, searching
for a process which performs a name ($E \derives{a} E^\prime$).  For
each one it finds, it iterates over the transitions again, this time
in search of the corresponding coname.  If it finds a match, and both
transitions originate from different processes, then it creates a new
$\tau$ transition, using this instance of \texttt{Par} as the start
state, an instance of \texttt{Sync} as the action and a final state
created by joining together the two final states from each transition
in a new \texttt{Par} instance.  The two transitions from which the
transition was derived, \texttt{t} and \texttt{t2}, are stored by the
\texttt{Sync} action.

The \texttt{Par} implementations for CaSE and Nomadic Time are created
by extending this class and creating further transitions, based on
their additional rules.  The other constructs, including $+$, the
timeout operators and clock hiding are also implemented in much the
same way.  DynamiTE itself is concerned with more than just generating
the transitions of a process from its semantics, however, and in the
next section we see how this is handled by considering a class
introduced in the implementation of \texttt{Par} above:
\texttt{Context}.

\section{The Context of the Calculus}
\label{dyn:context}

Beyond the operational semantics, there are two important issues
involved in representing a process calculus programatically:

\begin{enumerate}
\item We need to know the \emph{context} in which algebraic
  constructions in the calculus will operate.  This includes the sets
  of names and co-names (from CCS), the set of clocks (from CaSE) and
  the environ names (from Nomadic Time); the foundations on which our
  semantic rules are built.  Although this is not essential for
  implementing CCS, where the names and co-names appear as part of the
  prefix construct $\alpha.E$, it is a necessary part of both CaSE and
  Nomadic Time.  Both these calculi have rules defined with respect to
  the set of clocks, but this set is not defined by other constructs
  in the calculus.  Instead, time is always present and we need to
  know the set of clocks to derive even the transitions for the $\nil$
  process using the $Idle$ rule.  The \texttt{Context} class maintains
  these, and more, in DynamiTE and we will look at this in more depth
  in the remainder of this section.
\item We need to know the \emph{execution semantics} for each process.
  These answer questions such as: what happens when there are multiple
  transitions from a particular process? And are there any
  side-effects to performing a transition?  In DynamiTE, these
  semantics are encoded using the \texttt{Evolver} framework, which we
  cover in \ref{dyn:evolvers}.
\end{enumerate}

In the implementation of \texttt{Par} in the previous section, we saw
how the current \texttt{Context} instance could be used to find out
whether or not a label referred to a name, using
\texttt{isRegisteredName(String)}.  In our implementation of CCS, all
names and co-names are registered with the \texttt{Context}.  This
happens automatically on the user's behalf as part of the construction
of a \texttt{Name} or \texttt{Coname} instance, or as part of
generating the transitions for the renaming operation, $E[f]$.  This
centralised checking of names and co-names gives two primary
advantages over just allowing the use of any arbitrary string:

\begin{enumerate}
\item We can prevent the silent action, $\tau$, being used as a name
  or co-name.  In CaSE and Nomadic Time, we can also prevent conflicts
  with clock names, environ names and the new mobility primitives
  such as $\tntin$.
\item We can enforce registration (and thus existence of the name or
  co-name) as a pre-requisite for other methods.  This is especially
  useful in working with channels (see \ref{dyn:plugin}).
\end{enumerate}

As a repository for names and co-names, \texttt{Context} becomes an
appropriate place for other methods related to their use.  As a
result, it also includes a number of static utility methods which are
used with co-names; these are \texttt{convertLabelToName(String)},
\texttt{convertConameToLabel(String)} and \texttt{isConame(String)}.
In our implementation, we differentiate names from co-names by marking
each letter with a combining macron (so $a$ becomes $\overline{a}$).
\texttt{convertConameToLabel(String)} creates these labels from the
original name, and \texttt{convertLabelToName(String)} returns the
original name by removing the macrons.  The \texttt{isConame(String)}
method is a simple method which just checks to see if any macrons are
present in the name.  The benefit of abstracting all these methods out
into the \texttt{Context} class is that we can later change the way
co-names are represented by modifying just one class.

The other main use for the \texttt{Context} is as the user's interface
to the plugin framework.  We have already touched on how DynamiTE
supplies implementations for multiple calculi: CCS, CaSE and Nomadic
Time.  One way this is made possible is by making the implementation
as generic as possible; we avoid relying on the specific structure or
types in a particular calculus, separating them out into methods which
can be overridden by other implementations as with
\texttt{substitute(String,Process)}.  Another aspect of this is
allowing the user to select which calculus they want to work with at
run-time.  This is made possible by the plugin framework.

\subsection{The Plugin Abstraction}
\label{dyn:plugin}

In the \texttt{Par} implementation in \ref{dyn:maptheory}, we obtain
an instance of \texttt{Context} not by calling a construct but using
\texttt{Context.getContext()}.  When using DynamiTE, only one global
instance of \texttt{Context} exists\footnote{Should DynamiTE be used
  across multiple host virtual machines, then there may be multiple
  instances, but these communicate between each other to provide one
  central store.}, which is created at startup by a
\texttt{ContextFactory}.  As with \texttt{Context}, an instance of
\texttt{ContextFactory} is obtained using a static method rather than
a constructor.  The user calls
\texttt{ContextFactory.getInstance(String,String,String)}, supplying
the name of a process calculus, a \emph{channel implementation} and a
\emph{locality implementation}.  The two latter arguments are used to
determine the execution semantics for synchronisation and movement
respectively.  The returned \texttt{ContextFactory} will be able to
supply a \texttt{Context} instance for the specified process calculus
which uses the given channel and locality implementations.  If one can
not be found, an \texttt{UnsupportedContextException} is thrown.

DynamiTE supplies an implementation of \texttt{ContextFactory} in the
form of the\\ \texttt{DynamiTEContextFactory}.  It in turn probes for
classes which implement \texttt{Calculus}, \texttt{ChannelFactory} and
\texttt{LocalityFactory} and an instance of it is returned to the user
if it finds an implementation of each which meets the user's
requirements.  Once the user has obtained an instance of
\texttt{ContextFactory}, they can call \texttt{getContext} on it to
return a \texttt{Context} instance, which can then be supplied to the
\texttt{Context.setContext(Context)} method to later be returned by
\texttt{Context.getContext()}.

Although this may sound convoluted and unnecessary, it makes the
framework much more flexible and ready for future extension, while
giving the user greater freedom of choice.  A new calculus can be
implemented simply by creating an implementation of \texttt{Process}
for each construct and an instance of \texttt{Calculus}.  The same
goes for new channel and locality implementations, and the option is
there for the entire \texttt{ContextFactory} to be replaced if needed.
From the user's perspective, everything can be handled in a single
line of code:

\begin{verbatim}
Context.setContext(ContextFactory.getInstance("CCS",
  "threaded", "dummy").getContext());
\end{verbatim}

\noindent which supplies an implementation of CCS with threaded
channels and a dummy locality implementation.

One advantage of using Java is that support for dynamically probing
for implementations at runtime is built into its class library.  The
library itself already includes a number of frameworks which work in
this fashion (including image I/O, sound and XML support) and the
\texttt{java.util.ServiceLoader} API provided in 1.6 makes it easy for
developers to define new ones.  In the \texttt{plugin} subpackage,
DynamiTE provides a means of using this API to support plugins:

\begin{verbatim}
public static <T extends Probeable> Map<String,T>
  probePlugins(ServiceLoader<T> sl)
{
  Map<String,T> map = new HashMap<String,T>();
  for (T probeable : sl)
  {
    Config.logger.config(String.format("Loaded plugin: " +
      "%s %d.%d.%d%s", probeable.getName(),
      probeable.getMajorVersion(), probeable.getMinorVersion(),
      probeable.getMicroVersion(), probeable.getAdditionalInfo()));
    map.put(probeable.getName(), probeable);
  }
  return Collections.unmodifiableMap(map);
}
\end{verbatim}

\noindent Plugins are required to implement the interface
\texttt{Probeable}, so that the name and version information can be
obtained programatically.  In DynamiTE, the \texttt{Calculus},
\texttt{ChannelFactory} and \texttt{LocalityFactory} interfaces all
extend \texttt{Probeable} so \texttt{DynamiTEContextFactory} need only
call the above method with an appropriate service loader for the
interface and it will receive back a \texttt{Map} linking names to
instances of implementations of that interface.  Most of the actual
work is done by \texttt{ServiceLoader} which reads from a text file
named after the interface, which contains a list of implementing
classes.  The DynamiTE framework supplies such text files for its
implementations of \texttt{ContextFactory}, \texttt{Calculus},
\texttt{ChannelFactory} and \texttt{LocalityFactory}.  The
\texttt{ServiceLoader} loads a listed class each time its
\texttt{next()} method is called\footnote{This happens indirectly in
  \texttt{probePlugins} via each iteration of the for-each loop.}, and
returns an instance of it, which \texttt{probePlugins} then stores in
the map.

The \texttt{Context} class stores and provides indirect access to the
implementations chosen via the \texttt{ContextFactory}.  For instance,
calling \texttt{getSyntax()} on the current \texttt{Context} instance
will return the set of syntactic constructs which form the calculus
currently in use.  The \texttt{Calculus} instance is also used in the
process of registering a name; the \texttt{Context} calls the
\texttt{Calculus} implementation to obtain the transition label for
the name, which gives the \texttt{Calculus} a chance to veto the
choice.  This is used by the \texttt{CCS} class to prevent $\tau$
being used as a name, by returning an instance of \texttt{CCSLabel},
the constructor of which contains the following check:

\begin{verbatim}
if (label.equals(TAU))
  throw new IllegalArgumentException(TAU +
                                     " is a reserved label.");
\end{verbatim}

\noindent where \texttt{TAU} is a unique instance of
\texttt{CCSLabel}.  The other instances maintained by \texttt{Context}
are used in the execution semantics which we will cover next.

\section{The Evolver Framework}
\label{dyn:evolvers}

For DynamiTE to actually be useful to users for building concurrent
applications, it needs to do something more than just evaluating an
algebraic construct and providing the possible transitions according
to the semantics of the calculus.  For an application, such as our
music player example (see \ref{app:req} and \ref{app:nt}), to actually
work, the user needs to be able to define their own internal behaviour
and share the results.

The evolver and channel frameworks provide this facility.  An
implementation of the \texttt{Evolver} interface implements the method
\texttt{evolve(Process)} according to its own particular execution
semantics.  Through this method, it is the \texttt{Evolver} instance
that makes decisions such as which transition to follow to find the
next state and also whether to process any side effects.  Side effects
take the form of additional methods which may optionally be called
after a transition has been followed.  As we saw in
\ref{dyn:maptheory}, each transition references an \texttt{Action};
this is an abstract class which provides a method \texttt{perform()}
for the purpose of implementing side effects.

DynamiTE provides a simple implementation of \texttt{Evolver} called
\texttt{Simulator} which ignores side effects.  While this is of
little use for applications, it is useful for testing design
constructs as it allows the possible transitions from a process to be
visualised.  All \texttt{Simulator} does is take a \texttt{Process}
and loop over its transitions, calling itself recursively with each
final state.  In this way, it explores the possible transitions in a
depth-first manner, until it reaches a process with either no
transitions or where all transitions have equal start and end states.
The latter condition prevents it looping forever over states with just
clock transitions or simple forms of infinite recursion, such as $\mu
X.a.X$.

\begin{verbatim}
public void evolve(Process p)
{
  System.out.println("Evolving process: " + p);
  Set<Transition> trans = p.getPossibleTransitions();
  System.out.println("Possible transitions: " + trans);
  for (Transition t : trans)
  {
    State f = t.getFinish();
    if (f instanceof Process)
    {
      if (f.equals(p))
        System.out.println("Not following transition " + t);
      else
      {
        System.out.println("Following transition " + t);
        evolve((Process) f);
      }
    }
  }
}
\end{verbatim}

A more practical \texttt{Evolver} is a more complex undertaking.  It
has to make choices as to which transition to pick when several are
presented; although CaSE and Nomadic Time have a notion of priority in
maximal progress, choices must still be made between the ticks of
different clocks, or between a clock tick and an action.  These
choices form the execution semantics of an \texttt{Evolver}
implementation, and there is plenty of room for further
experimentation in this area.

Returning to the notion of side effects, the channel framework is
accessed through four subclasses of \texttt{Action}, three of which
are also used in \texttt{Prefix}, the implementation of $\alpha.E$:

\begin{enumerate}
\item The class \texttt{Name} is used to represent the use of a name
  as part of the process $\alpha.E$.  It is also a subclass of
  \texttt{Action} and implements \texttt{perform()} by reading from an
  \texttt{InputChannel} and storing the result.
\item Likewise, the class \texttt{Coname} represents the use of a
  co-name in $\alpha.E$ and implements \texttt{perform()} by
  retrieving a value from storage and transmitting it over an \\
  \texttt{OutputChannel}.
\item The \texttt{Tau} class is the last of the three classes used in
  $\alpha.E$ and is used for the internal action, $\tau$.
  Implementing \texttt{perform()} for \texttt{Tau} is left to the
  user, who can use it to implement arbitrary sequential behaviour as
  required.
\item The \texttt{Sync} class is created through \texttt{Par} (see
  \ref{dyn:maptheory}) and is a subclass of \texttt{Tau} which
  implements \texttt{perform()} using the two synchronising
  transitions provided to it on construction.
\end{enumerate}

The \texttt{InputChannel} and \texttt{OutputChannel} instances are
obtained from the \\ \texttt{ChannelFactory}, via the \texttt{Context},
and provide \texttt{read()} and \texttt{write(Object)} methods
respectively.  Although there is currently no realisation of data
within the formal layer of the calculus, this only matters to the
extent that we wish transmitted data to alter the constructs
themselves via substitution\footnote{The $\pi$ calculus (see
  \ref{picalculus}) is an obvious example of such behaviour, which
  goes to the extreme of not only allowing data to be transferred but
  also references to channels which can then later be used in the
  language constructs.  This, in essence, provides the form of
  mobility present in the $\pi$ calculus.}.  Data can be transferred
between processes and used within internal actions without having to
be explicitly realised at the formal level.  The operation of these
I/O operations, and the creation of a suitable environment in which
this may happen, is left to the implementation of the
\texttt{ChannelFactory} and there are a multitude of ways of doing so.
These range from simple mechanisms like files and sockets to complex
interprocess communication protocols such as Java's Remote Method
Invocation (RMI), the Common Object Request Broker Architecture
(CORBA) and web services.  The plugin nature of the channel
architecture means that any of these possibilities may be used and
more besides.

DynamiTE provides a sample implementation,
\texttt{ThreadedChannelFactory}, built on a \texttt{SynchronousQueue};
the \texttt{read} and \texttt{write} operations are performed by
synchronising two different threads and transferring the data
directly.  Thus, if \texttt{read} is called, and another thread is not
already waiting in the \texttt{write} method, it will block until this
is the case.  The same is true for \texttt{write}.  As noted above,
the \texttt{ChannelFactory} is also responsible for creating the
necessary environment for these operations, so the
\texttt{ThreadedChannelFactory} has to ensure that appropriate threads
are created and used.  A hook, \texttt{runInParallel(Process,
  Process)} is used for this purpose and is called indirectly by the
constructor of \texttt{Par}.

Data storage is also provided by the \texttt{ChannelFactory}.  The
channel I/O operations are automated side-effects of following a
transition labelled with a name or co-name, so user code (implemented
in a \texttt{Tau} subclass) must be able to access any values
retrieved and store new ones when it is itself performed.  The
\texttt{ChannelFactory} provides storage repositories, keyed by the
name of the channel, for this purpose.  When a user wishes to transmit
a value, they call \texttt{store(String,Object)}, where the first
argument is the channel name and the second the data to store.  Later,
the performance of a \texttt{Coname} with that channel name will
lookup the data and transmit it to the corresponding \texttt{Name}.
User code in a further \texttt{Tau} implementation can then retrieve
this using \texttt{retrieve(String)}, passing to it the name of the
channel and receiving back the data.  The
\texttt{ThreadedChannelFactory} implements this by storing data in a
\texttt{ThreadLocal}, so that it is only retrievable by the same
thread that stored it:

\begin{verbatim}
public void store(String name, Object data)
{
  ThreadLocal<Object> store = repositories.get(name);
  if (store == null)
  {
    ThreadLocal<Object> newStore = new ThreadLocal<Object>();
    store = repositories.putIfAbsent(name, newStore);
    if (store == null)
      store = newStore;
  }
  store.set(data);
}
\end{verbatim}

The \texttt{repositories} variable stores an instance of
\texttt{ConcurrentHashMap}, which guarantees that retrieval operations
will always return results which reflect the most recently completed
operations.  As we described in \ref{java:concurrency},
\texttt{ConcurrentHashMap} is thread-safe while also being much more
efficient than \texttt{Hashtable} which locks the entire collection on
any operation.  These guarantees do not, however, help in performing a
sequence of operations on the map; specifically, the map may still be
changed by another thread in the interim period between performing a
\texttt{get} and a \texttt{put}.

This applies to the method above, as we need to check whether or not a
\texttt{ThreadLocal} instance has been created for a particular
channel name, and if not, create one.  The initial \texttt{get}
operation will return \texttt{null} if we have not yet created a mapping
between that channel name and a \texttt{ThreadLocal}.  However, in the
time it takes for this thread to check the return value and create a
\texttt{ThreadLocal}, another thread may have added such a mapping.
The \texttt{ConcurrentMap} interface (which \texttt{ConcurrentHashMap}
implements) provides an additional operation for just this issue:
\texttt{putIfAbsent}.  This provides an atomic version of the
following code:

\begin{verbatim}
if (map.containsKey(key))
  return map.get(key);
else
  return map.put(key, value);
\end{verbatim}

If, as in the most likely case, a mapping has not been added since our
initial \texttt{get}, then the \texttt{putIfAbsent} call folds down to
a normal \texttt{put} call.  The \texttt{put} method returns either
the previous value of the mapping or \texttt{null} if there wasn't
one.  In this case, it will always return \texttt{null} as the
\texttt{containsKey} call has already ensured no mapping exists.
Thus, if \texttt{null} is returned, we know that our new
\texttt{ThreadLocal} was used for the mapping.  If the return value is
not \texttt{null}, then another thread managed to add a mapping before
us, and the \texttt{ThreadLocal} we created is simply discarded when
we leave the scope of the \texttt{if} block in which it was created.

The complexity of this operation again shows how difficult it can be
to make operations involving shared objects thread-safe.  The
advantage of using DynamiTE is that the user doesn't have to come into
contact with this.  The user merely store a value and the framework
handles the thread safety issues involved.  Providing these kind of
reusable generic constructs makes it much easier to build concurrent
applications that are thread-safe.

From this discussion, it should now be clear how concurrent
applications are created in DynamiTE; the user constructs
\texttt{Process} instances which match their design in the process
calculus, and provides \texttt{Tau} subclasses for the actual work
performed by the application.  Data is passed between \texttt{Tau}
subclasses via the channel framework.

DynamiTE also provides hooks for a locality framework, which allows
side effects to take place on the performance of mobility actions,
just as the channel framework provides side effects for
synchronisation.  At present, DynamiTE simply has a dummy
implementation for this, but this has great scope for being used to
implement process migration.  Migrating an active process is not a
simple operation -- not only must the continuation of the code be
transferred, but any local data must also migrate.  Using the Nomadic
Time process calculus as our basis allow us to achieve a significant
amount of simplification here; the transferred process is already
separated from other code within the system by virtue of the moving
process being in the form of a \texttt{Prefix} instance.  When the
action is matched to the one used for the mobility operation, the
\texttt{Process} instance is transferred to its new location.  There
is no need to deal with code that is currently being executed.  We
also know exactly what data to be transferred, as this is centrally
managed by the channel framework.  Again, this is an interesting area
for future work.

Now we have explored DynamiTE, the next section provides a walkthrough
example of creating an application using the framework, following on
from \ref{app:req} and \ref{app:nt}.  We also compare this
implementation with a standard implementation using low-level
concurrency primitives, thus allowing us to evaluate whether our
claims made in \ref{solution} hold true.

\section{A Prototypical Application in DynamiTE}
\label{app:dynamite}

In \ref{app:nt}, we created a design for the music player application
using the Nomadic Time calculus.  This resulted in a system defined by
the following equations:

\begin{equation}
\label{app:eqn}
\begin{aligned}
  & Out \eqdef o.\stimeout{\Delta}{\sigma}{\tau.\nil} \\
  & Analy \eqdef o.\stimeout{\Delta}{\sigma}{\tau.\nil} \\
  & IntSys \eqdef i.\mu X.(\tau.\mu W.\stimeout{\overline{o}.W}{\sigma}{X} \pc Out \pc Analy) \\
  & Intf \eqdef useri.(\overline{i}.\nil | IntSys) \\
  & App \eqdef \loc{player}{Intf}{\bin.\Omega}{\sigma} 
\end{aligned}
\end{equation}

In this section, we will see how these constructions can be turned
into Java objects.  The process is fairly simple, and results in a
system which leverages the existing concurrency work performed in the
development of the DynamiTE framework.  We then look at how the same
application could be written without the framework, using objects in
shared memory.

The operators in the above all map to Java classes as we saw in
\ref{dyn:maptheory}; $\Delta$ is implemented by \texttt{Delta}, the
stable timeout by \texttt{STo}, recursion by \texttt{Rec} and the
environ by \texttt{Env}.  The channels, $i$, $o$ and $useri$ are
represented using instances of \texttt{Name} and
\texttt{Coname}. Thus, all that remains is the internal silent actions
$\tau$.

Each silent action is represented by a different subclass of
\texttt{Tau} and its \texttt{perform()} method is implemented so as to
do the actual work required of the application.  The implementation
for the silent action used in $Out$ looks something like this:

\begin{verbatim}
public class Out
  extends Tau
{
  public void perform()
  {
    Context ctx = Context.getContext();
    Object soundData = ctx.retrieve("o");
    if (soundData != null && soundData instanceof byte[])
    {
      play((byte[]) soundData);
    }
    else
    {
      throw new InternalError("Didn't receive sound data.");
    }
  }
}
\end{verbatim}

The \texttt{perform()} method first retrieves the data from the
\verb!"o"! repository, where it should have been placed earlier by
the $o$ action.  If it is successfully, the data is played out on the
speakers.  In the unlikely case that something went wrong, and the
expected data is not held in the repository, we throw an error.

The implementation for the silent action in $Analy$ is identical,
except that the sound data retrieved is visualised rather than played.
The one for $In$ is slightly more complicated, as we have to both
store and retrieve from different repositories:

\begin{verbatim}
public class In
  extends Tau
{

  private InputStream is;

  public void perform()
    throws IOException
  {
    if (is == null)
    {
      Context ctx = Context.getContext();
      Object fileName = ctx.retrieve("i");
      is = new FileInputStream((String) fileName);
    }
    byte[] soundData = readAndProcessData(is);
    ctx.store("o", soundData);
  }
}
\end{verbatim}

This time we allow the use of the retrieved value by the file routines
throw up any issues.  If this is the first time the $\tau$ action is
encountered, then the filename is retrieved from the \verb!"i"!
repository where it was stored by the earlier $i$ action and an
\texttt{InputStream} created so that data may be read from the file.
After this, and on each subsequent invocation, data is read from the
stream, processed and store in a \texttt{byte} array in the
\verb!"o"! repository.

Now we have our silent actions, we can construct our processes:

\begin{verbatim}
Context.setContext(ContextFactory.getInstance("NT",
  "threaded", "dummy").getContext());
Context ctx = Context.getContext();
Clock sigma = new Clock("\u03C3"); \\ u03C3 = sigma in Unicode
Name output = new Name("o");
Process out = new Prefix(output,
  new STo(Delta.DELTA, sigma, new Prefix(new Out(), Nil.NIL));
Process analy = new Prefix(output,
  new STo(Delta.DELTA, sigma, new Prefix(new Analy(), Nil.NIL));
Process inLoop = new Prefix(new In(), new Rec("W",
  new STo(new Prefix(new Coname("o"), new Var("W")), sigma,
          new Var("X"))));
Process intSys = new Prefix(new Name("i"),
  new Rec("X", new Par(new Par(inLoop, out), analy)));
Process intf = new Prefix(new Name("useri",
  new Par(new Prefix(new Coname("i"), Nil.NIL), intSys)));
Set<Clock> hiddenClocks = new HashSet<Clock>();
hiddenClocks.add(sigma);
Process app = new Env("player", intf,
  new MobPrefix(MobPrim.BOUNCER_IN, Omega.OMEGA),
  hiddenClocks);
\end{verbatim}

If the above is compared with Eqn. \ref{app:eqn}, it should be clear
how each term is turned into a instance of a Java class.  The
references to \texttt{Delta.DELTA}, \texttt{Nil.NIL},
\texttt{MobPrim.BOUNCER\_IN} and \texttt{Omega.OMEGA} refer to
singleton instances, as none of these have any variable attributes,
and the constructors of \texttt{Name}, \texttt{Coname}, \texttt{Clock}
and \texttt{Env} register the new entity with the \texttt{Context}.
At the end of running this code, we are left with a \texttt{Context}
containing the names \texttt{i},\texttt{o} and \texttt{useri}, the
conames \texttt{i} and \texttt{o}, the clock $\sigma$, the environ
\texttt{player}, and an instance \texttt{app} which can be passed to
an \texttt{Evolver} instance to run the application.

The same application can be implemented without DynamiTE in
innumerable ways, but one thing holds for all of them; they must
either include provisions to ensure thread safety or be purely single
threaded.  Writing such an application using a single thread produces
an unworkable result; while the sound data is being output, nothing
else can be done so any visualiser will be out of sync with the sound
being played.  Additionally, no data will be being read while this is
happening, so the application has to rely on there being enough time
between the data has been sent to the speakers and the actual sound
finishing for it to read more data and send it.  This becomes even
more unlikely when the visualiser is factored in.

Thus, we will assume that any sensible implementation, like our
DynamiTE-based application, will use a thread for each of the input,
output and visualisation processes.  Unlike with DynamiTE, we now have
to consider how the sound data will be stored and how it will be
shared between threads.  For simplicity, we start by just considering
the input and output threads:

\begin{verbatim}
public class Player
{

  private BlockingQueue<byte[]> queue;

  private String fileName;

  public void input()
  {
    InputStream is = new FileInputStream((String) fileName);
    while (true)
    {
      byte[] soundData = readAndProcessData(is);
      queue.put(soundData);
    }
  }

  public void output()
  {
    while (true)
    {
       byte[] soundData = queue.take();
       play(soundData);
    }
  }

  public static void main(String[] args)
  {
    fileName = args[0];
    queue = new LinkedBlockingQueue();
    Thread input = new Thread(new Runnable()
    {
      public void run() { input(); }
    }, "input");
    Thread output = new Thread(new Runnable()
    {
      public void run() { output(); }
    }, "output");
    input.join();
    output.join();
  }
}     
\end{verbatim}

We've kept the implementation as close as possible to that for
\texttt{In} and \texttt{Out} above, including not handling the end of
the file.  Realistically, the thread should terminate when this
happens.  Although the similarities between the two should be clear,
so should the differences.  In this example, we have had to consider
both how data is stored and how the methods are run; these are handled
by the channel and evolver frameworks respectively in DynamiTE.  The
\texttt{BlockingQueue} is thread-safe, so the threads can not corrupt
the data structure if two or more happen to try and perform an
operation on the queue at the same time.  This is the same as the
locks used in \ref{semaphores} with the locking occurring inside the
collection rather than visibly in the surrounding code.  The
\texttt{take} operation also blocks if the queue is empty until an
item is added\footnote{The \texttt{put} operation also blocks if a
  capacity is given for the list on creation; we don't do so here.};
again the behaviour inside the queue is akin to what we saw with
signalling in \ref{semaphores}.

This works fine for this example, but there is an immediate problem if
we want to introduce the visualiser into the mix; the \texttt{take}
operation performed on the queue by both \texttt{output} and the new
\texttt{visualise} method will remove the item from the queue so one
thread will get a particular value and the other one won't.  There are
a number of possible solutions to this:

\begin{enumerate}
\item We can replace the queue with an indexed collection and remember
  which index was last used.  The new collection also has to be
  thread-safe and we put ourselves at risk of running out of memory as
  the queue will only ever increase in size.
\item We add each item to the queue twice.  This makes the
  \texttt{input} method dependent on the number of consuming threads.
  It also means we have to add our own external locking to ensure that
  one of the consumers does not perform a \texttt{take} while the
  items are being added, and we have to check on each iteration that
  we are getting a new value and not a copy of the previous one still
  waiting to be taken by the other thread.
\end{enumerate}

Below we provide an implementation of the second solution:

\begin{verbatim}
public class Player
{

  private static final int NUMBER_OF_CONSUMERS = 2;

  private Queue<byte[]> queue;

  private String fileName;

  public void input()
  {
    InputStream is = new FileInputStream((String) fileName);
    while (true)
    {
      byte[] soundData = readAndProcessData(is);
      synchronized (this)
      {
        for (int a = 0; a < NUMBER_OF_CONSUMERS; ++a)
          queue.offer(soundData);
        notifyAll();
      }
    }
  }

  public void output()
  {
    byte[] soundData = null;
    while (true)
    {
       synchronized (this)
       {
         while (queue.peek() == soundData ||
                queue.peek() == null)
         {
           wait();
         }
         soundData = queue.poll();
         notifyAll();
       }
       play(soundData);
    }
  }

  public void visualise()
  {
    byte[] soundData = null;
    while (true)
    {
       synchronized (this)
       {
         while (queue.peek() == soundData ||
                queue.peek() == null)
         {
           wait();
         }
         soundData = queue.poll();
         notifyAll();
       }
       visualise(soundData);
    }
  }

  public static void main(String[] args)
  {
    fileName = args[0];
    queue = new LinkedList();
    Thread input = new Thread(new Runnable()
    {
      public void run() { input(); }
    }, "input");
    Thread output = new Thread(new Runnable()
    {
      public void run() { output(); }
    }, "output");
    Thread analy = new Thread(new Runnable()
    {
      public void run() { visualise(); }
    }, "analy");
    input.join();
    output.join();
  }
}     
\end{verbatim}

This is clearly more complicated than the example with just input and
output.  We dispense with the \texttt{BlockingQueue} as we need to
obtain a lock anyway to ensure the atomicity of the multiple
\texttt{offer} calls in \texttt{input} and the checks in
\texttt{output} and \texttt{visualise}.  Both the \texttt{output} and
\texttt{visualise} methods are very similar and it would be a very
good idea to generalise these in a \texttt{Consumer} superclass; as we
noted in \ref{semaphores}, the correct placement of locking and
signalling constructs is prone to simple errors so anything we can do
to simplify this and minimise the risk is advantageous.

The consumer methods now loop until \texttt{peek} returns a new unseen
value.  When we reach this loop, \texttt{peek} may return one of three
things:

\begin{enumerate}
\item It may be \texttt{null} if nothing has yet been added or both
  values from the last run have been retrieved.
\item It may equal the previous value, indicating that the other
  consumer has not yet read its data.
\item It may be a new unseen value which is neither \texttt{null} or
  equal to \texttt{soundData}.  In this case, we exit the loop, remove
  the value from the queue and notify any waiting threads that the
  thread has changed.
\end{enumerate}

If either of the first two conditions hold, \texttt{wait} is called,
causing the thread to relinquish its lock and sleep until notified by
a call to \texttt{notifyAll}.  This solution should be thread safe as
presented, but it should also be obvious how a simple misplaced call
could break this; thread safety is a very fragile property.

It should be clear from this example that DynamiTE makes implementing
this a lot simpler; not only do we not have to worry about locking the
data structure and signalling other processes at the correct points,
we have a solution which works for any number of consumers without
changing \texttt{input}.  The only way that would be possible here is
by a hack to use the names of threads as identifiers whether they are
consumers or not; not a very elegant solution.  The abstraction in
DynamiTE also means that a simple one line change to the channel
factory being used can turn an application communicating between
threads as above, to one communicating over a network with no other
changes to the application.

In summary, we think DynamiTE has achieved its goals; it abstracts
away many complex and fragile pieces of code which ensure thread
safety, in much the same way as the concurrency collection classes do
in the class library.  They are replaced by simple abstract concepts
such as \texttt{store} and \texttt{retrieve} calls to the repositories.
In implementing DynamiTE, we believe we have also made it approachable
for existing Java programmers, either to use to build their own
applications or even to contribute to DynamiTE itself.  By learning
the meaning of a small number of algebraic concepts, they can leverage
the power of DynamiTE to create applications limited only by the
plugin factories being used.

\section{Related Work}
\label{dyn:relatedwork}

In this final section, we look briefly at the existing body of
research concerned with providing concurrent frameworks, including
those based on process calculi.

The $\pi$ calculus has been the subject of much of this work,
primarily due to its status as the most prevalant mobile process
calculus.  Obliq \cite{obliq} and Pict \cite{daveturner:phd} are both
programming languages with semantics founded in the $\pi$ calculus,
while Nomadic Pict \cite{wojciechowski:phd} extends Pict by
introducing distribution, a feature not usually present in the $\pi$
calculus.

\subsection{Obliq}
Obliq was originally developed by Luca Cardelli in 1995, prior to his
work on the ambient calculus.  While being an object-oriented
language, it has no notion of classes in the same way that languages
like C++ and Java do.  Objects are instead constructed directly and
assigned to variables:

\begin{verbatim}
let o = 
  { 
    x => 3,
    inc => meth(s,y) s.x := s.x+y; s end
    next => meth(s) s.inc(1).x end 
  }
\end{verbatim}

In the above example, an object is created with two methods,
\texttt{inc} and \texttt{next}.  The first argument of a method is
explicitly named (\texttt{s} in the above) but always contains a
reference to the object itself (\texttt{this} in Java), rather than
some argument passed by the method call.  The object instance is
assigned to \texttt{o}, so the only way of executing its methods or
manipulating its values is via either \texttt{o} itself or a clone of
it (created by \texttt{clone(o)}.  The method \texttt{inc} increases
the value of \texttt{x} by \texttt{y}, while the method \texttt{next}
uses this method to give the next value of \texttt{x}.  Methods
implicitly return the value computed by their body; \texttt{s} and
\texttt{s.inc(1).x} respectively in this case.

Objects in Obliq contain only fields, but these fields can contain
\emph{methods}, \emph{aliases} and \emph{values}.  The latter
including procedures, which differ from methods in not having a
reference to the object as their first argument.  Fields are
dynamically typed, so, for example, even if \texttt{x} is given the
value \texttt{3} initially, it can later be assigned a method.  An
alias allows an operation to be redirected; for example \texttt{a.x :=
  alias y of b end} makes any attempt to access \texttt{a.x}
equivalent to accessing \texttt{b.y}.

The main feature of Obliq is that it contains concurrency and remote
invocation primitives.  The following implements the producer/consumer
queue we saw in \ref{semaphores} in Obliq:
\begin{verbatim}
let queue =
  (let nonEmpty = condition();
   var q = [];

   { protected, serialized
     write =>
       meth(s,elem)
         q := q @ [elem];
         signal(nonEmpty);
       end,
     read =>
       meth(s)
         watch nonEmpty until #(q)>0 end;
         let q0 = q[0];
         q := q[1 for #(q)-1];
         q0;
       end;
   }
  );
\end{verbatim}
The field \texttt{nonEmpty} is assigned a condition queue, like the
implicit one provided to all objects in Java, while \texttt{q} stores
the data.  The \texttt{protected} modifier prevents external
modification to the fields (the equivalent of declaring them all
\texttt{private} in Java), while \texttt{serialized} is akin to
acquiring a mutex at the beginning of each method and releasing it at
the end.  The latter thus ensures that the queue is only manipulated
by one thread at a time, and also works in tandem with the condition
queue.  The \texttt{watch} statement is equivalent to \texttt{wait},
except that it is slightly safer as the need to loop over a condition
is tied to the waiting process by the use of unique syntax as opposed
to a method call.  The statement translates to \texttt{while
  (q.length() == 0) \{ wait(); \}} in Java.  Likewise, \texttt{signal}
is equivalent to \texttt{notifyAll}.

The following code:
\begin{verbatim}
let t = fork(proc() queue.read() end, 0);
queue.write(3);
let result = join(t);
\end{verbatim}
shows how the queue can be used.  A separate thread, \texttt{t}, is
forked to read from the queue.  This is necessary as the call will
block until the queue's length becomes greater than zero.  We then
write the value 3 to the queue, and wait using a \texttt{join} call
for \texttt{t}. Once \texttt{t} has awoken and read from the queue, it
should return a result of 3 which ends up in \texttt{result}.

Remote invocation works in the same style as protocols such as CORBA
\cite{omg:2009} and Java RMI \cite{rmi}; a name server allows objects
to be registered with a name, allowing them to be looked up and
retrieved.  The following Obliq code implements object migration:

\begin{verbatim}
let migrateProc =
  proc(obj, engineName)
    let engine = net_importEngine(engineName, namer);
    let remoteObj = engine(proc(arg) clone(obj) end);
    redirect obj to remoteObj end;
    remoteObj;
  end;
\end{verbatim}

The procedure takes two arguments: the object to migrate
(\texttt{obj}) and the remote site to migrate it to
(\texttt{engineName}).  The invocation of \texttt{net\_importEngine}
obtains a reference to an \emph{execution engine} from the name
server, \texttt{namer}.  Execution engines accept a procedure as an
argument, which is then run at the remote site; thus
\texttt{clone(obj)} is run not locally but at the site of
\texttt{engineName} so that \texttt{remoteObj} ends up being a
reference to the remote clone.  The \texttt{redirect} statement then
acts as a shorthand for aliasing each field in the local \texttt{obj}
to point to \texttt{remoteObj} so that future invocations on
\texttt{obj} access the remote clone.

Obliq has two advantages over traditional languages:

\begin{enumerate}
\item Threads, condition queues and remote invocation are built into
  the language as primitives, so developers do not have to depend on
  external libraries.
\item It can be given a semantics using the $\pi$ calculus
  \cite{obliqsem}, which allow its form of migration, known as
  \emph{object surrogation}, described above to be proved correct.
\end{enumerate}

However, on the downside, Obliq requires the user to learn the syntax
and semantics of a completely new language, as well as giving up any
existing libraries that may be available for development in the
language they traditionally work in.  It also doesn't particularly
simplify anything; locking still has to be performed at the same
intricate level of granularity as in a language like Java, while
foregoing the much greater base of experience and libraries available
in that language.  This is unlike our framework, DynamiTE, where
locking and transmission of data is abstracted away via the use of
storage repositories, and the user can work in the familiar Java
programming language, thus having only to learn a fairly small API in
order to use the framework.  In all, Obliq is interesting as a piece
of early research in this area, but we don't see it being of much
practical use.

\subsection{Nomadic Pict}

Pict is a programming language based on the asynchronous $\pi$
calculus (see \ref{pivariants}).  Nomadic Pict extends this by adding
in a notion of distribution; users can create \emph{agents} which
migrate between \emph{sites}, and channels are located at a particular
site.  An example Nomadic Pict program looks like this:

\begin{verbatim}
new answer : ^String
  def spawn [s:Site prompt:String] =
    (agent b =
      (migrate to s
       <a@s'>answer!(sys.read prompt))
     in
      ()
    )
  (spawn ! [s1 "How are you? - "]
  |spawn ! [s2 "When does the meeting start? - "]
  |answer ?* s = print!s)
\end{verbatim}

The code runs inside an agent \texttt{a} located at the site
\texttt{s'}.  It defines a function \texttt{spawn} which, when called,
creates an agent \texttt{b} which migrates to the supplied site
\texttt{s}.  Once at \texttt{s}, the agent attempts to output on the
channel \texttt{answer} which is located back at site \texttt{s'} and
attached to agent \texttt{a}, having being created by the \texttt{new}
statement.  The value transmitted is first input by the user, who is
first prompted with the string supplied to the \texttt{spawn}
function.

The agent \texttt{a} itself spawns three processes, two of which call
\texttt{spawn}.  The third forms the other end of the communication
with agent \texttt{b} by waiting for input on \texttt{answer}.  The
use of \texttt{?*} for input as opposed to just \texttt{?} makes the
input replicated using the $!$ operator from the $\pi$ calculus, so it
is always available.  When the program is run, the first two processes
will migrate to \texttt{s1} and \texttt{s2} respectively.  They then
prompt the user with their respective messages, and return the user's
input to agent \texttt{a} via the \texttt{answer} channel.  The third
process in \texttt{a} prints whatever is received.

Perhaps the clearest thing about Nomadic Pict from this example is
that it is not the most readable of languages.  Being designed
primarily as a way of programming in the $\pi$ calculus rather than as
a usable language means that the syntax leads something to be desired
and is fairly unaccessible for those who don't know the calculus in
detail.  On the positive side, this does give the language a strong
formal background and semantics, and unlike Obliq, it has a rich type
system with polymorphism and subtyping. This can be seen above where
\texttt{answer} is declared as \texttt{\textasciicircum String}, a channel type which
both inputs and outputs values of type \texttt{String}.  This type
itself is a subtype of both \texttt{!String} (an output channel which
sends a \texttt{String}) and \texttt{?String} (an input channel
expecting a \texttt{String}).

Most of the criticisms we had of Obliq do apply to Nomadic Pict
however.  While it represents interesting research, and could prove
very useful in some areas, it's not suitable as a general purpose
language for the masses.

\subsection{The Safe Ambients Abstract Machine}

Something closer to our work on DynamiTE is the abstract machine PAN
for safe ambients \cite{sangiorgi:safeambientsmachine}.  Safe
ambients were discussed back in \ref{ambvariants}; they provide a form
of the ambient calculus which is protected from \emph{grave
  interferences} by requiring each mobility action ($in$, $out$ and
$open$) to be matched by a corresponding co-action.  We adopt the same
idea ourselves in Nomadic Time with bouncers, but unlike the
co-actions in the safe ambient calculus, ours must only be used by the
bouncer which is located on each environ.

There are also similarities between our implementation, DynamiTE, and
PAN, as both are implemented in Java.  They differ, however, in that
with PAN, the implementation takes the form of an abstract machine
which is first formally specified and then implemented in Java.
Rather than attaching Java code to certain terms in the calculus, as
in DynamiTE, PAN users write their programs in the abstract machine,
and the code is then compiled and executed by the Java implementation,
which maintains a 1:1 relationship between ambients and threads.  As
this means the input to PAN is a slightly extended form of the safe
ambient calculus, the present implementation described in
\cite{sangiorgi:safeambientsmachine} would need to be extended further
`to embed this core language into a real language'.  Interestingly,
the authors also mention that an `orthogonal direction would be to
make the ambient constructs into a framework'; this sounds very
similar to what we have now with DynamiTE.  Sadly, we have been unable
to find any details of further work beyond this paper.

The majority of the paper is concerned with the design of the abstract
machine itself, rather than the implementation.  The main focus is on
clearly defining the separation between the logical distribution of
the ambients derived from terms in the safe ambient calculus, and the
physical distribution used by the machine.  This work is fairly
generic, and could also be applied as a means to formalise and clarify
the use of environs within DynamiTE; these also have a logical
distribution given by Nomadic Time and a physical distribution as
defined by the \texttt{LocalityFactory}.

A term in the safe ambient calculus is mapped onto a flat physical
topology of \emph{located ambients}.  For example, given the following
safe ambient construct:

\begin{equation}
\lncloc{n}{P_1 \pc P_2 \pc \lncloc{m_1}{Q_1} \pc \lncloc{m_2}{Q_2}}
\end{equation}

A located ambient has the form $\lclocv{h:n}{P}{k}$, where $h$ is the
location of the ambient and $k$ is the location of the parent.  For
the above, we end up with three locations: $h$, $k_1$ and $k_2$, all
of which co-exist at the same level; there is no hierarchical topology
to the locations as there is for the ambients.  This gives us
$\lclocv{h:n}{P_1 \pc P_2}{root}$, $\lclocv{k_1:m_1}{Q_1}{h}$ and
$\lclocv{k_2:m_2}{Q_2}{h}$, where the subambient relationship between
$n$ and $m_1$, and likewise $n$ and $m_2$, is represented by the use
of $h$ in the latter two terms rather than by physical placement.

This changes how the mobility operations proceed as well; $in$ and
$out$ have no effect on the physical structure, as they simply change
the parent location.  The $open$ operation does cause a physical move,
as the processes within the destroyed ambient are moved into the
parent.  For example, if $P_1 = \ambopen{k_1}.P_1'$ and $P_2 =
\sambopen{k_1}.P_2'$, then $k_1$ would be destroyed and $Q_1$ would
move into $h$, giving $\lclocv{h:n}{P_1' \pc P_2' \pc Q_1}{root}$.

This representation makes a lot of sense, and relates closely to how
we foresee a full \texttt{LocalityFactory} implementation operation
mapping the logical to the physical.  Thus, although work on the
implementation of PAN seems to have come to a halt, we can make use of
the research the project produced by feeding it into the development
of DynamiTE.

\subsection{JavaSeal}

The final piece of related work we will consider is the Seal calculus
\cite{seal}.  This is probably the closest of those covered here to
DynamiTE; it was designed with implementation in mind \cite{javaseal},
specifically `secure distributed applications', and uses a similar
technique of `cherry-picking' some of the best features from other
calculi to create a new one that best fits the proposed goal of the
project.

The implementation of the Seal calculus, the JavaSeal Mobile Agent
Kernel, also uses a similar approach to DynamiTE, with the constructs
of the calculus represented as objects:

\begin{verbatim}
public abstract class Seal
  implements Runnable, Serializable
{
  public static Seal currentSeal();
  public static void dispose(Name subseal);
  public static void rename(Name subseal, Name subseal);
  public static SAF wrap(Name subseal);
  public void run();
}
\end{verbatim}

A \texttt{Seal} instance is \emph{wrapped} for migration by stopping
its threads and serialising its contents into a byte array.  Both when
a seal is created and when it is \emph{unwrapped} following migration,
a \texttt{Strand} is created for it and the \texttt{run} method is
invoked.  Each \texttt{Strand} instance is bound to the particular
\texttt{Seal} instance that created it, and provides the necessary
mapping on to Java threads.

Formally, just as Nomadic Time combines the ambient calculus with
CaSE, the Seal calculus takes the synchronous polyadic $\pi$ calculus,
and adds localities to it, in the form of \emph{seals}.  One of the
main differences between Nomadic Time and the Seal calculus is that
the latter uses channels for migration, rather than applying the
ambient set of mobility primitives ($in$, $out$ and $open$).  Seal
migration takes place objectively over channels, with the local
process as the sender and the remote process as the recipient.  

The action prefix, $\alpha.E$, in the Seal calculus has four forms:

\begin{enumerate}
\item $\overline{x}^\eta(\vec{y})$
\item $x^\eta(\lambda \vec{y})$
\item $\overline{x}^\eta\{y\}$
\item $x^\eta\{\vec{y}\}$
\end{enumerate}

The first two handle the communication of names, as in the $\pi$
calculus, and the second two are for the transmission of seals.
Channels exist within a certain location, and $\eta$ is used to direct
the communication.  It takes one of three values: $\eta ::= \star |
\uparrow | n$, where $\star$ refers to a local channel, $\uparrow$ to
a channel in the parent seal and $n$ to a channel in a child seal.
Communication occurs between either a pair of corresponding local
prefixes, or between a local prefix and a remote prefix such as
$x^\star(\lambda y)$ and $\overline x^n(z)$, or $x^\star(\lambda z)$
and $\overline{x}^\uparrow(y)$.  Access restriction can be enforced
via the use of \emph{portals} and the $open_\eta x$ syntax; for remote
interaction to take place, the corresponding $open$ action must be
provided by a process running in parallel with the process offering
the local prefix, in the same way that a co-capability must be
provided in the Safe Ambients calculus or a bouncer in Nomadic Time.
For example, in

\begin{equation}
  open_n \overline{x}.S_1 \pc x^\star(\lambda z).\overline{z}^n().S_2 \pc 
  n[\overline{x}^\uparrow(y).open_\uparrow \overline{y}.P_1 \pc y^\star ().P2]
\end{equation}

\noindent the provision of $open_n \overline{x}$ allows communication
to occur between $x^\star(\lambda z)$ and $\overline{x}\uparrow(y)$,
and $open_\uparrow \overline{y}$ does likewise for $\overline{z}^n()$
and $y^\star ()$.

  As with PAN, it seems that development on the Seal calculus has
  stopped, especially as regards the implementation work.  Further
  research into the calculus itself has been performed
  \cite{seal01,seal02} since the initial publication of the Seal
  calculus, but this now also seems to have finished.  Unfortunately,
  it seems common in the academic community for implementations to be
  written as a proof of concept, but then not taken any further.
  Outside academia, even the newest approaches are still relatively
  low-level; the latest offering is CUDA \cite{cuda}, a C-style
  parallel processing language for programming NVIDIA graphics
  processing units (GPUs).  The development direction with frameworks
  and languages like CUDA is usually the inverse of the academic case;
  little research goes into formalising it but it does get heavily
  used in real-world situations to produce results, some of which may
  be true.

As far as implementations go, we believe our work to be novel in
approaching the task of translating a calculus with both global
discrete time and mobility into a usable programming framework.  We
also seem to be one among only a few research projects to create an
implementation in an existing programming language and allow users to
work with it in that language; of the above, this only applies to
JavaSeal.  This is a shame, as it means otherwise good ideas are let
down by the implementation being simply unapproachable for the
majority of software developers.

\section{Conclusion}

To conclude, we have presented the overall structure of the DynamiTE
framework for concurrent systems, including the process framework for
implementing operational semantics and the evolver framework for
working with execution semantics.  We believe that the framework
provides a unique way of developing concurrent systems.  It provides
features which have already proved advantageous in a theoretical
setting, such as the n-ary process synchronisation mechanism described
in chapter \ref{globsync}.  The existence of a formal theory for
DynamiTE's behaviour gives many advantages over more ad-hoc
approaches, potentially allowing the underlying design to be
rigorously examined before being applied to the implementation.  The
behaviour of the system may be established clearly and unambiguously
in the underlying process calculus before implementation even begins,
giving a solid grounding on top of which the individual tasks may be
developed.

The actual implementation of the DynamiTE framework is still in heavy
development; the code is available at:
\begin{center}
\url{https://savannah.nongnu.org/projects/dynamite/} 
\end{center}
\noindent and patch submissions are welcome.  At its lowest level, it
provides a means of simulating the operations of the Nomadic Time
process calculus, allowing them to be more clearly understood.  In
application, it can provide a useful mechanism for structuring
concurrent programs, clearly dividing internal behaviour and
interprocess communication.  The possibility to add further
implementations of the channel and locality factories, via the plugin
mechanism, also means that fairly complex concepts can then be
leveraged by the programmer in the simple manner provided by the
framework.
