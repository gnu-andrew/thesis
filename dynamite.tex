% Thesis: DynamiTE
% Author: Andrew Hughes

\chapter{The DynamiTE Framework}
\label{dynamite}

\section{Introduction}

In this chapter, we introduce DynamiTE, the Dynamic Theory Execution
framework.  This provides the solution we first proposed in
\ref{solution}, using the Nomadic Time (NT) calculus introduced in
\ref{nt} and \ref{tnt} as a foundation for application development.
Through using DynamiTE, programmers compose NT processes, realised as
Java objects, to create a working system.  The framework handles
running these processes, in parallel if necessary, and negotiates the
communication between them.  Both features are provided by leveraging
existing facilities in the underlying Java virtual machine and class
library.

Over the course of this chapter, we will describe how NT processes are
mapped onto Java objects (see \ref{dyn:maptheory}) and then show how
DynamiTE can be used to create an implementation of the application we
introduced in \ref{app:req} (see \ref{app:dynamite}).  But first, we
discuss why we chose Java as the host language for DynamiTE and what
advantages and disadvantages this brings to its implementation.

\section{Why Java?}

The first implementation of the Java programming language was released
in 1995 by Sun Microsystems.  It takes the form of a block structured
language with a syntax akin to C or C++.  However, unlike programs
written in those languages, Java applications tend to be compiled to
platform-independent Java bytecodes which are then executed by a Java
Virtual Machine or JVM.  This allows the same Java program to be
executed on multiple platforms without the need for recompilation.
With this new operating environment comes the removal of a number of
features found in Java's predecessors and the restriction of others,
with the aim of creating a safer and more portable language:

\begin{itemize}
\item \textbf{No pointer manipulation}.  All primitive types in Java
  (integer, floating point numbers, booleans and single characters)
  are passed by value.  All objects are stored and passed as pointers
  or references to their location in memory.  These pointers are
  immutable, removing the ability to perform pointer arthimetic
  (e.g. for iterating over arrays) and with it, a host of problems
  inherent with inappropriate memory access.  For example, attempts to
  use a \texttt{null} pointer are caught by the virtual machine and
  produce a checked exception, rather than causing a segmentation
  fault which brings down the entire process.
\item \textbf{All arrays are bounds checked}.  A major cause of errors
  and security issues in C and C++ programs is the possibility of
  buffer overflows, where programs write to memory beyond the end of
  an array.  In Java, such errors are prevented by the virtual
  machine; any attempt to access an index outside the bounds of an
  array causes a checked exception to be thrown and direct access to
  the array's memory is forbidden by the lack of pointer manipulation.
\item \textbf{All memory management is performed by a garbage
  collector}.  While allowing manual memory management allows the
  programmer greater control, it leads to an equivalent to the issue
  we saw with semaphores in \ref{semaphores}; every allocation must be
  paired with a later deallocation to avoid the possibility of an
  application leaking memory.  The problem is even more pronounced
  with regard to memory management as, while the acquisition and
  release of a lock tend to occur in close proximity to one another,
  allocation and deallocation can occur in quite disparate parts of
  the application.  In Java, memory is instead managed by a
  \emph{garbage collector} which allocates memory for objects as
  needed and periodically reclaims those that are no longer
  referenced.  The downside of this is that the garbage collector has
  to use processor time to perform its scans which would otherwise be
  used by the application.  However, as new garbage collection
  techniques, such as concurrent and generational collectors, become
  prevalent, this disadvantage is further outweighed by the prospect
  of chasing memory leaks.
\item \textbf{Lack of unsigned types}.  All integer types in Java use
  a bit to store the sign of the value, with no equivalent unsigned
  types that instead use this bit to store larger values.  This makes
  bitwise operations (and (\texttt{\&}), or(\texttt{|}) and
  not(\texttt{\~})) more inefficient as they need to operate on the
  type one size above (bytes ($2^8$) on shorts ($2^{16}$), shorts on
  ints ($2^{32}$), etc.).  Indeed, section 15.22.1 of the Java
  language specification\cite{javaspec} states that \emph{binary
    numeric promotion} (as defined in 5.6.2) should be applied to the
  operands, causing them to be converted to integer or long integer
  levels of precision before the operation is performed.  Thus, it
  logically follows that it is impossible to work with unsigned long
  integers ($2^64$) without resorting to the overhead of a class which
  implements arbitrary precision integers, such as
  \texttt{java.lang.BigInteger}.  Unsigned types continue to be
  proposed for addition to the Java language, but no such extension is
  scheduled for the next release (Java 7).
\end{itemize}

Although these changes are made at the expense of flexibility for the
programmer and possible efficency gains, they save time overall in
chasing bugs caused by memory allocation errors, buffer overflows or
leaks.  Besides, the Java Native Interface (JNI) can be used to
implement certain methods in C, should the need arise.  Many of the
methods provided by the Java class library do just that, usually to
make use of a platform-specific application programming interface
(API).  Doing so has some overhead and means losing the safety and
memory management benefits of Java, but is possible where necessary.

Performance has been a common criticism of Java, not just because of
these features but also because the Java bytecodes it uses must be
either interpreted or compiled into native code at run time.  This is
much less of an issue than it once was, due to advances in virtual
machine design and Just-In-Time (JIT) compilation techniques.
Theoretically, JIT compilation should eventually exceed the
performance of code compiled Ahead-Of-Time (AOT) as it can take
advantage of information only available at runtime.  This includes
knowing the exact platform on which the code will execute and being
able to make better optimisations based on statistics gathered through
execution (e.g. better branch prediction).  For example, HotSpot, the
virtual machine used by Sun's implementation of Java, only uses the
JIT compiler to create native code when it believes the code is used
enough (`hot' enough) to make doing so worthwhile.

Of these changes, the absence of unsigned types is the only one that
seems to have no advantage, other than simplifing the language.  Many
file formats and network protocols include unsigned types, so working
with them in Java becomes harder than is necessary.  Although their
absence may have made sense in earlier versions of the language, the
complexity of understanding unsigned arithmetic now seems trivial when
compared with the existential type system and its lack of reification
which was introduced by the addition of `\emph{generics}' in Java 5.
We thus hope that they may make an appearance in Java 8.

\subsection{Concurrency Provision}
\label{java:concurrency}

From the perspective of implementing DynamiTE, one advantage of Java
is its broad support for concurrency.  Java is one of the few
languages to have an implementation of \emph{monitors}, a feature we
demonstrated in \ref{semaphores}.  It has also had support for threads
from the very beginning, with support as a core part of the virtual
machine rather than as an auxillary library (the approach used for C).
Java's platform independence means that the same threading constructs
and semantics, as mandated by the VM specification\cite{vmspec}, can
be used across all operating systems supported by a Java virtual
machine.  The actual implementation is provided by the virtual machine
and class library, which may either map them on to native threads or
provide \emph{green} threads, where the virtual machine itself creates
and schedules threads.  The main disadvantage of the latter is that
blocking calls to the operating system performed by one thread will
cause the virtual machine and all its threads to be blocked; as the
system is unaware of the presence of the threads, its only option is
to block the entire VM process.  Green threads are however much faster
to create and synchronise, as everything takes place within the VM.
They can also match the required thread semantics exactly, rather than
having to map those provided by the operating system's threads.  While
earlier versions of Sun's implementation used green threads, native
threads are now used on all supported platforms.

The result of this early adoption of multithreading is that the
implementation in Java is reasonably mature and well-tested.  With
Java 5, this support was greatly expanded as a result of research led
by Doug Lea and incorporated into the Java platform via JSR166 and the
\texttt{java.util.concurrent} packages \cite{jsr166, concpractice}.

The extensions provided by JSR166 take the form of a host of new
classes, backed by support in the virtual machine.  The Java language
itself is not altered.  It provides support for:

\begin{itemize}
\item \textbf{Atomic Variables}.  These provide replacements for
  integer, long integer and reference fields which can be updated in
  an atomic fashion, are safer than \texttt{volatile} variables and
  more efficient than locking.  The Java memory model allows
  operations which alter the value of normal fields to be reordered by
  the VM as a form of optimisation, as long as this reordering is not
  visible from within the same thread.  However, this means that other
  threads may see the changes in the wrong order or not at all.
  Marking a field as \texttt{volatile} makes the VM aware that it may
  be accessed by multiple threads, causing updates to be made visible
  to all threads immediately.  However, \texttt{volatile} fields are
  still prone to race conditions when used in non-atomic operations
  such as incrementing a value or performing a conditional
  update\footnote{e.g. in \texttt{if (x == 4) x = 5}, it is possible
    for \texttt{x}'s value to have been changed by another thread
    before the assignment but after the comparison}.  The usual
  solution is to obtain a lock on the class every time the variable is
  altered; this provides both the update guarantees of a
  \texttt{volatile} variable and blocks other threads trying to obtain
  the same lock.  Atomic variables provide an alternate solution by
  allowing the processor's CAS operation (see \ref{semaphores}) to be
  used.  This is usually more efficient than locking the entire class,
  which will involve the VM performing a CAS operation on the lock at
  some point anyway.  While locking takes a pessimistic approach to
  thread safety by blocking all other threads, CAS operations are
  optimistic; the update is attempted, and if it fails, we try again
  until it succeeds.  Implementing such a check successfully is even
  more prone to error than locking, as the programmer has to ensure
  they check the result of the CAS and loop accordingly, but it is
  usually much more efficient when contention is low.  With the
  addition of atomic variables to Java, programmers now have the
  choice of using either.
  \item \textbf{Explicit Locks}.  As we saw in \ref{semaphores}, Java
    has implicit reentrant locking via the \texttt{synchronised}
    keyword.  Their use, however, is limited; there is only one lock
    per class, so all its variables must be protected by the same
    lock, and threads are always blocked until they either acquire the
    lock or the thread is interrupted by \texttt{Thread.interrupt()}.
    The \texttt{ReentrantLock} class provides a more advanced version
    with the following additional features:
    \begin{itemize}
      \item \texttt{tryLock()} can be called to perform a non-blocking
        acquisition of the lock.  It immediately returns with
        \texttt{true} if the lock was acquired, and \texttt{false} if
        it wasn't.
      \item \texttt{tryLock(long, TimeUnit)} can be called to perform
        a timed acquisition.  If the lock is available, it acquires it
        and returns immediately.  Otherwise, it blocks.  However,
        unlike the implicit lock provision and the \texttt{lock()}
        method, it will become unblocked after the given timeout and
        return \texttt{false}.
        \item The lock can operate in a fair mode, where threads
          acquire the lock in the order they requested it.  Both
          implicit and explicit locks default to unfair behaviour,
          which permits \emph{barging} if a new thread happens to
          request a lock when it is unheld.  Unfair locks are much
          faster\footnote{If threads are not allowed to jump the
            queue, then we end up blocking and descheduling a thread
            which could have quite happily acquired the lock but isn't
            allowed to do so because of the fairness policy}, but
          fairness is sometimes needed to ensure correctness.
    \end{itemize}
    A class can have multiple instances of an explicit lock, just like
    any other variable, and this benefit is utilised by
    \texttt{ReentrantReadWriteLock}.  This class provides both a
    shared (\textbf{read}) lock and an exclusive (\textbf{write})
    lock.  Multiple threads can acquire the read lock, but to acquire
    the write lock, both locks must be unheld.  This can be used to
    make classes more efficient when compared with the brute force
    approach of enforcing mutual exclusion for all operations.  For
    example, a collection class can allow multiple threads to read
    values as long as there is no thread altering the collection.
    Both locks, and other implementations such as \texttt{Semaphore},
    are based on \texttt{AbstractQueuedSynchronizer}\cite{aqs} which
    provides a common framework thread queues.
    \item \textbf{Explicit Condition Queues}.  As in the case of
      locks, Java already has its own implicit condition queues,
      accessible via the \texttt{wait}, \texttt{notify} and
      \texttt{notifyAll} methods.  These also have similar limitations
      to the implicit locks; only one queue is available per class and
      either one or all threads must be notified.  With only one
      condition queue, the usability of \texttt{notify} to alert a
      single thread is extremely limited; using it is dangerous if
      there is more than one condition as the wrong thread may be
      awoken, and it is inefficient unless a change in the condition
      means that one and only one thread may proceed.  The former can
      be observed in the buffer example of \ref{semaphores} where
      there are two conditions: \texttt{used == BUFFER\_SIZE} and
      \texttt{used == 0}. The latter is observable in a `gate'
      scenario where multiple threads queue up waiting for a condition
      to hold, and then all proceed when it does.  Explicit condition
      queues address these issues by allowing a class to have multiple
      condition queues.  Each \texttt{Condition} is obtained from a
      \texttt{Lock} by a call to \texttt{Lock.newCondition} and that
      same lock must be held when calling its methods.  In the buffer
      example, the synchronized blocks would be replaced by the use of
      explicit locks and the calls to \texttt{wait} and
      \texttt{notifyAll} by \texttt{await} and \texttt{signal} calls
      on one of two \texttt{Condition} objects.  The \texttt{signal}
      method can now be used rather than \texttt{signalAll}, awakening
      just one thread, as we know the thread will be waiting for the
      condition whose state has changed and no other.  This avoids
      waking all threads and having all but one go back to sleep.
    \item \textbf{Executors and Thread Pools}.  The new classes
      provide a framework for executing tasks in the form of the
      \texttt{Executor} interface.  This decouples the process of
      submitting a task from how it is executed.  Tasks (in the form
      of an object which implements the \texttt{Runnable} or
      \texttt{Callable} interface) are submitted to an
      \texttt{Executor} instance, and then performed in a manner
      determined by the \texttt{Executor} implementation.  The
      \texttt{Executors} class provides a number of pre-defined
      instances:
      \begin{itemize}
        \item A single thread executor, which performs tasks
          sequentially.
        \item An executor with a fixed size pool of threads.
        \item An executor with an unbounded pool that grows and
          shrinks as demand allows.
        \item An executor with a fixed size pool of threads and
          delayed or periodic task execution.
      \end{itemize}
      The programmer is also, of course, free to define their own
      implementation.  This feature is very useful for implementing
      parallel composition in DynamiTE as each process may be
      submitted to an executor, the choice of which is left up to the
      user of the framework.
    \item \textbf{New Collections}.  The standard Java collections
      apply an all-or-nothing approach to thread safety; either the
      instance is unsafe for multithreaded use (as with instances of
      the Java 1.2 classes -- \texttt{HashMap}, \texttt{ArrayList},
      etc.) or every method call locks the class (as with the legacy
      classes such as \texttt{Vector} and \texttt{Hashtable} or the
      1.2 classes when wrapped by the \texttt{synchronizedX} methods
      in \texttt{Collections}).  The JSR166 extensions provide a new
      set of collections which utilise the features listed above.  For
      example, \texttt{ConcurrentHashMap} provides a hash map which
      utilises \emph{lock striping}; the map is protected by multiple
      read and write locks which protect only a segment of the whole
      map each.  Thus, not only can multiple readers access the map
      concurrently, but it may be possible to perform multiple writes
      concurrently if they effect different areas of the map.  The new
      collections also include various \texttt{BlockingQueue}
      implementations, which implement the producer-consumer model we
      demonstrated with the buffer example in \ref{semaphores}.  One
      such implementation is \texttt{SynchronousQueue} which closely
      matches the semantics of synchronous channels in Nomadic Time;
      it has no storage so a thread performing a \texttt{put} blocks
      until a receiving thread calls \texttt{take}.
\end{itemize}

With these additions, the programmer is given a lot of control and
flexibility when implementing concurrent programs in Java, and we will
leverage many of these features when implementing DynamiTE.  Having
essential components such as locks and concurrent collections already
available and well tested makes it much easier to meet the
requirements of the framework.

Other languages are not so lucky.  In C and C++, threads are provided
by an operating system library and thus vary depending on platform.
The POSIX standard for threads attempts to overcome this by providing
a standard threading interface and semantics for POSIX systems.  While
POSIX-based systems including GNU/Linux, Solaris, FreeBSD and Mac OS X
all provide implementations. the problem remains with systems that do
not provide such by default, notably Microsoft Windows.

Haskell has been slow to introduce threading support.  Although the
Concurrent Haskell\cite{conchaskell} extension was originally proposed
in 1996, it does not form part of the Haskell 98 standard and the GHC
documentation still lists it as experimental.  Both Hugs and the
Glasgow Haskell Compiler (GHC), the two main implementations of
Haskell, provide an implementation of Concurrent Haskell's
\texttt{Control.Concurrent} module, they do so using green threads.
As mentioned above, while these are faster than native threads,
blocking calls to the operating system, such as I/O, will cause all
threads to be blocked.  A workaround is provided in GHC when it is
built with the \texttt{-threaded} option; it uses a pool of worker
threads to execute Haskell code and switches to a new one when a
\texttt{safe} foreign call is made.  It also allows native threads via
\texttt{forkOS} when built in this manner.  As with C, this makes
Haskell's thread behaviour dependent on the underlying system as
opposed to providing a standard set of operations and semantics;
whether threads are provided and how well they perform depends
entirely on which implementation of Haskell is being used.

However, functional languages in general should be a good basis for
concurrency.  They already operate in a task-oriented manner through
\emph{pure functions}; data is fed in, manipulated as desired and the
result output without altering memory.  Those that do alter memory,
and thus could lead to concurrency issues, are clearly denoted
(e.g. by monads in Haskell), reducing the amount of code that has to
be checked for race conditions.

It is thus a pity that they are not more widely used and their
concurrency facilities not more well developed.  This is changing,
however.  GHC has recently been extended with support for Software
Transactional Memory (STM) \cite{haskellstm}, which provides a new
\texttt{atomic} function and \texttt{STM} monad for implementing
transactions.  The STM logs all actions and then performs a single
atomic commit, provided there are no conflicts with other updates.
This allows Haskell programmers to compose new atomic transactions
from others, and moves the need to ensure atomicity away from each
individual function to the caller, who can only invoke them from
within an atomic environment.

Erlang\cite{erlang} is another interesting case, as both it and
DynamiTE focus on message passing between processes as opposed to
shared data and locking.  Erlang differs in that it operates
asynchronously, collecting messages in a mailbox on a per-process
basis and filtering which ones are received in any one operation.
However, synchronous delivery can be implemented by requiring messages
to be acknowledged.  The main limitation of current Erlang
implementations is that they use green processes; unlike green
threads, these don't share state but they do have the same downside
that a blocking system call from one will cause them all to become
blocked by the system.

Both Erlang and Haskell provide an interesting environment in which to
implement a framework like DynamiTE.  Indeed, we hope that the
majority of the design explained here in \ref{dyn:maptheory} can be
applied to most languages with sufficient threading support.  However,
there is another reason for our choice of Java as the initial
prototype language.

\subsection{Popularity}

Popularity is rarely a good reason to do anything but, in combatting
developer inertia, it is a good weapon to have.  The simple fact is
that most of today's developers know Java and sometimes little else;
it (or its close relative, C\#) is taught as part of most computer
science degrees and is used as the language of choice for many
applications, especially in the area of enterprise web applications.

As we discussed in \ref{solution}, easing the barriers for adoption is
an essential aspect in influencing developers to try something new.
With DynamiTE, we are already advocating the idea of using message
passing rather than state manipulation to Java developers, a body of
programmers who will generally be more familiar with object-oriented
design techniques which focus on manipulating data.  Adding the
prospect of learning an entirely new language is not going to help our
case, and we believe this to be the main reason other solutions have
not moved far beyond their academic roots.  Instead, DynamiTE is
developed as a Java class library like any other, which leverages
standard features of the Java platform and which can be further
developed by the very people that use it.

We will be the first to admit that Java has issues; its age means that
with hindsight many design decisions can now be seen as flawed and
attempting to change this leads us to consider the bane of all
programming languages -- backwards compatibility.  Most features, good
or bad, are now enshrined in the language and further development
rightly takes a conservative attitude to avoid breaking the huge body
of existing code already in use.  This means that APIs are deprecated
rather than removed, causing the class library to become more bloated
than ever, and new language features such as generics take years to
appear and even then have to be limited.  No consensus has yet been
reached on how closures should be implemented, so they will not appear
in Java 7 either.  These issues are here to stay; Java is unlikely to
ever have a type system as advanced as that of most functional
languages or a separation between pure and impure functions.  But with
these come maturity and a vast body of developers which we believe to
be far more useful in achieving our goal than the possibilities of a
perfect but niche language.

\section{Mapping Theory to Practicality}
\label{dyn:maptheory}

In this section, we show how the syntactic constructs of NT introduced
in chapter \ref{nt} are mapped on to Java classes by the DynamiTE
framework.  Within DynamiTE, developers can create concurrent
applications simply by implementing the specific behaviour they
require in appropriate subclasses.  Recall the syntax of NT from
\ref{eqn:tnt-syntax}:

\begin{equation}
  \begin{aligned}
    \expr, \exprb \quad \mathrel{::=} \quad &
      \nil  \mid
      \Omega \mid
      \Delta \mid
      \Delta_{\sigma} \mid
      \alpha . \expr  \mid
      \expr + \exprb \mid
      \expr \mathrel{\!|\!} \exprb \mid
      \timeout{\expr}{\sigma}{\exprb} \mid \\
    & \stimeout{\expr}{\sigma}{\exprb} \mid 
      \mu X . \expr \mid
      X \mid 
      \expr \res{A} \mid
      \locv{m}{\expr}{\exprb}{\vec{\sigma}} \mid
      \ambop . \expr \\
   \ambop \quad \mathrel{::=} \quad & \tntin{m} \mid \tntout{m} \mid \tntopen{m} \mid
      \procin{\beta}{m} \mid \procout{\beta}{m} \mid \bin \mid
      \bout \mid \bopen
   \end{aligned}
\end{equation}

Each process term ($\expr, \exprb$) above becomes a class that implements \texttt{Process}:

\begin{verbatim}
public interface Process
  extends State
{
  Set<Transition> getPossibleTransitions();
  Process substitute(String var, Process proc);
}
\end{verbatim}

Operation follows a top-down approach; the complete system is
represented by a single instance of one of these classes which, in
most cases, will be an operator that composes together further
instances as appropriate.

The \texttt{Process} interface itself extends the marker interface,
\texttt{State}.  This, along with \texttt{Transition}, forms part of
our implementation of a labelled transition system, found under the
\texttt{lts} subpackage.  By making implementations of
\texttt{Process} also implement \texttt{State}, they can be used as
the start and finish state in the transitions represented by the
\texttt{Transition} class.  The label used by the transition is
provided by a subclass of \texttt{Action}, which also allows for the
possibility of side effects, which we cover in \ref{dyn:evolvers}.

Each \texttt{Process} is required to implement
\texttt{getPossibleTransitions()} and it is in this method where the
operational semantics found in tables \ref{tab:casesubset} and
\ref{tab:mobsubset} are realised in Java code.  The simplest
implementation is found in the representation of $\Delta$, realised as
the class \texttt{Delta}, as it has no transitions.

\begin{verbatim}
  public Set<Transition> getPossibleTransitions()
  {
    return Collections.emptySet();
  }
\end{verbatim}

The other method in \texttt{Process},
\texttt{substitute(String,Process)} is primarily used to implement
recursion.  The arguments passed to \texttt{substitute} are the
variable name and the process bound to that name respectively, and the
implementation is expected to return the same process with this
substitution applied.

We implement substitution in this manner so that it is independent of
the syntax of the calculus.  Nomadic Time is implemented in DynamiTE
by deriving from classes which implement CaSE, which in turn derive
from those implementing CCS.  Of these, only CCS has an implementation
of recursion:

\begin{verbatim}
  public Set<Transition> getPossibleTransitions()
  {
    Set<Transition> trans = new HashSet<Transition>();
    for (Transition t : proc.getPossibleTransitions())
      {
        Process end = (Process) t.getFinish();
        trans.add(new Transition(this,
                                 end.substitute(var, this),
                                 t.getAction()));
      }
    return trans;
  }
\end{verbatim}

\noindent as the same rule is used in all three calculi.  Because the
new constructs in CaSE and Nomadic Time all provide an implementation
of \texttt{substitute}, the call to \texttt{substitute} in the CCS
implementation of recursion (provided by a class called \texttt{Rec}
with variables \texttt{proc} and \texttt{var}) will still work, even
when one of these forms the final state, \texttt{end}.

The above implementation of recursion highlights a common pattern in
the semantics, which is visible both in their formal representation
and the Java version; the rules reference one or more component
processes, and create new transitions based on the transitions of
these processes.  As a result, most of the implementations of
\texttt{getPossibleTransitions()} in DynamiTE operate by looping over
the set of transitions from each component process, checking if they
meet the prerequisties for one of the rules and then creating new
transitions.

Recursion is probably one of the simplest examples of this.  From its
semantics,
\begin{center}
      \Rule{Rec}
      {E \derives{\gamma} E'}
      {\mu X.E \derives{\gamma} E' \{ \mu X.E / X\}}
      {}
\end{center}
\noindent we can see that it applies its transformation to all
transitions ($\gamma$ ranges over all possible labels), and the only
change it makes is to apply substitution to $E'$, represented in Java
as the final state of the \texttt{Transition} object
(\texttt{t.getFinish()}).  Thus, all the new transitions returned
perform the same action (\texttt{t.getAction()}), have the current
instance of \texttt{Rec} as the start state and a final state derived
from the original via substitution.

Implementing the $\mid$ operator, via the \texttt{Par} class, is a
more involved task.  In CCS alone, $\mid$ features in three of its
operational rules: $Par1$, $Par2$ and $Par3$ (see table
\ref{tab:ccssemantics}).  CaSE adds a further rule, $Par4$, to deal
with the passage of time over the operator (see table
\ref{tab:casesemantics}).  With Nomadic Time, the first two rules are
combined due to structural congruence\footnote{This makes no
  difference to the implementation; it merely cuts down on the number
  of rules that need to be listed in the semantics.  The case of $F
  \derives{\alpha}$, which is missing in the rules for Nomadic Time,
  is handled by a combination of $StrPar1$ ($E \pc F \equiv F \pc E$)
  and $SCong$}, but a further five are introduced ($InEnv$, $OutEnv$,
$Open$, $ProcIn$ and $ProcOut$) to handle mobility.  All of these are
handled in much the same way as $Par3$ (inspect the composed processes
and their transitions, and apply as required) so we will just look at
the CCS implementation here for brevity:

\begin{verbatim}
  public Set<Transition> getPossibleTransitions()
  {
    Set<Transition> trans = new HashSet<Transition>();
    // Par1
    for (Transition t : left.getPossibleTransitions())
      {
        Process nextLeft = (Process) t.getFinish();
        trans.add(new Transition(this,
                                 new Par(nextLeft, right),
                                 t.getAction()));
      }
    // Par2
    for (Transition t : right.getPossibleTransitions())
      {
        Process nextRight = (Process) t.getFinish();
        trans.add(new Transition(this,
                                 new Par(left, nextRight),
                                 t.getAction()));
      }
    // Now find pairs for synchronisation (Par3)
    Set<Transition> syncTrans = new HashSet<Transition>();
    for (Transition t : trans)
      {
        String label = t.getAction().getLabel().getText();
        if (Context.getContext().isRegisteredName(label))
          {
            for (Transition t2 : trans)
              {
                String label2 = t2.getAction().getLabel().getText();
                if (Context.isConame(label2) &&
                    label.equals(Context.convertLabelToName(label2)))
                  {
                    Par finish1 = (Par) t.getFinish();
                    Par finish2 = (Par) t2.getFinish();
                    Action sync = new Sync(t, t2);
                    if (!finish1.left.equals(left) &&
                        !finish2.right.equals(right))
                      syncTrans.add(new Transition(this,
                        new Par(finish1.left, finish2.right),
                        sync);
                    else if (!finish1.right.equals(right) &&
                             !finish2.left.equals(left))
                      syncTrans.add(new Transition(this,
                        new Par(finish2.left, finish1.right),
                        sync);
                  }
              }
          }
      }
    trans.addAll(syncTrans);
    return trans;
  }
\end{verbatim}

The class \texttt{Par} maintains references to the two composed
processes as \texttt{left} and \texttt{right}.  Thus, $Par1$ and
$Par2$ are implemented by iterating over the transitions of these
processes, as with recursion.  For each original transition, each
iteration produces a new transition, with the \texttt{Par} instance as
the start state, the same transition action as the original
(\texttt{t.getAction()} and a new \texttt{Par} instance as the final
state, where one argument is the unchanged process (either
\texttt{left} or \texttt{right}) and the other is the final state of
the original transition.  Looking at $Par1$,

\begin{center}
     \Rule{Par1}
     {E \derives{\alpha} E^\prime}
     {E \;|\; F \derives{\alpha} E^\prime \;|\; F}
     {}
\end{center}

\noindent it should be clear how this corresponds to the behaviour
described there, if $E$ is \texttt{left} and $F$ is \texttt{right}.

The majority of the method is spent handling $Par3$:

\begin{center}
      \Rule{Par3}
      {E \derives{a} E^\prime,
        F \derives{\overline{a}} F^\prime}
      {E \;|\; F \derives{\tau} E^\prime \;|\; F^\prime}
      {}
\end{center}

\noindent which represents synchronisation.  The implementation loops
over the set of new transitions\footnote{We could equally loop over
  the transitions of \texttt{left} and \texttt{right}, but we'd then
  need to store these together in another new set.  It seems slightly
  more efficient to iterate over the new transitions and decompose
  them as needed, adding any resulting transitions to a new set which
  is then added to the other new transitions at the end.}, searching
for a process which performs a name ($E \derives{a} E^\prime$).  For
each one it finds, it iterates over the transitions again, this time
in search of the corresponding coname.  If it finds a match, and both
transitions originate from different processes, then it creates a new
$\tau$ transition, using this instance of \texttt{Par} as the start
state, an instance of \texttt{Sync} as the action and a final state
created by joining together the two final states from each transition
in a new \texttt{Par} instance.  The two transitions from which the
transition was derived, \texttt{t} and \texttt{t2}, are stored by the
\texttt{Sync} action.

The \texttt{Par} implementations for CaSE and Nomadic Time are created
by extending this class and creating further transitions, based on
their additional rules.  The other constructs, including $+$, the
timeout operators and clock hiding are also implemented in much the
same way.  DynamiTE itself is concerned with more than just generating
the transitions of a process from its semantics, however, and in the
next section we see how this is handled by considering a class
introduced in the implementation of \texttt{Par} above:
\texttt{Context}.

\section{The Context of the Calculus}
\label{dyn:context}

Beyond the operational semantics, there are two important issues
involved in representing a process calculus programatically:

\begin{enumerate}
\item We need to know the \emph{context} in which algebraic
  constructions in the calculus will operate.  This includes the sets
  of names and co-names (from CCS), the set of clocks (from CaSE) and
  the environ names (from Nomadic Time); the foundations on which our
  semantic rules are built.  Although this is not essential for
  implementing CCS, where the names and co-names appear as part of the
  prefix construct $\alpha.E$, it is a necessary part of both CaSE and
  Nomadic Time.  Both these calculi have rules defined with respect to
  the set of clocks, but this set is not defined by other constructs
  in the calculus.  Instead, time is always present and we need to
  know the set of clocks to derive even the transitions for the $\nil$
  process using the $Idle$ rule.  The \texttt{Context} class maintains
  these, and more, in DynamiTE and we will look at this in more depth
  in the remainder of this section.
\item We need to know the \emph{execution semantics} for each process.
  These answer questions such as: what happens when there are multiple
  transitions from a particular process? And are there any
  side-effects to performing a transition?  In DynamiTE, these
  semantics are encoded using the \texttt{Evolver} franework, which we
  cover in \ref{dyn:evolvers}.
\end{enumerate}

In the implementation of \texttt{Par} in the previous section, we saw
how the current \texttt{Context} instance could be used to find out
whether or not a label referred to a name, using
\texttt{isRegisteredName(String)}.  In our implementation of CCS, all
names and co-names are registered with the \texttt{Context}.  This
happens automatically on the user's behalf as part of the construction
of a \texttt{Name} or \texttt{Coname} instance, or as part of
generating the transitions for the renaming operation, $E[f]$.  This
centralised checking of names and co-names gives two primary
advantages over just allowing the use of any arbitrary string:

\begin{enumerate}
\item We can prevent the silent action, $\tau$, being used as a name
  or co-name.  In CaSE and Nomadic Time, we can also prevent conflicts
  with clock names, locality names and the new mobility primitives
  such as $\tntin$.
\item We can enforce registration (and thus existence of the name or
  co-name) as a pre-requisite for other methods.  This is especially
  useful in working with channels (see \ref{dyn:plugin}).
\end{enumerate}

As a repository for names and co-names, \texttt{Context} becomes an
appropriate place for other methods related to their use.  As a
result, it also includes a number of static utility methods which are
used with co-names; these are \texttt{convertLabelToName(String)},
\texttt{convertConameToLabel(String)} and \texttt{isConame(String)}.
In our implementation, we differentiate names from co-names by marking
each letter with a combining macron (so $a$ becomes $\overline{a}$).
\texttt{convertConameToLabel(String)} creates these labels from the
original name, and \texttt{convertLabelToName(String)} returns the
original name by removing the macrons.  The \texttt{isConame(String)}
method is a simple method which just checks to see if any macrons are
present in the name.  The benefit of abstracting all these methods out
into the \texttt{Context} class is that we can later change the way
co-names are represented by modifying just one class.

The other main use for the \texttt{Context} is as the user's interface
to the plugin framework.  We have already touched on how DynamiTE
supplies implementations for multiple calculi: CCS, CaSE and Nomadic
Time.  One way this is made possible is by making the implementation
as generic as possible; we avoid relying on the specific structure or
types in a particular calculus, separating them out into methods which
can be overridden by other implementations as with
\texttt{substitute(String,Process)}.  Another aspect of this is
allowing the user to select which calculus they want to work with at
run-time.  This is made possible by a plugin framework.

\subsection{The Plugin Abstraction}
\label{dyn:plugin}

You may have noticed that, in the \texttt{Par} implementation in
\ref{dyn:maptheory}, we obtain an instance of \texttt{Context} not by
calling a construct but using \texttt{Context.getContext()}.  In using
DynamiTE, only one instance of \texttt{Context} usually
exists\footnote{Should DynamiTE be used across multiple host virtual
  machines, then there may be multiple instances, and it will be these
  that allow communication between each host.}.  This is created at
startup by a \texttt{ContextFactory}.  As with \texttt{Context}, an
instance of \texttt{ContextFactory} is obtained using a static method
rather than a constructor.  The user calls
\texttt{ContextFactory.getInstance(String,String,String)}, supplying
the name of a process calculus, a \emph{channel implementation} and a
\emph{locality implementation}.  The two latter arguments are used to
determine the execution semantics for synchronisation and movement
respectively.  The returned \texttt{ContextFactory} will be able to
supply a \texttt{Context} instance for the specified process calculus
which uses the given channel and locality implementations.  If one can
not be found, a \texttt{UnsupportedContextException} is thrown.

DynamiTE supplies an implementation of \texttt{ContextFactory} in the
form of the \texttt{DynamiTEContextFactory}.  It in turn probes for
classes which implement \texttt{Calculus}, \texttt{ChannelFactory} and
\texttt{LocalityFactory} and an instance of it is returned to the user
if it finds an implementation of each which meets the user's
requirements.  Once the user has obtained an instance of
\texttt{ContextFactory}, they can call \texttt{getContext} on it to
return a \texttt{Context} instance, which can then be supplied to the
\texttt{Context.setContext(Context)} method to later be returned by
\texttt{Context.getContext()}.

Although this may sound convoluted and unnecessary, it makes the
framework much more flexible and ready for future extension, while
giving the user greater freedom of choice.  A new calculus can be
implemented simply by creating an implementation of \texttt{Process}
for each construct and an instance of \texttt{Calculus}.  The same
goes for new channel and locality implementations, and the option is
there for the entire \texttt{ContextFactory} to be replaced if needed.
From the user's perspective, everything can be handled in a single
line of code:

\begin{verbatim}
Context.setContext(ContextFactory.getInstance("CCS",
  "threaded", "dummy").getContext());
\end{verbatim}

\noindent which supplies an implementation of CCS with threaded
channels and a dummy locality implementation.

One advantage of using Java is that support for dynamically probing
for implementations at runtime is built into its class library.  The
library itself already includes a number of frameworks which work in
this fashion (including image I/O, sound and XML support) and the
\texttt{java.util.ServiceLoader} API provided in 1.6 makes it easy for
developers to define new ones.  In the \texttt{plugin} subpackage,
DynamiTE provides a means of using this API to support plugins:

\begin{verbatim}
public static <T extends Probeable> Map<String,T>
  probePlugins(ServiceLoader<T> sl)
{
  Map<String,T> map = new HashMap<String,T>();
  for (T probeable : sl)
    {
      Config.logger.config(String.format("Loaded plugin: " +
        "%s %d.%d.%d%s", probeable.getName(),
        probeable.getMajorVersion(), probeable.getMinorVersion(),
        probeable.getMicroVersion(), probeable.getAdditionalInfo()));
      map.put(probeable.getName(), probeable);
    }
  return Collections.unmodifiableMap(map);
}
\end{verbatim}

\noindent Plugins are required to implement the interface
\texttt{Probeable}, so that the name and version information can be
obtained programatically.  In DynamiTE, the \texttt{Calculus},
\texttt{ChannelFactory} and \texttt{LocalityFactory} interfaces all
extend \texttt{Probeable} so \texttt{DynamiTEContextFactory} need only
call the above method with an appropriate service loader for the
interface and it will receive back a \texttt{Map} linking names to
instances of implementations of that interface.  Most of the actual
work is done by \texttt{ServiceLoader} which reads from a text file
named after the interface, which contains a list of implementing
classes.  The DynamiTE framework supplies such text files for its
implementations of \texttt{ContextFactory}, \texttt{Calculus},
\texttt{ChannelFactory} and \texttt{LocalityFactory}.  The
\texttt{ServiceLoader} loads a listed class each time its
\texttt{next()} method is called\footnote{This happens indirectly in
  \texttt{probePlugins} via each iteration of the for-each loop}, and
returns an instance of it, which \texttt{probePlugins} then stores in
the map.

The \texttt{Context} class stores and provides indirect access to the
implementations chosen via the \texttt{ContextFactory}.  For instance,
calling \texttt{getSyntax()} on the current \texttt{Context} instance
will return the set of syntactic constructs which form the calculus
currently in use.  The \texttt{Calculus} instance is also used in the
process of registering a name; the \texttt{Context} calls the
\texttt{Calculus} implementation to obtain the transition label for
the name, which gives the \texttt{Calculus} chance to veto the choice.
This is used by the \texttt{CCS} class to prevent $\tau$ being used as
a name, by returning an instance of \texttt{CCSLabel}, the constructor
of which contains the following check:

\begin{verbatim}
if (label.equals(TAU))
  throw new IllegalArgumentException(TAU +
                                     " is a reserved label.");
\end{verbatim}

\noindent where \texttt{TAU} is a unique instance of
\texttt{CCSLabel}.  The other instances maintained by \texttt{Context}
are used in the execution semantics which we will cover next.

\section{The Evolver Framework}
\label{dyn:evolvers}

For DynamiTE to actually be useful to users for building concurrent
applications, it needs to do something more than just evaluating an
algebraic construct and providing the possible transitions according
to the semantics of the calculus.  For an application, such as our
music player example (see \ref{app:req} and \ref{app:nt}), to actually
work, the user needs to be able to define their own internal behaviour
and share the results.

The evolver and channel frameworks provide this facility.  An
implementation of the \texttt{Evolver} interface implements the method
\texttt{evolve(Process)} according to its own particular execution
semantics.  Through this method, it is the \texttt{Evolver} instance
that makes decisions such as which transition to follow to find the
next state and also whether to process any side effects.  Side effects
take the form of additional methods which may optionally be called
after a transition has been followed.  As we saw in
\ref{dyn:maptheory}, each transition references an \texttt{Action};
this is an abstract class which provides a method \texttt{perform()}
for the purpose of implementing side effects.

DynamiTE provides a simple implementation of \texttt{Evolver} called
\texttt{Simulator} which ignores side effects.  While this is of
little use for applications, it is useful for testing design
constructs as it allows the possible transitions from a process to be
visualised.  All \texttt{Simulator} does is take a \texttt{Process}
and loop over its transitions, calling itself recursively with each
final state.  In this way, it explores the possible transitions in a
depth-first manner, until it reaches a process with either no
transitions or where all transitions have equal start and end states.
The latter condition prevents it looping forever over states with just
clock transitions or simple forms of infinite recursion, such as $\mu
X.a.X$.

\begin{verbatim}
  public void evolve(Process p)
  {
    System.out.println("Evolving process: " + p);
    Set<Transition> trans = p.getPossibleTransitions();
    System.out.println("Possible transitions: " + trans);
    for (Transition t : trans)
      {
        State f = t.getFinish();
        if (f instanceof Process)
          {
            if (f.equals(p))
              System.out.println("Not following transition " + t);
            else
              {
                System.out.println("Following transition " + t);
                evolve((Process) f);
              }
          }
      }
  }
\end{verbatim}

A more practical \texttt{Evolver} is a more complex undertaking.  It
has to make choices over which transition to pick when several are
presented; although CaSE and Nomadic Time have a notion of priority in
maximal progress, choices must still be made between the ticks of
different clocks, or between a clock tick and an action.  These
choices form the execution semantics of an \texttt{Evolver}
implementation, and there is plenty of room for further
experimentation in this area.

Returning to the notion of side effects, the channel framework is
accessed through four subclasses of \texttt{Action}, three of which
are also used in \texttt{Prefix}, the implementation of $\alpha.E$:

\begin{enumerate}
\item The class \texttt{Name} is used to represent the use of a name
  as part of the process $\alpha.E$.  It is also a subclass of
  \texttt{Action} and implements \texttt{perform()} by reading from an
  \texttt{InputChannel} and storing the result.
\item Likewise, the class \texttt{Coname} represents the use of a
  co-name in $\alpha.E$ and implements \texttt{perform()} by
  retrieving a value from storage and transmitting it over an
  \texttt{OutputChannel}.
\item The \texttt{Tau} class is the last of the three classes used in
  $\alpha.E$ and is used for the internal action, $\tau$.
  Implementing \texttt{perform()} for \texttt{Tau} is left to the
  user, who can use it to implement arbitrary sequential behaviour as
  required.
\item The \texttt{Sync} class is created through \texttt{Par} (see
  \ref{dyn:maptheory}) and is a subclass of \texttt{Tau} which
  implements \texttt{perform()} using the two synchronising
  transitions provided to it on construction.
\end{enumerate}

The \texttt{InputChannel} and \texttt{OutputChannel} instances are
obtained from the \texttt{ChannelFactory}, via the \texttt{Context},
and provide \texttt{read()} and \texttt{write(Object)} methods
respectively.  Although there is currently no realisation of data
within the formal layer of the calculus, this only matters to the
extent that we wish transmitted data to alter the constructs
themselves via substitution\footnote{The $\pi$ calculus (see
  \ref{picalculus}) is an obvious example of such behaviour, which
  goes to the extreme of not only allowing data to be transferred but
  also references to channels which can then later be used in the
  language constructs.  This, in essence, provides the form of
  mobility present in the $\pi$ calculus.}.  Data can be transferred
between processes and used within internal actions without having to
be explicitly realised at the formal level.  The operation of these
I/O operations, and the creation of a suitable environment in which
this may happen, is left to the implementation of the
\texttt{ChannelFactory} and there are a multitude of ways of doing so.
These range from simple mechanisms like files and sockets to complex
interprocess communication protocols such as Java's Remote Method
Invocation (RMI), the Common Object Request Broker Architecture
(CORBA) and web services.  The plugin nature of the channel
architecture means that any of these possibilities may be used and
more besides.

DynamiTE provides a sample implementation,
\texttt{ThreadedChannelFactory}, built on a \texttt{SynchronousQueue};
the \texttt{read} and \texttt{write} operations are performed by
synchronising two different threads and transferring the data
directly.  Thus, if \texttt{read} is called, and another thread is not
already waiting in the \texttt{write} method, it will block until this
is the case.  The same is true for \texttt{write}.  As noted above,
the \texttt{ChannelFactory} is also responsible for creating the
necessary environment for these operations, so the
\texttt{ThreadedChannelFactory} has to ensure that appropriate threads
are created and used.  A hook, \texttt{runInParallel(Process,
  Process)} is used for this purpose and is called indirectly by the
constructor of \texttt{Par}.

Data storage is also provided by the \texttt{ChannelFactory}.  The
channel I/O operations are automated side-effects of following a
transition labelled with a name or co-name, so user code (implemented
in a \texttt{Tau} subclass) must be able to access any values
retrieved and store new ones when it is itself performed.  The
\texttt{ChannelFactory} provides storage repositories, keyed by the
name of the channel, for this purpose.  When a user wishes to transmit
a value, they call \texttt{store(String,Object)}, where the first
argument is the channel name and the second the data to store.  Later,
the performance of a \texttt{Coname} with that channel name will
lookup the data and transmit it to the corresponding \texttt{Name}.
User code in a further \texttt{Tau} implementation can then retrieve
this using \texttt{retrieve(String)}, passing to it the name of the
channel and receiving back the data.  The
\texttt{ThreadedChannelFactory} implements this by storing data in a
\texttt{ThreadLocal}, so that it is only retrievable by the same
thread that stored it:

\begin{verbatim}
  public void store(String name, Object data)
  {
    ThreadLocal<Object> store = repositories.get(name);
    if (store == null)
      {
        ThreadLocal<Object> newStore = new ThreadLocal<Object>();
        store = repositories.putIfAbsent(name, newStore);
        if (store == null)
          store = newStore;
      }
    store.set(data);
  }
\end{verbatim}

The \texttt{repositories} variable stores an instance of
\texttt{ConcurrentHashMap}, which guarantees that retrieval operations
will always return results which reflect the most recently completed
operations.  As we described in \ref{java:concurrency},
\texttt{ConcurrentHashMap} is thread-safe while also being much more
efficient than \texttt{Hashtable} which locks the entire collection on
any operation.  These guarantees do not, however, help in performing a
sequence of operations on the map; specifically, the map may still be
changed by another thread in the interim period between performing a
\texttt{get} and a \texttt{put}.

This applies to the method above, as we need to check whether or not a
\texttt{ThreadLocal} instance has been created for a particular
channel name, and if not, create one.  The initial \texttt{get}
operation will return \texttt{null} if we have not yet created a mapping
between that channel name and a \texttt{ThreadLocal}.  However, in the
time it takes for this thread to check the return value and create a
\texttt{ThreadLocal}, another thread may have added such a mapping.
The \texttt{ConcurrentMap} interface (which \texttt{ConcurrentHashMap}
implements) provides an additional operation for just this issue:
\texttt{putIfAbsent}.  This provides an atomic version of the
following code:

\begin{verbatim}
if (map.containsKey(key))
  return map.get(key);
else
  return map.put(key, value);
\end{verbatim}

If, as in the most likely case, a mapping has not been added since our
initial \texttt{get}, then the \texttt{putIfAbsent} call folds down to
a normal \texttt{put} call.  The \texttt{put} method returns either
the previous value of the mapping or \texttt{null} if there wasn't
one.  In this case, it will always return \texttt{null} as the
\texttt{containsKey} call has already ensured no mapping exists.
Thus, if \texttt{null} is returned, we know that our new
\texttt{ThreadLocal} was used for the mapping.  If the return value is
not \texttt{null}, then another thread managed to add a mapping before
us, and the \texttt{ThreadLocal} we created is simply discarded when
we leave the scope of the \texttt{if} block in which it was created.

The complexity of this operation again shows how difficult it can be
to make operations involving shared objects thread-safe.  The
advantage of using DynamiTE is that the user doesn't have to come into
contact with this.  They merely store a value and the framework
handles the thread safety issues involved.  Providing these kind of
reusable generic constructs makes it much easier to build concurrent
applications that are thread-safe.

From this discussion, it should now be clear how concurrent
applications are created in DynamiTE; the user constructs
\texttt{Process} instances which match their design constructed in the
process calculus, and provides \texttt{Tau} subclasses for the actual
work performed by the application.  Data is passed between
\texttt{Tau} subclasses via the channel framework.

DynamiTE also provides hooks for a locality framework, which allows
side effects to take place on the performance of mobility actions,
just as the channel framework provides side effects for
synchronisation.  At present, DynamiTE simply has a dummy
implementation for this, but this has great scope for being used to
implement process migration.  Migrating an active process is not a
simple operation -- not only must the continuation of the code be
transferred, but any local data must also migrate.  Using the Nomadic
Time process calculus as our basis allow us to achieve a significant
amount of simplification here; the transferred process is already
separated from other code within the system by virtue of the moving
process being in the form of a \texttt{Prefix} instance.  When the
action is matched to the one used for the mobility operation, the
\texttt{Process} instance is transferred to its new location.  There
is no need to deal with code that is currently being executed.  We
also know exactly what data to be transferred, as this is centrally
managed by the channel framework.  Again, this is an interesting area
for future work.

Now we have explored DynamiTE, the next section provides a walkthrough
example of creating an application using the framework, following on
from \ref{app:req} and \ref{app:nt}.  We also compare this
implementation with a standard implementation using low-level
concurrency primitives, thus allowing us to evaluate whether our
claims made in \ref{solution} hold true.

\section{A Prototypical Application in DynamiTE}
\label{app:dynamite}
                                   
\section{Related Work}
\label{dyn:relatedwork}

In this final section, we look briefly at the existing body of
research concerned with providing concurrent frameworks, including
those based on process calculi.  

The $\pi$ calculus has been the subject of much of this work, primarily
due to its status as the most prevalant mobile process calculus.  Obliq
\cite{obliq} and Pict \cite{daveturner:phd} are both programming
languages with semantics founded in the $\pi$ calculus, while Nomadic
Pict \cite{wojciechowski:phd} takes this further, introducing
distribution not usually present in the $\pi$ calculus.  Within research
related to the ambient calculus, a machine framework (PAN
\cite{sangiorgi:safeambientsmachine}) has been developed and
implemented.  Process calculi, such as the Seal calculus \cite{seal}
have also been developed specifically to provide a formal framework for
a distributed implementation.

We believe our work to be novel in approaching the implementation of a
calculus with both global discrete time and mobility.

\section{Conclusion}

To conclude, we have presented the overall structure of the DynamiTE
framework for concurrent systems, including the process framework for
implementing operational semantics and the evolver framework for
working with execution semantics.  We believe that the framework
provides a unique way of developing concurrent systems.  It provides
features which have already proved advantageous in a theoretical
setting, such as the n-ary process synchronisation mechanism described
in chapter \ref{globsync}.  The existence of a formal theory for
DynamiTE's behaviour gives many advantages over more ad-hoc
approaches, allowing the underlying design to be rigorously examined
before being applied to the implementation.  The behaviour of the
system may be established clearly and unambiguously in the underlying
process calculus before implementation even begins, giving a solid
grounding on top of which the individual tasks may be developed.

The actual implementation of the DynamiTE framework is still in heavy
development.  At its lowest level, it provides a means of simulating
the operations of the Nomadic Time process calculus, allowing them to
be more clearly understood.  In application, it can provide a useful
mechanism for structuring concurrent programs, clearly dividing
internal behaviour and interprocess communication.  The possibility to
add further implementations of the channel and locality factories, via
the plugin mechanism, also means that fairly complex concepts can then
be leveraged by the programmer in the simple manner provided by the
framework.
