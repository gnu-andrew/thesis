% Thesis: DynamiTE
% Author: Andrew Hughes

\chapter{The DynamiTE Framework}
\label{dynamite}

\section{Introduction}

In this chapter, we introduce DynamiTE, the Dynamic Theory Execution
framework.  This provides the solution we first proposed in
\ref{solution}, using the Nomadic Time (NT) calculus introduced in
\ref{nt} and \ref{tnt} as a foundation for application development.
Through using DynamiTE, programmers compose NT processes, realised as
Java objects, to create a working system.  The framework handles
running these processes, in parallel if necessary, and negotiates the
communication between them.  Both features are provided by leveraging
existing facilities in the underlying Java virtual machine and class
library.

Over the course of this chapter, we will describe how NT processes are
mapped onto Java objects (see \ref{dyn:maptheory}) and then show how
DynamiTE can be used to create an implementation of the application we
introduced in \ref{protoapp} (see \ref{app:dynamite}).  But first, we
discuss why we chose Java as the host language for DynamiTE and what
advantages and disadvantages this brings to its implementation.

\section{Why Java?}

The first implementation of the Java programming language was released
in 1995 by Sun Microsystems.  It takes the form of a block structured
language with a syntax akin to C or C++.  However, unlike programs
written in those languages, Java applications tend to be compiled to
platform-independent Java bytecodes which are then executed by a Java
Virtual Machine or JVM.  This allows the same Java program to be
executed on multiple platforms without the need for recompilation.
With this new operating environment comes the removal of a number of
features found in Java's predecessors and the restriction of others,
with the aim of creating a safer and more portable language:

\begin{itemize}
\item \textbf{No pointer manipulation}.  All primitive types in Java
  (integer, floating point numbers, booleans and single characters)
  are passed by value.  All objects are stored and passed as pointers
  or references to their location in memory.  These pointers are
  immutable, removing the ability to perform pointer arthimetic
  (e.g. for iterating over arrays) and with it, a host of problems
  inherent with inappropriate memory access.  For example, attempts to
  use a \texttt{null} pointer are caught by the virtual machine and
  produce a checked exception, rather than causing a segmentation
  fault which brings down the entire process.
\item \textbf{All arrays are bounds checked}.  A major cause of errors
  and security issues in C and C++ programs is the possibility of
  buffer overflows, where programs write to memory beyond the end of
  an array.  In Java, such errors are prevented by the virtual
  machine; any attempt to access an index outside the bounds of an
  array causes a checked exception to be thrown and direct access to
  the array's memory is forbidden by the lack of pointer manipulation.
\item \textbf{All memory management is performed by a garbage
  collector}.  While allowing manual memory management allows the
  programmer greater control, it leads to an equivalent to the issue
  we saw with semaphores in \ref{semaphores}; every allocation must be
  paired with a later deallocation to avoid the possibility of an
  application leaking memory.  The problem is even more pronounced
  with regard to memory management as, while the acquisition and
  release of a lock tend to occur in close proximity to one another,
  allocation and deallocation can occur in quite disparate parts of
  the application.  In Java, memory is instead managed by a
  \emph{garbage collector} which allocates memory for objects as
  needed and periodically reclaims those that are no longer
  referenced.  The downside of this is that the garbage collector has
  to use processor time to perform its scans which would otherwise be
  used by the application.  However, as new garbage collection
  techniques, such as concurrent and generational collectors, become
  prevalent, this disadvantage is further outweighed by the prospect
  of chasing memory leaks.
\item \textbf{Lack of unsigned types}.  All integer types in Java use
  a bit to store the sign of the value, with no equivalent unsigned
  types that instead use this bit to store larger values.  This makes
  bitwise operations (and (\texttt{\&}), or(\texttt{|}) and
  not(\texttt{\~})) more inefficient as they need to operate on the
  type one size above (bytes ($2^8$) on shorts ($2^{16}$), shorts on
  ints ($2^{32}$), etc.).  Indeed, section 15.22.1 of the Java
  language specification\cite{javaspec} states that \emph{binary
    numeric promotion} (as defined in 5.6.2) should be applied to the
  operands, causing them to be converted to integer or long integer
  levels of precision before the operation is performed.  Thus, it
  logically follows that it is impossible to work with unsigned long
  integers ($2^64$) without resorting to the overhead of a class which
  implements arbitrary precision integers, such as
  \texttt{java.lang.BigInteger}.  Unsigned types continue to be
  proposed for addition to the Java language, but no such extension is
  scheduled for the next release (Java 7).
\end{itemize}

Although these changes are made at the expense of flexibility for the
programmer and possible efficency gains, they save time overall in
chasing bugs caused by memory allocation errors, buffer overflows or
leaks.  Besides, the Java Native Interface (JNI) can be used to
implement certain methods in C, should the need arise.  Many of the
methods provided by the Java class library do just that, usually to
make use of a platform-specific application programming interface
(API).  Doing so has some overhead and means losing the safety and
memory management benefits of Java, but is possible where necessary.

Performance has been a common criticism of Java, not just because of
these features but also because the Java bytecodes it uses must be
either interpreted or compiled into native code at run time.  This is
much less of an issue than it once was, due to advances in virtual
machine design and Just-In-Time (JIT) compilation techniques.
Theoretically, JIT compilation should eventually exceed the
performance of code compiled Ahead-Of-Time (AOT) as it can take
advantage of information only available at runtime.  This includes
knowing the exact platform on which the code will execute and being
able to make better optimisations based on statistics gathered through
execution (e.g. better branch prediction).  For example, HotSpot, the
virtual machine used by Sun's implementation of Java, only uses the
JIT compiler to create native code when it believes the code is used
enough (`hot' enough) to make doing so worthwhile.

Of these changes, the absence of unsigned types is the only one that
seems to have no advantage, other than simplifing the language.  Many
file formats and network protocols include unsigned types, so working
with them in Java becomes harder than is necessary.  Although their
absence may have made sense in earlier versions of the language, the
complexity of understanding unsigned arithmetic now seems trivial when
compared with the existential type system and its lack of reification
which was introduced by the addition of `\emph{generics}' in Java 5.
We thus hope that they may make an appearance in Java 8.

\subsection{Concurrency Provision}

From the perspective of implementing DynamiTE, one advantage of Java
is its broad support for concurrency.  Java is one of the few
languages to have an implementation of \emph{monitors}, a feature we
demonstrated in \ref{semaphores}.  It has also had support for threads
from the very beginning, with support as a core part of the virtual
machine rather than as an auxillary library (the approach used for C).
Java's platform independence means that the same threading constructs
and semantics, as mandated by the VM specification\cite{vmspec}, can
be used across all operating systems supported by a Java virtual
machine.  The actual implementation is provided by the virtual machine
and class library, which may either map them on to native threads or
provide \emph{green} threads, where the virtual machine itself creates
and schedules threads.  The main disadvantage of the latter is that
blocking calls to the operating system performed by one thread will
cause the virtual machine and all its threads to be blocked; as the
system is unaware of the presence of the threads, its only option is
to block the entire VM process.  Green threads are however much faster
to create and synchronise, as everything takes place within the VM.
They can also match the required thread semantics exactly, rather than
having to map those provided by the operating system's threads.  While
earlier versions of Sun's implementation used green threads, native
threads are now used on all supported platforms.

The result of this early adoption of multithreading is that the
implementation in Java is reasonably mature and well-tested.  With
Java 5, this support was greatly expanded as a result of research led
by Doug Lea and incorporated into the Java platform via JSR166 and the
\texttt{java.util.concurrent} packages \cite{jsr166, concpractice}.

The extensions provided by JSR166 take the form of a host of new
classes, backed by support in the virtual machine.  The Java language
itself is not altered.  It provides support for:

\begin{itemize}
\item \textbf{Atomic Variables}.  These provide replacements for
  integer, long integer and reference fields which can be updated in
  an atomic fashion, are safer than \texttt{volatile} variables and
  more efficient than locking.  The Java memory model allows
  operations which alter the value of normal fields to be reordered by
  the VM as a form of optimisation, as long as this reordering is not
  visible from within the same thread.  However, this means that other
  threads may see the changes in the wrong order or not at all.
  Marking a field as \texttt{volatile} makes the VM aware that it may
  be accessed by multiple threads, causing updates to be made visible
  to all threads immediately.  However, \texttt{volatile} fields are
  still prone to race conditions when used in non-atomic operations
  such as incrementing a value or performing a conditional
  update\footnote{e.g. in \texttt{if (x == 4) x = 5}, it is possible
    for \texttt{x}'s value to have been changed by another thread
    before the assignment but after the comparison}.  The usual
  solution is to obtain a lock on the class every time the variable is
  altered; this provides both the update guarantees of a
  \texttt{volatile} variable and blocks other threads trying to obtain
  the same lock.  Atomic variables provide an alternate solution by
  allowing the processor's CAS operation (see \ref{semaphores}) to be
  used.  This is usually more efficient than locking the entire class,
  which will involve the VM performing a CAS operation on the lock at
  some point anyway.  While locking takes a pessimistic approach to
  thread safety by blocking all other threads, CAS operations are
  optimistic; the update is attempted, and if it fails, we try again
  until it succeeds.  Implementing such a check successfully is even
  more prone to error than locking, as the programmer has to ensure
  they check the result of the CAS and loop accordingly, but it is
  usually much more efficient when contention is low.  With the
  addition of atomic variables to Java, programmers now have the
  choice of using either.
  \item \textbf{Explicit Locks}.  As we saw in \ref{semaphores}, Java
    has implicit reentrant locking via the \texttt{synchronised}
    keyword.  Their use, however, is limited; there is only one lock
    per class, so all its variables must be protected by the same
    lock, and threads are always blocked until they either acquire the
    lock or the thread is interrupted by \texttt{Thread.interrupt()}.
    The \texttt{ReentrantLock} class provides a more advanced version
    with the following additional features:
    \begin{itemize}
      \item \texttt{tryLock()} can be called to perform a non-blocking
        acquisition of the lock.  It immediately returns with
        \texttt{true} if the lock was acquired, and \texttt{false} if
        it wasn't.
      \item \texttt{tryLock(long, TimeUnit)} can be called to perform
        a timed acquisition.  If the lock is available, it acquires it
        and returns immediately.  Otherwise, it blocks.  However,
        unlike the implicit lock provision and the \texttt{lock()}
        method, it will become unblocked after the given timeout and
        return \texttt{false}.
        \item The lock can operate in a fair mode, where threads
          acquire the lock in the order they requested it.  Both
          implicit and explicit locks default to unfair behaviour,
          which permits \emph{barging} if a new thread happens to
          request a lock when it is unheld.  Unfair locks are much
          faster\footnote{If threads are not allowed to jump the
            queue, then we end up blocking and descheduling a thread
            which could have quite happily acquired the lock but isn't
            allowed to do so because of the fairness policy}, but
          fairness is sometimes needed to ensure correctness.
    \end{itemize}
    A class can have multiple instances of an explicit lock, just like
    any other variable, and this benefit is utilised by
    \texttt{ReentrantReadWriteLock}.  This class provides both a
    shared (\textbf{read}) lock and an exclusive (\textbf{write})
    lock.  Multiple threads can acquire the read lock, but to acquire
    the write lock, both locks must be unheld.  This can be used to
    make classes more efficient when compared with the brute force
    approach of enforcing mutual exclusion for all operations.  For
    example, a collection class can allow multiple threads to read
    values as long as there is no thread altering the collection.
    Both locks, and other implementations such as \texttt{Semaphore},
    are based on \texttt{AbstractQueuedSynchronizer}\cite{aqs} which
    provides a common framework thread queues.
    \item \textbf{Explicit Condition Queues}.  As in the case of
      locks, Java already has its own implicit condition queues,
      accessible via the \texttt{wait}, \texttt{notify} and
      \texttt{notifyAll} methods.  These also have similar limitations
      to the implicit locks; only one queue is available per class and
      either one or all threads must be notified.  With only one
      condition queue, the usability of \texttt{notify} to alert a
      single thread is extremely limited; using it is dangerous if
      there is more than one condition as the wrong thread may be
      awoken, and it is inefficient unless a change in the condition
      means that one and only one thread may proceed.  The former can
      be observed in the buffer example of \ref{semaphores} where
      there are two conditions: \texttt{used == BUFFER\_SIZE} and
      \texttt{used == 0}. The latter is observable in a `gate'
      scenario where multiple threads queue up waiting for a condition
      to hold, and then all proceed when it does.  Explicit condition
      queues address these issues by allowing a class to have multiple
      condition queues.  Each \texttt{Condition} is obtained from a
      \texttt{Lock} by a call to \texttt{Lock.newCondition} and that
      same lock must be held when calling its methods.  In the buffer
      example, the synchronized blocks would be replaced by the use of
      explicit locks and the calls to \texttt{wait} and
      \texttt{notifyAll} by \texttt{await} and \texttt{signal} calls
      on one of two \texttt{Condition} objects.  The \texttt{signal}
      method can now be used rather than \texttt{signalAll}, awakening
      just one thread, as we know the thread will be waiting for the
      condition whose state has changed and no other.  This avoids
      waking all threads and having all but one go back to sleep.
    \item \textbf{Executors and Thread Pools}.  The new classes
      provide a framework for executing tasks in the form of the
      \texttt{Executor} interface.  This decouples the process of
      submitting a task from how it is executed.  Tasks (in the form
      of an object which implements the \texttt{Runnable} or
      \texttt{Callable} interface) are submitted to an
      \texttt{Executor} instance, and then performed in a manner
      determined by the \texttt{Executor} implementation.  The
      \texttt{Executors} class provides a number of pre-defined
      instances:
      \begin{itemize}
        \item A single thread executor, which performs tasks sequentially.
        \item An executor with a fixed size pool of threads.
        \item An executor with an unbounded pool that grows and shrinks as demand allows.
        \item An executor with a fixed size pool of threads and delayed or periodic task execution.
      \end{itemize}
      The programmer is also, of course, free to define their own
      implementation.  This feature is very useful for implementing
      parallel composition in DynamiTE as each process may be
      submitted to an executor, the choice of which is left up to the
      user of the framework.
    \item \textbf{New Collections}.  The standard Java collections
      apply an all-or-nothing approach to thread safety; either the
      instance is unsafe for multithreaded use (as with instances of
      the Java 1.2 classes -- \texttt{HashMap}, \texttt{ArrayList},
      etc.) or every method call locks the class (as with the legacy
      classes such as \texttt{Vector} and \texttt{Hashtable} or the
      1.2 classes when wrapped by the \texttt{synchronizedX} methods
      in \texttt{Collections}).  The JSR166 extensions provide a new
      set of collections which utilise the features listed above.  For
      example, \texttt{ConcurrentHashMap} provides a hash map which
      utilises \emph{lock striping}; the map is protected by multiple
      read and write locks which protect only a segment of the whole
      map each.  Thus, not only can multiple readers access the map
      concurrently, but it may be possible to perform multiple writes
      concurrently if they effect different areas of the map.  The new
      collections also include various \texttt{BlockingQueue}
      implementations, which implement the producer-consumer model we
      demonstrated with the buffer example in \ref{semaphores}.  One
      such implementation is \texttt{SynchronousQueue} which closely
      matches the semantics of synchronous channels in Nomadic Time;
      it has no storage so a thread performing a \texttt{put} blocks
      until a receiving thread calls \texttt{take}.
\end{itemize}

With these additions, the programmer is given a lot of control and
flexibility when implementing concurrent programs in Java, and we will
leverage many of these features when implementing DynamiTE.  Having
essential components such as locks and concurrent collections already
available and well tested makes it much easier to meet the
requirements of the framework.

Other languages are not so lucky.  In C and C++, threads are provided
by an operating system library and thus vary depending on platform.
The POSIX standard for threads attempts to overcome this by providing
a standard threading interface and semantics for POSIX systems.  While
POSIX-based systems including GNU/Linux, Solaris, FreeBSD and Mac OS X
all provide implementations. the problem remains with systems that do
not provide such by default, notably Microsoft Windows.

Haskell has been slow to introduce threading support.  Although the
Concurrent Haskell\cite{conchaskell} extension was originally proposed
in 1996, it does not form part of the Haskell 98 standard and the GHC
documentation still lists it as experimental.  Both Hugs and the
Glasgow Haskell Compiler (GHC), the two main implementations of
Haskell, provide an implementation of Concurrent Haskell's
\texttt{Control.Concurrent} module, they do so using green threads.
As mentioned above, while these are faster than native threads,
blocking calls to the operating system, such as I/O, will cause all
threads to be blocked.  A workaround is provided in GHC when it is
built with the \texttt{-threaded} option; it uses a pool of worker
threads to execute Haskell code and switches to a new one when a
\texttt{safe} foreign call is made.  It also allows native threads via
\texttt{forkOS} when built in this manner.  As with C, this makes
Haskell's thread behaviour dependent on the underlying system as
opposed to providing a standard set of operations and semantics;
whether threads are provided and how well they perform depends
entirely on which implementation of Haskell is being used.

However, functional languages in general should be a good basis for
concurrency.  They already operate in a task-oriented manner through
\emph{pure functions}; data is fed in, manipulated as desired and the
result output without altering memory.  Those that do alter memory,
and thus could lead to concurrency issues, are clearly denoted
(e.g. by monads in Haskell), reducing the amount of code that has to
be checked for race conditions.

It is thus a pity that they are not more widely used and their
concurrency facilities not more well developed.  This is changing,
however.  GHC has recently been extended with support for Software
Transactional Memory (STM) \cite{haskellstm}, which provides a new
\texttt{atomic} function and \texttt{STM} monad for implementing
transactions.  The STM logs all actions and then performs a single
atomic commit, provided there are no conflicts with other updates.
This allows Haskell programmers to compose new atomic transactions
from others, and moves the need to ensure atomicity away from each
individual function to the caller, who can only invoke them from
within an atomic environment.

Erlang\cite{erlang} is another interesting case, as both it and
DynamiTE focus on message passing between processes as opposed to
shared data and locking.  Erlang differs in that it operates
asynchronously, collecting messages in a mailbox on a per-process
basis and filtering which ones are received in any one operation.
However, synchronous delivery can be implemented by requiring messages
to be acknowledged.  The main limitation of current Erlang
implementations is that they use green processes; unlike green
threads, these don't share state but they do have the same downside
that a blocking system call from one will cause them all to become
blocked by the system.

Both Erlang and Haskell provide an interesting environment in which to
implement a framework like DynamiTE.  Indeed, we hope that the
majority of the design explained here in \ref{dyn:maptheory} can be
applied to most languages with sufficient threading support.  However,
there is another reason for our choice of Java as the initial
prototype language.

\subsection{Popularity}

Popularity is rarely a good reason to do anything but, in combatting
developer inertia, it is a good weapon to have.  The simple fact is
that most of today's developers know Java and sometimes little else;
it (or its close relative, C\#) is taught as part of most computer
science degrees and is used as the language of choice for many
applications, especially in the area of enterprise web applications.

As we discussed in \ref{solution}, easing the barriers for adoption is
an essential aspect in influencing developers to try something new.
With DynamiTE, we are already advocating the idea of using message
passing rather than state manipulation to Java developers, a body of
programmers who will generally be more familiar with object-oriented
design techniques which focus on manipulating data.  Adding the
prospect of learning an entirely new language is not going to help our
case, and we believe this to be the main reason other solutions have
not moved far beyond their academic roots.  Instead, DynamiTE is
developed as a Java class library like any other, which leverages
standard features of the Java platform and which can be further
developed by the very people that use it.

We will be the first to admit that Java has issues; its age means that
with hindsight many design decisions can now be seen as flawed and
attempting to change this leads us to consider the bane of all
programming languages -- backwards compatibility.  Most features, good
or bad, are now enshrined in the language and further development
rightly takes a conservative attitude to avoid breaking the huge body
of existing code already in use.  This means that APIs are deprecated
rather than removed, causing the class library to become more bloated
than ever, and new language features such as generics take years to
appear and even then have to be limited.  No consensus has yet been
reached on how closures should be implemented, so they will not appear
in Java 7 either.  These issues are here to stay; Java is unlikely to
ever have a type system as advanced as that of most functional
languages or a separation between pure and impure functions.  But with
these come maturity and a vast body of developers which we believe to
be far more useful in achieving our goal than the possibilities of a
perfect but niche language.

\section{Mapping Theory to Practicality}
\label{dyn:maptheory}

In this section, we show how the syntactic constructs of NT introduced
in chapter \ref{nt} are mapped on to Java classes by the DynamiTE
framework.  Within DynamiTE, developers can create concurrent
applications simply by implementing the specific behaviour they
require in appropriate subclasses.  Recall the syntax of NT from
\ref{eqn:tnt-syntax}:

\begin{equation}
  \begin{aligned}
    \expr, \exprb \quad \mathrel{::=} \quad &
      \nil  \mid
      \Omega \mid
      \Delta \mid
      \Delta_{\sigma} \mid
      \alpha . \expr  \mid
      \expr + \exprb \mid
      \expr \mathrel{\!|\!} \exprb \mid
      \timeout{\expr}{\sigma}{\exprb} \mid \\
    & \stimeout{\expr}{\sigma}{\exprb} \mid 
      \mu X . \expr \mid
      X \mid 
      \expr \res{A} \mid
      \locv{m}{\expr}{\exprb}{\vec{\sigma}} \mid
      \ambop . \expr \\
   \ambop \quad \mathrel{::=} \quad & \tntin{m} \mid \tntout{m} \mid \tntopen{m} \mid
      \procin{\beta}{m} \mid \procout{\beta}{m} \mid \bin \mid
      \bout \mid \bopen
   \end{aligned}
\end{equation}

Each syntactic construct is realised as a class that implements \texttt{Process}:

\begin{verbatim}
public interface Process
{
  Set<Transition> getPossibleTransitions();
}
\end{verbatim}

Operation follows a top-down approach; the complete system is
represented by a single instance of one of these classes which, in
most cases, will be an operator that composes together further
instances as appropriate.

The simplest \texttt{Process} subclass is the representation of $\nil$,
realised as a class \texttt{Nil} which provides process termination.
The internal action $\tau$ is realised as an abstract class \texttt{Tau}
and this is where the user can implement arbitrary sequential behaviour
as required, by providing a subclass. The observable actions form part
of the channel subsystem, described in \ref{dyn:channels}.

The $+$ operator is implemented as a class which contains a list of
subprocesses from which one is chosen at random.  The action to perform
is computed by traversing the hierarchy, so restriction is simply a
matter of providing appropriate filtering, thus preventing the
restricted names from travelling further up the hierarchy.

More interesting is the \texttt{Par} class which implements the $\mid$
operator, as it must allow its subprocesses to operate concurrently.
The most obvious way to achieve this is by mapping individual processes
onto Java threads.  This also means that data can be stored with the
process by means of thread-local variables.  However, we are keen to
offer flexibility in how the individual features of the framework are
implemented.  Java thread mapping is only one way in which concurrent
processing may be implemented and so we abstract away \texttt{Par} from
the threading implementation as much as possible, thus allowing it to be
replaced by other implementations at a later date.  For example,
concurrent processing could also be provided by distinct processes
spawned by the VM or a more complex distributed solution may become
apparent.

\subsection{The Channel Abstraction}
\label{dyn:channels}

In the same vein, the implementation of synchronisation channels is
abstracted in such as way as to allow for differing implementations.
Here, the provision of multiple implementations is more prevalent and so
a plugin mechanism is already present.  Fortunately, Java already has
plenty of support for plugin based frameworks (imaging and sound already
being implemented in this fashion) and the new
\texttt{java.util.ServiceLoader} API provided in 1.6 makes this simpler
still.  This allows the user to have freedom of choice with respect to
their chosen channel implementation, which may even be further extended
by their own or third-party plugins.

At its simplest, DynamiTE provides a way of testing NT processes and
ensuring they perform as expected.  In this respect, the simplest
channel plugin is a dummy channel, which need do nothing more than
simply exist.  More complex solutions are of course possible and are
needed to make the framework both usable and interesting.  

Although currently there is no realisation of data within the formal
layer of the calculus, this only matters to the extent that we wish
transmitted data to alter the constructs themselves via
substitution\footnote{The $\pi$ calculus \cite{picalctutorial} is an
obvious example of such behaviour, which goes to the extreme of not only
allowing data to be transferred but also references to channels which
can then later be used in the language constructs.  This, in essence,
provides the form of mobility present in the $\pi$ calculus.}.  Data can
be transferred between processes and used within internal actions
without having to be explicitly realised at the formal level.  There are
a multitude of ways of implementing data transfer, ranging from simple
mechanisms like files and sockets to more full-blown interprocess
communication protocols such as Java's Remote Method Invocation (RMI),
the Common Object Request Broker Architecture (CORBA) and web services.
The plugin nature of the channel architecture means that any of these
possibilities may be used and more besides.

While the implementations of the channels themselves can provide the
input and output mechanisms, interoperability between the two has to
take place at a higher level.  Thus, the onus is on the parallel
implementation, \texttt{Par}, to co-ordinate the communication between
the two, by virtue of discovering which names are exposed at the point
of composition.

A possible simplification becomes apparent here, as some implementations
may make use of channel naming.  For example, if the channel name refers
to a host and port for a TCP/IP implementation, then the sender need
only try and connect to see if a recipient is available.  Channel names
are assumed to be unique, so such a mapping is possible.  However, they
are not unique to a particular process, making it perfectly plausible
for the channel name to occur simultaneously on multiple processes and
thus for a competition to occur.  There is also the issue of whether
they can actually `see' each other, according to the constraints of the
calculus, so the decision should still be left to an appropriate
parallel composition operator.

\subsection{Signalling}
\label{dyn:signalling}

One of the most interesting parts of the DynamiTE framework is the
implementation of clock signals.  While there have been other attempts
to produce frameworks or languages based on process calculi (see section
\ref{dyn:relatedwork}), we believe that the rendering of discrete time into
such a context is novel.

The first question to answer when attempting to perform such a
translation is where to actually locate the clocks.  Within NT, the
obvious answer is within each environ, as these are responsible for
providing the division between processes which can observe clock ticks
and those which can not.  For example, the following environ
\begin{displaymath}
\loc{m}{P}{\Omega}{\sigma}
\end{displaymath}
would be realised as an instance of the \texttt{Environ} class with the
name $m$.  This instance would maintain a reference to the process $P$
with which it interacts.  Not only is the execution of $P$ controlled by
the environ (as with the implementations of $+$ and $\mid$ above), but
it also controls when and how the ticks of $\sigma$ reach $P$.

Recall our earlier description of the calculus, where we mentioned how
clock ticks are always pre-empted by high priority actions, which may
arise either from explicit internal actions denoted by $\tau$, implicit
internal actions caused by synchronisation or movement.  So, in order
for the environ to know whether to propagate a clock tick to the
process, it must first probe it to find out whether such a high priority
action is pending.  Clock ticks may also be prevented by the $\Delta$
and $\Delta_\sigma$ constructs, so these must also be checked for.

Both can actually be achieved in one transaction by making the probe the
clock tick.  The clock tick is sent down the process hierarchy until it
reaches a point at which a decision can be made as to whether the tick
should occur or not.  If the tick can occur, it is propagated back up
the hierarchy, eventually stopping when it reaches its host environ
again.  The host environ can be determined by the set of clocks
associated with each environ, which is also used to calculate the
signals to be propagated initially.  If the clock is not allowed to
tick, then the actual action performed is sent instead.

This algorithm is best explained by a couple of prototypical examples.
First, consider 
\begin{displaymath}
\loc{m}{a.\nil + b.\nil}{\Omega}{\sigma}
\end{displaymath}
where the process inside $m$ has no $\tau$ actions, synchronisations,
mobility or clock stop operators, and thus clearly allows the clock
$\sigma$ to tick.  The environ $m$ iterates over its set of clocks (here
just $\sigma$), and sends a tick from each to its process, $a.\nil +
b.\nil$.

This process is realised by an instance of the \texttt{Sum} class, which
composes the two processes together.  A clock can only tick over the
summation operator if it can tick over both sides, so the result from
this instance is simply the result of combining the return value from
probing each of the constituent processes.

Both $a.\nil$ and $b.\nil$ are implemented using instances of the
\texttt{Prefix} class, which composes a \texttt{Channel}\footnote{An
abstract class, instances of which are provided by the channel
architecture described in \ref{dyn:channels}} or \texttt{Tau} instance
(unified by the \texttt{Action} class) with another instance of a
\texttt{Process} subclass.  In determining whether a clock can tick, it
first checks that the action is a channel rather than a \texttt{Tau}
instance (which would pre-empt the clock), and then probes the
\texttt{Process} instance.  In both these simple cases, this is an
instance of \texttt{Nil}, which allows clock ticks.

Having determined that the clock may tick, each nested call returns with
the $\sigma$ clock tick, thus propagating it up to the original call in
the environ $m$.  Having seen how this operates for a process that can
tick, it is simple to see how it differs when something prevents the
clock from ticking.  If any part of the query returns something other
than a clock tick, this will be propagated upwards in preference.

Consider what happens if $a.\nil$ is changed to $\tau.\nil$.  The
left-hand side of the summation will receive the $\tau$ action from the
\texttt{Prefix} instance, which then takes priority over the $\sigma$
from the right-hand side and is propagated to the environ, $m$.  This is
the case in any situation where the $\sigma$ is required to compete
against an action, a $\tau$ or a mobility primitive.  The clock stop
operators behave slightly differently in that they don't replace the
action, but instead mark the $\sigma$ action as \emph{stopped}.

Note that a similar method of determining the presence of clock ticks
must take place to handle the \texttt{STimeout} and \texttt{FTimeout}
classes.  Both sides of the timeout are inspected, and behaviour
determined as follows:
\begin{enumerate}
\item If the left-hand side can perform a high-priority action, it will
      be allowed to proceed and the right-hand side need not be
      considered.
\item Otherwise, the possible actions include unpaired actions (such as
      $a$ and $b$) and clock ticks (both from the clock involved in the
      timeout and from other clocks), one of which is chosen to be
      performed.
\item Once the chosen action has been performed, the timeout instance
      will be replaced as appropriate (see chapter \ref{nt}). 
\end{enumerate}

\subsection{Structural Changes}
\label{dyn:structchange}

The \texttt{Environ} class also places a central role in providing
system structure.  In chapter \ref{nt}, we described how
processes are organised into environs and the way communication is
limited to its bounds.  Within DynamiTE, one possible use of environs
is to map them to physical or virtual hosts.  While a simple testing
solution can execute the entire system on a single platform, environs
provide a natural form of process distribution which can be leveraged by
the framework.

This does however give the initial impression that structural mobility
will become very inefficient, if hosts are expected to interact to
determine the feasibility of a move and then actually change position
during execution.  In reality, these issues are minimal.  An inward
movement is always in relation to a sibling, while an outward movement
concerns some parent environ.  As the structure of environs is expected
to closely match the actual physical structure of the hosts, such
interactions should be relatively low cost to perform.  Also, a
structural movement does not change the contents of the moving environ,
only its context.  Thus, only later communication with surrounding
environs is affected.  For example, it may have been able to see a
sibling environ before the movement, but is now inside this environ and
can receive clock ticks emitted by it.

If hosts do not physically move, then what is the point in allowing such
structural changes?  The change in clock signalling just mentioned is
one effect.  In addition, we also make provision for contextual data to
be stored at the environ level, in addition to that stored local to a
particular thread, and transferred via channels.  This gives additional
purpose to the use of structural mobility and process migration, which
we describe next.

\subsection{Migration}
\label{dyn:migration}

The final aspect of DynamiTE that we describe here is the migration of a
process from one environ to another, which occurs both as a result of
using one of the process mobility operators and from the behaviour of
$\tntopen$.  This is perhaps one of the most interesting aspects, as it
represents the movement of code from one environment to another,
possibly located in a different physical location.

Migrating an active process is not a simple operation.  Not only must
any remaining code to be executed be transferred, but any local data
must also migrate.  NT does allow us to achieve a significant amount of
simplification here.  The transferred process is already separated from
other code within the system by virtue of the moving process being in
the form of a \texttt{Prefix} instance.  When the action is matched to
the one used for the mobility operation, the \texttt{Process} instance
is transferred to its new location.  There is no necessity to deal with
code that is currently being executed.

As with concurrency and channel operation, how movement is achieved is
designed to be flexible, with provision being made for distribution and
code migration to be implemented in different ways.  One of the most
obvious ways is to serialise the \texttt{Process} instance and
reconstitute it at its destination.  Migrating a process should then
just be a case of transmitting the serialised instance, followed by any
local data, and beginning execution at the destination.  However, this
is one area in which we expect further study of the existing literature
to enlighten us with more sophisticated ways of achieving such
migration.

\section{A Prototypical Application in DynamiTE}
\label{app:dynamite}
                                   
\section{Related Work}
\label{dyn:relatedwork}

There has already been a significant body of research into providing
concurrent frameworks, including those based on process calculi.
However, we believe our work to be novel in approaching the
implementation of both global discrete time, via clock signalling, and
mobility.

The $\pi$ calculus has been the subject of much of this work, primarily
due to its status as the most prevalant mobile process calculus.  Obliq
\cite{obliq} and Pict \cite{daveturner:phd} are both programming
languages with semantics founded in the $\pi$ calculus, while Nomadic
Pict \cite{wojciechowski:phd} takes this further, introducing
distribution not usually present in the $\pi$ calculus.  Within research
related to the ambient calculus, a machine framework (PAN
cite{sangiorgi:safeambientsmachine}) has been developed and
implemented.  Process calculi, such as the Seal calculus \cite{seal}
have also been developed specifically to provide a formal framework for
a distributed implementation.
%The $\pi$ calculus has been the subject of much of this work, primarily
%due to its status as the most prevalent mobile process calculus.  Obliq
%\cite{obliq} and Pict \cite{daveturner:phd} are both programming
%languages with semantics founded in the $\pi$ calculus, while a machine
%framework (PAN \cite{sangiorgi:safeambientsmachine}) has been developed
%and implemented for the ambient calculus.

\section{Conclusion}
