% Thesis: Mobility
% Author: Andrew Hughes

\chapter{Mobility}
\label{mobility}

Within the field of algebraic process calculi, there are two clear ways
in which the dynamic nature of a system is modelled.  The most
well-known is the form of mobility present within Milner's $\pi$
calculus which allows the scope of a name to change as the system
evolves.  This concept can be thought of in a similar way to the
reference passing that occurs in most programming languages; part of the
program begins with no knowledge of an entity, and later gains knowledge
by obtaining a reference to it.

Models in the $\pi$ calculus are not really mobile in the sense of
something moving from one place to another.  This isn't possible, as
there is no real notion of `place' to begin with.  However, the addition
of this mechanism does allow the modelling of dynamic systems, such as a
mobile phone network \cite{milner:lecture}, and is sufficiently
expressive as to allow it to encode Church's $\lambda$ calculus
\cite{funcproc}.

A more naturalistic form of mobility is found in calculi which allow
entities to \emph{migrate}.  One of the primary exponents of this is
Cardelli and Gordon's ambient calculus \cite{amb}, which groups
composed processes inside \emph{ambients}.  These ambients can be
moved up and down a nested hierarchy of such objects, or destroyed.  The
calculus differs from those previously considered, in that it
lacks communication primitives.  Surprisingly, the base syntax is
sufficient to allow communication to be encoded within them, and
indeed the entire asynchronous form of the $\pi$ calculus can be
represented.

The following two sections consider examples of both types of mobile
calculi in more detail.
 
\subsection{Scope Mobility}
\label{scopemobility}

\subsubsection{The $\pi$ Calculus}
\label{picalculus}

The $\pi$ calculus follows on from Milner's earlier work on CCS
discussed in \ref{ccs}. Essentially, it is a value-passing form of CCS
with a generalisation from values and channels to simple \emph{pure
  names}.  Thus, channels can be passed between processes, as well as
values, which means that their scope may change during execution.

To make this clearer, consider the syntax of the form of $\pi$ calculus
given in \cite{funcproc}

\begin{equation}
\label{pisyntax}
  E, F\ ::=\ 
  0\ |\ 
  \overline{x}y.E\ |\ 
  x(y).E\ |\ 
  (a)E\ |\ 
  (E\ |\ F)\ |\ 
  !E
\end{equation}
  
\noindent which is a minimal version containing replication as opposed
to recursion, with $a$ a channel name and $x$ and $y$ being defined
below.  Compare this with the syntax given for CCS in Eqn.
\ref{ccssyntax}.  The nil process, $0$, is still present, as is parallel
composition and restriction (although in a new form, $(a)E$).
Non-deterministic choice is present in the original version of the $\pi$
calculus presented in \cite{picalctutorial}, but is removed from the
version given in \cite{funcproc} due to the formulation of semantics
used there.  $!E$ is the syntax for replication, which replaces
recursion in this particular variant of the calculus to give a simpler
theoretical treatment, while still doing much the same job.

The main distinction between the two lies in the remaining element of
the syntax: prefixing.  In CCS, a more general syntax, $\alpha.E$,
where $\alpha \in \mathcal{N} \cup \overline{\mathcal{N}} \cup
\{\tau\}$, is used and includes input, output and silent actions.  In
the syntax given above for the $\pi$ calculus, the input ($x(y)$) and
output ($\overline{x}y$) syntax are given separately, and the input
prefix is \emph{binding}\footnote{When an input is received on $x$,
  $y$ is bound to the value of that input, which is then substituted
  for $y$ in the continuation of that process.} like restriction. $x$
and $y$ are both names, where `$x$ [is] the \emph{subject} and $y$ the
\emph{object}' \cite{funcproc}.  Silent actions no longer appear in
prefix form, but do occur as $\tau.E$ in some variants of the $\pi$
calculus.

The distinction between the $\pi$ calculus and value-passing forms of
CCS, which also use this form of prefixing, lies in $x$ and $y$ being
drawn from the same set in the $\pi$ calculus.  In contrast,
value-passing forms of CCS keep the two sets distinct, so that the
channel and value names do not intersect.  This change is what
gives $\pi$ calculus its power, as channels can now be used as the
object of an input or output.  Thus,

\begin{equation}
x(y).\overline{y}x.0
\end{equation}

\noindent becomes perfectly valid.

This also has an effect on restriction.  Recall that, in CCS,
$(a.0|\overline{a}.0)\backslash a$ restricts the scope of $a$ to just
the two processes, $a.0$ and $\overline{a}.0$, making a synchronisation
the only possible action which may be performed.  Now consider the
following processes defined using the $\pi$ calculus:

\begin{equation}
(a)(a(x).\overline{x}a.0\;|\;\overline{a}y.0)\;|\;y(z).P
\end{equation}

\noindent where the scope of $a$ is again restricted, this time to the
two processes $a(x).\overline{x}a.0$ and $\overline{a}y.0$.  If these
two processes synchronize, the system evolves to:

\begin{equation}
(a)(\overline{y}a.0\;|\;0)\;|\;y(z).P
\end{equation}

\noindent with $x$ becoming bound to the channel name, $y$.  This shows
how the $\pi$ calculus allows channel names to be passed between
processes, but it is the next transition that is really interesting.
$\overline{y}a.0$ will pass the channel name, $a$, to $y(z).P$, which is
outside the scope of the restriction imposed on $a$.  As a result, the
scope of $a$ is \emph{extruded}:

\begin{equation}
(a)(0\;|\;0\;|\;P\{a/z\})
\end{equation}

\noindent so as to include the process, $P$, in which $a$ is now
substituted for $z$.  Further, one of the structural congruence rules of
the $\pi$ calculus \cite{funcproc}:

\begin{equation}
(x)(P\;|\;Q) \equiv P\;|\;(x)Q\text{ if x not free in P}
\end{equation}

\noindent may be used to perform \emph{scope intrusion}, giving:

\begin{equation}
0\;|\;0\;|\;(a)(P\{a/z\})
\end{equation}

\noindent as the channel $a$ no longer occurs in the other two
processes.  These changes in scope are central to the concept of
mobility within the $\pi$ calculus.  They reflect the dynamic
environment of the processes represented, and give the calculus a
greater expressivity.

\subsubsection{Variants of the $\pi$ Calculus}
\label{pivariants}

Multiple variants of the $\pi$ calculus exist, including various
evolutions of the syntax and semantics.  As noted above, replication
is only introduced in the version of the calculus given in
\cite{funcproc}, which also defines a reduction-based semantics.  The
earlier tutorial papers \cite{picalctutorial} instead use recursion
and a structured operational semantics, based on a labelled transition
system.

The polyadic $\pi$ calculus \cite{milner:93polyadic} is a more
distinct variant.  Essentially, this involves a syntactic change to
input and output, so that a tuple is used, as opposed to the single
names used in the monadic $\pi$ calculus\footnote{This is a term used
  to refer to the original $\pi$ calculus in retrospect.}.  Having
this as a core part of the syntax provides advantages in representing
abstractions and giving a natural sort discipline\footnote{Sorts are a
  way of applying typing to the $\pi$ calculus, which will be covered
  further in section \ref{typedcalculi} on typed calculi.}.  However,
it is also possible to simply provide an encoding of this in the
monadic variant.

Doing so is not simply a matter of transmitting each value in
sequence; the operation needs to respect the atomicity implicit in the
use of multiple names.  Observe the following example from
\cite{milner:93polyadic}:

\begin{equation}
x(yz)\;|\;\overline{x}y_1z_1\;|\;\overline{x}y_2z_2
\end{equation}

\noindent where the process on the left should receive either $y_1$ and
$z_1$ or $y_2$ and $z_2$.  With the following semantics,

\begin{align}
\seml x(yz) \semr & \eqdef x(y).x(z) \\
\seml \overline{x}yz \semr & \eqdef \overline{x}y.\overline{x}z
\end{align}

\noindent the two sending processes can interfere with one another.
$y$ will become bound to either $y_1$ or $y_2$ on the first
synchronisation, which is fine, but $z$ may then receive whichever of
these two remains instead of the second element in the tuple.  This
happens because there is no link between the two synchronisations.
Thus, each subsequent transmission results in a new competition
between the two processes as to who actually synchronises with the
receiver.

The solution to this problem is to make use of a \emph{private
  channel}.  Before transmitting any of the names that form part of
tuple, the sending process passes a reference to a new channel to the
receiver.  The receiver then uses this channel to receive the contents
of the tuple, rather than relying on an existing channel, which may be
prone to interference.  Thus, the semantics become:

\begin{align}
\seml x(yz) \semr & \eqdef x(w).w(y).w(z) \\
\seml \overline{x}yz \semr & \eqdef (w)(\overline{x}w.\overline{w}y.\overline{w}z)
\end{align}

\noindent where $w$ is the new private channel created to facilitate
the process of transmitting the tuple.  This ability to encode the
polyadic variant in the original monadic calculus implies that the new
syntax fails to yield any greater expressivity, but this is not really
the motivation behind this extension.  Instead, what this provides is
a more natural way of transmitting information, which makes modelling
relatively complex systems easier.

The asynchronous $\pi$ calculus \cite{boudol:asynchrony,
  honda:asynchronouscommunication, sangiorgi:asynchronousprocesscalculi}
  deliberately reduces the level of expressivity in order to simplify
  reasoning and provide a better framework for distributed
  implementations.  The output prefix, $\overline{x}y.E$ is replaced
  with $\overline{x}y.0$, so that there is no continuation after an
  output.  In the original synchronous $\pi$ calculus, the behaviour of
  the continuation, $E$, is blocked until a synchronisation with a
  recipient can occur.  This doesn't occur in the asynchronous variant,
  as there is no longer any behaviour dependent on this output
  occurring.

  Synchrony can be emulated in the asynchronous polyadic $\pi$
  calculus, just as synchronous messaging frameworks, such as TCP, can
  be implemented on top of an asynchronous network.  The receiver
  simply has to acknowledge receipt of the message by replying to the
  sender.  The following semantics are given for the monadic prefixes
  in \cite{boxedamb01}:

\begin{align}
\seml \overline{c}x.P \semr & \eqdef (r)(\overline{c}xr\;|\;r.P) \\
\seml cy.P \semr & \eqdef c(yr).(\overline{r}\;|\;P)
\end{align}

\noindent where $r$ is not free in $P$.  The output is encoded as the
transmission of a tuple containing two names: $x$, the original name
being sent, and $r$, a new channel created to receive the
acknowledgement from the recipient.  This runs in parallel with
another process that awaits an input on $r$\footnote{$r$ is a
  syntactic abbreviation for $r()$ i.e. the input is an empty tuple.}
before continuing with $P$.  Thus, the original synchronous behaviour
is emulated, as $P$ will not evolve until the receiver has obtained
the private channel, $r$, and replied.

Other changes to the calculus are also commonly adopted to reduce its
expressivity, thus making more proofs feasible.  These include:

\begin{itemize}
\item \emph{input localisation} \cite{merro:locality}, whereby a link
  received from another process can not be used for input.  For
  example, a process $a(x).P$ may not use $x$ as a channel upon which
  to receive input in $P$.
\item \emph{uniform receptiveness}
  \cite{sangiorgi:uniformreceptiveness}, where the input end of a link
  occurs only once syntactically and is replicated so as to be always
  available.
\item \emph{input-guarded replication}, which is not just restricted
  to uniform receptiveness variants, but is generally used as a more
  restricted form of replication (so the replication operator becomes
  $!a(x).P$ rather than $!P$).
\end{itemize}

The final variant of the $\pi$ calculus considered here is the
extension to higher-order operations.  The most obvious change to make
in this direction is to allow processes to be exchanged.  Such a
second-order form of the calculus is given by the \emph{Calculus of
  Higher Order Communicating Systems} (CHOCS) \cite{thomsen:chocs},
which actually predates the $\pi$ calculus itself.  This extended CCS
with mobility by allowing processes, rather than channel names, to be
transmitted.

The more general area of higher-order $\pi$ calculus, and the theory
behind it, is covered in Sangiorgi's thesis \cite{sangiorgi:phd}.  It
defines an extension to the $\pi$ calculus, HO$\pi$, which not only
allows the transmission of names (first-order) and processes
(second-order), but also parameterised processes of arbitrarily high
order ($\omega$-order).  This is best illustrated by some examples,
drawn from \cite{sangiorgi:phd}.  In the simplest case, an `executor'
process can be defined, $x(X).X$, which will receive and then execute an
arbitrary process.  Placing this in an appropriate context,

\begin{equation}
\overline{x}P.Q\;|\;x(X).X
\end{equation}

\noindent the process on the left, $\overline{x}P.Q$, will transmit
the process, $P$, to the executor before continuing as $Q$.  Thus,
following the synchronisation of the two processes, this system
evolves to become:

\begin{equation}
Q\;|\;P
\end{equation}

\noindent where the process $P$ having being substituted for $X$.  

A more complex example is given by considering Milner's encoding of
the natural numbers \cite{milner:93polyadic}.  A natural number, $n$,
is encoded as a series of outputs on $y$, the number of which is equal
to $n$ (represented as $\overline{y}^n$), followed by a transmission
on $z$ to indicate zero and thus, the end of the number:

\begin{equation}
\seml n \semr \eqdef (y,z)\overline{y}^n.\overline{z}
\end{equation}

\noindent Using HO$\pi$, the addition of these numbers can be encoded
in a very simple way.  In the $\pi$ calculus, summation is achieved
via an indirect reference to the two numbers, using channel names.  In
HO$\pi$, the parameterised processes or \emph{agents} that represent
the numbers can be used directly in the representation of addition.
Thus, actually adding the two numbers together becomes a simple matter
of running the two concurrently, and linking them via a common
channel.

A $Plus$ agent, which performs the addition of two numbers, can be
defined as follows:

\begin{equation}
Plus \eqdef (X,Y)(y,z)((x)(X\langle y,x\rangle \;|\;x.Y\langle y,z\rangle ))
\end{equation}

\noindent where both $X$ and $Y$ are agents with two parameters,
corresponding to $y$ and $z$ respectively in the definition of $\seml
n \semr$ above.  The operation of this agent is best demonstrated by
example.  Assume $X$ is two and $Y$ is three, represented in HO$\pi$ as:

\begin{align}
X(y,z) & \eqdef \overline{y}.\overline{y}.\overline{z} \\
Y(y,z) & \eqdef \overline{y}.\overline{y}.\overline{y}.\overline{z}
\end{align}

\noindent and retaining the same representation used for $\seml n
\semr$ above.  When $X$ and $Y$ are passed to the $Plus$ agent, $X$ is
instantiated with a new private channel, $x$, in place of $z$ in the
above.  $Y$ is then prefixed with an input on this same channel, so
that the $y$ outputs occurring in $Y$ only execute after those in $X$.
This leads to the following sequence of transitions:

\begin{equation}
  \lderives{y} \lderives{y} \lderives{\tau} \lderives{y} \lderives{y} \lderives{y} \lderives{z}
\end{equation}

\noindent which is close to the sequence that occurs for the
representation of five in HO$\pi$:

\begin{equation}
  \lderives{y} \lderives{y} \lderives{y} \lderives{y} \lderives{y} \lderives{z}
\end{equation}

Formally, the two are \emph{weakly bisimilar}.  A \emph{bisimulation} is
a symmetric binary relation between two processes, which exists if each
process can simulate the behaviour of the other.  $R$ is such a relation
iff, for all pairs of processes $(p,q)$ in $R$ and all actions,
$\alpha$\footnote{The bisimulation definition given here is more
applicable to the static systems of CCS.  Although it holds for this
simple example, a more detailed method of bisimulation is required to
handle the dynamic binding that occurs in the $\pi$ calculus and its
derivatives.}:

\begin{enumerate}
\item $P \derives{\alpha} P^\prime \implies \exists Q^\prime\ such\
  that\ Q \derives{\alpha} Q^\prime\ and\ (P^\prime,Q^\prime) \in R$
\item $Q \derives{\alpha} Q^\prime \implies \exists P^\prime\ such\
  that\ P \derives{\alpha} P^\prime\ and\ (P^\prime,Q^\prime) \in R$
\end{enumerate}

For a weak bisimulation, $\tau$ transitions are effectively ignored.
A series of such transitions,
$\derives{\tau}\derives{\tau}\derives{\tau}\dots$ is abbreviated to
$\obsderives{\tau}$ and $\obsderives{\tau} \derives{a}
\obsderives{\tau}$ is deemed equivalent to $\derives{a}$.  As the
additional $\tau$ transition in the $Plus$-based derivation is the
only difference between the two, the two can be deemed equivalent
under the rules of weak bisimulation.

Returning to HO$\pi$, the most interesting point about this calculus
is not that it provides the means to formulate abstractions of the
type just demonstrated, but that, in doing so, it adds no further
expressivity.  Indeed, Sangiorgi, in his thesis \cite{sangiorgi:phd}
demonstrates how a HO$\pi$ calculus can be represented in the $\pi$
calculus.  Thus, just as with the polyadic variant, the benefit of
using HO$\pi$ comes not from increased expressivity, but from the
additional ease it provides in modelling certain scenarios.

\subsubsection{The Join Calculus}
\label{join}

The Join calculus \cite{join} takes the asynchronous $\pi$ calculus as
its basis, and focuses on providing a formalism better suited as the
basis for a distributed implementation.

Take the following example of a $\pi$ calculus process given in
\cite{joinresults}:

\begin{equation}
x(y).P\;|\;x(z).Q\;|\;\overline{x}a
\end{equation}

\noindent where two processes are waiting to receive input on $x$.
The problem with implementing this in a distributed setting is that
there is no concept of location with the $\pi$ calculus.  Each of the
two receiving processes or \emph{receptors}\footnote{The join calculus
  uses an analogy with chemistry to describe its behaviour, based on
  the \emph{CHemical Abstract Machine} (CHAM) \cite{cham}.} may be
located at an arbitrary distance both from each other and from the
transmitter, $\overline{x}a$.  As a result, a \emph{distributed
  consensus problem} arises as to which of the two receptors will
receive the transmission.

The join calculus provides a solution to this problem by altering the
syntax of the $\pi$ calculus.  The asynchronous variant of the
syntax given in Eqn. \ref{pisyntax} becomes:

\begin{align}
\label{joinsyntax}
  P, Q\ & ::=\ 
  0\ |\ 
  \mathtt{def}\ D\ \mathtt{in}\ P\ |\
  (P\;|\;Q)\ |\ 
  x\langle \tilde{v} \rangle \\
  D, E\ & ::=\
  J \rhd P\ |\
  D \wedge E\ |\ 
  \mathbf{T} \\
  J,J^\prime\ & ::=\ 
  x\langle \tilde{v} \rangle\ |\
  (J\;|\;J^\prime)
\end{align}

\noindent with $\mathbf{T}$ being the empty definition and a clear
focus on linking the receptors in $D$ to the emissions occurring in $P$
(both represented by the same syntax, $x\langle \tilde{v} \rangle$).
The use of this is most clearly demonstrated by example:

\begin{equation}
  \mathtt{def}\ (x\langle y \rangle \rhd P) \wedge (x\langle z \rangle \rhd Q)\ \mathtt{in}\ x \langle a \rangle
\end{equation}

\noindent which has essentially the same behaviour as the $\pi$
calculus example presented earlier.  $x\langle y \rangle \rhd P$
receives an input, $y$, on $x$ and then continues as $P$.  $x\langle y
\rangle$ is said to guard $P$, and multiple such guards may be applied
to a single such process.  Multiple such receptors may be defined via
use of the $\wedge$ operator.

It is impossible to provide an exact equivalent to the earlier series
of $\pi$ calculus processes, as the changes in the join calculus now
prevent such scenarios from being created.  Instead, the equivalent of
this join calculus example in the $\pi$ calculus is:

\begin{equation}
(x)(!(x(y).P\;|\;x(z).Q)\;|\;\overline{x}a)
\end{equation}

\noindent where the scope of $x$ is restricted to the $\mathtt{def}$
expression and the inputs are replicated, so as to be always
available.  Thus, a channel $x$ is always \emph{localised} to a
particular set of emitters and receptors.

Clearly, the join calculus, as a reformulation of the asynchronous
$\pi$ calculus with a new syntax, can not be used to express anything
which can't be expressed in the $\pi$ calculus.  However, it has a lot
of advantages in endowing the calculus with distributive properties at
the syntactic level.\footnote{Such changes have also been made using the
restrictions imposed by an appropriate type system \cite{sangiorgi:uniformreceptiveness}.}

\subsubsection{Advantages and Disadvantages of the $\pi$ Calculus}

The $\pi$ calculus is a powerful formalism drawn from a minimal
abstract syntax.  As noted at the start of this section, it is capable
of encoding the $\lambda$ calculus and so it follows that it is also
capable of simulating any recursive function.

The problem is that this makes it a little too powerful in some cases.
From \cite{sangiorgi:types-or}, we can see how much more difficult the
additional power given by the $\pi$ calculus makes proving
termination.  In contrast, a sufficiently restricted form of CCS
provides a trivial proof.  In the same paper, Sangiorgi also touches
on something which seems common within the literature
\cite{join,stefani:kells,wojciechowski:phd,failure2}; while the
expressiveness of the $\pi$ calculus is interesting, it is necessary
to restrict it in order to actually have something which is generally
useful for reasoning over or using as the basis for a full programming
language.  Specifically, to prove termination for the $\pi$ calculus,
it is necessary to employ the asynchronous variant with uniform
receptiveness and the input-guarded replication operator.

Another problem with the $\pi$ calculus is that it carries with it a
trait from CCS.  Namely, it can't be used to model synchronisation
with an arbitrary number of processes in a compositional way.  This
was considered earlier in \ref{ccslimit} for CCS, and solved in
\ref{timing} using the additions to the calculus given by TPL.  While
the $\pi$ calculus has a notion of mobility and is thus more
expressive than CCS, it still lacks an external entity with which to
co-ordinate such a transaction.  

A common motif reoccurs here, that was touched on earlier in the
introduction to this review; even though something has a certain level
of expressivity, it doesn't follow that it is the most appropriate
mechanism for modelling a particular phenomenon.  This also holds for
the distributed calculi considered in \ref{migration}.  The $\pi$
calculus may already model mobility, but these calculi do so in a
different way, which may prove more suitable in a particular context.
 
\subsection{Distribution and Migration}
\label{migration}

Allowing the scope of a name to change during execution is one possible
way of modelling dynamic behaviour, but it isn't the only way.  The
concept of \emph{mobility} naively implies the physical movement of
processes, but, as shown above, this is not what actually happens in the
$\pi$ calculus.  To do so requires some notion of \emph{distribution};
this can be provided by \emph{localities}, a term used to refer
generally to a higher-level form of grouping, above that of processes.
This concept has been applied to various calculi, in different forms, in
order to model physical sites \cite{wojciechowski:phd}, administrative
or security domains \cite{amb,seal} and biological cells \cite{brane04},
but can theoretically be applied in any context where the grouping of
processes is useful.  Localities can be used simply for observation or
as a means to further control the behaviour of the processes
encapsulated within them.  They are generally named, so as to provide a
communication target or a known destination for a migrating entity.

Originally, localities were used to distinguish between processes in
order to provide further equivalence theories.  Take the following simple
CCS-based example process:

\begin{equation}
\label{lccsspec}
Spec \eqdef in.\tau.\overline{out}.Spec
\end{equation}

\noindent which forms the \emph{specification} for the behaviour of a
system that receives an input, processes it and then returns the output.
The actual \emph{implementation} may differ from the specification by
instead involving two processes:

\begin{align}
\label{lccs2proc}
Receiver & \eqdef in.\overline{a}.Receiver \\
Sender & \eqdef a.\tau.\overline{out}.Sender
\end{align}

\noindent which communicate over another channel, $a$.  If these two
processes are run concurrently:

\begin{equation}
(Receiver\;|\;Sender)\setminus a
\end{equation}

\noindent with the scope of $a$ restricted, they are \emph{weakly
bisimilar} (see \ref{pivariants}) to one another.  The specification
performs the following derivations:

\begin{equation}
  \lderives{in} \lderives{\tau} \lderives{\overline{out}}
\end{equation}

\noindent prior to recursing and becoming $Spec$ again, whereas the
implementation produces:

\begin{equation}
  \lderives{in} \lderives{\tau} \lderives{\tau} \lderives{\overline{out}}
\end{equation}

\noindent with the extra $\tau$ transition caused by the synchronisation
on $a$.  As weak bisimulation effectively ignores $\tau$ actions, the
two are judged to be equivalent.  If the specification was to include a
further $\tau$ action, for an arbitrary reason, prior to the
$\overline{out}$, then the two would also be strongly bisimilar.  To
summarise, the difference between the two sets of derivations is
negligible, according to the bisimulation, yet the actual
difference between the specification and its implementation is fairly
significant.  The specification effectively requests a monolithic
solution, but weak bisimulation allows the final implementation to be
distributed over multiple processes.

In most situations, this is beneficial.  It means that the specification
can be met by a concurrent system, composed of multiple processes
running in parallel, superfluous $\tau$ transitions aside.  When a
distinction between the number of processes used is required, a finer
equivalence is needed.  \emph{Location bisimulation} \cite{obslocal}
provides exactly that, by assigning locations to processes and using
them as part of the relation between processes.

Essentially, this means that each transition is annotated with a
location name.  A variant of CCS, LCCS, adds an additional piece of
syntax, $l::E$ to signify that a process $E$ is located at $l$.  This
association is made within the operational semantics, of which there are
two variants.  The \emph{static} approach allocates locations initially,
while the \emph{dynamic} method generates a new location for each
non-silent transition.  Here, the focus is on the latter, shown in Table
\ref{tab:lccssemantics}, which essentially gives each process a
\emph{causal path}, by explicitly representing the number of transitions
that have been performed.

\begin{table}
  \caption{LCCS Dynamic SOS Rules}
  \label{tab:lccssemantics}
  \shrule
 \begin{center}
    \begin{tabular}{lcr}
      \Rule{\textsf{Act1}}
      {-}
      {a . E \xrightarrow[l]{a} l::E}
      {for\ any\ l \in Loc}
      &
      \Rule{\textsf{Act2}}{E \xrightarrow[u]{a} E^\prime}
      {l::E \xrightarrow[lu]{a} l::E^\prime}
      {}
      &
      \Rule{\textsf{Act3}}
      {-}
      {\tau . E \derives{\tau} E}
      {}
     \end{tabular}
  \end{center}
 \shrule
\end{table}

The semantics, as with those for CaSE and TNT given in chapter
\ref{currentwork}, are based on a \emph{labelled transition system}.
The possible behaviour of a process is defined as a series of labelled
transitions from one process to another, which are later used as the
basis for the bisimulation-based equivalence theories shown earlier.
The rules presented here are only a subset of those for LCCS, being
those that are relevant to the use of locations.  The remaining rules
for summation, parallel composition and restriction are as for CCS
itself, with the additional inclusion of the location on the transition.
These are discussed informally in section \ref{ccs}, and also appear as
part of the CaSE semantics.  

The rule, $\textsf{Act1}$, handles the initial assignment of a
location for any action, $a.E$, where $a \in \mathcal{N} \cup
\overline{\mathcal{N}}$ (i.e. $a \ne \tau$) and $Loc$ is simply a set
of location names.  The rule states that the process may perform a
transition to the process $l::E$.  The transition itself is annotated
with both the action $a$ and the new location, $l$, which causes the
locations to appear in the sequence of transitions for each process
(and, thus, the equivalence theory).

$\textsf{Act2}$ is a continuation of $\textsf{Act1}$, which handles
processes that have already been assigned a location.  If the process
itself, $E$, can perform some action, $a$, with the location, $u$, to
become $E^\prime$, then so can the located version of $E$.  The
interesting part of this rule is how the location is used in the new
transition.  The $u$ from the new transition is concatenated with the
$l$ from the current location, so the transition depicts the specific
route the process has taken through each location.  The final rule,
$\textsf{Act3}$, simply handles silent actions, which are unaltered from their
behaviour in CCS, and have no association with locations.

How this actually works in practice is best shown by reconsidering the
earlier CCS example.  Recall the specification defined in
\ref{lccsspec}.  This is a process with essentially three actions, $in$,
$\tau$ and $\overline{out}$, which may be localised via use of the LCCS
semantics given above.  As the process begins its life in an unlocated
form, $\textsf{Act1}$ is applied to assign it a location:

\begin{equation}
in.\tau.\overline{out}.Spec \locderives{l}{in}
l::\tau.\overline{out}.Spec
\end{equation}

\noindent where $l$ is an arbitrary location name\footnote{The name is
  arbitrary in the sense that it doesn't matter what the name is, but,
  as the later discussion of bisimulation shows, the location names
  must be assigned in some kind of regular fashion to facilitate
  comparison.}.  The evolution of the resulting process,
$l::\tau.\overline{out}.Spec$ utilises both $\textsf{Act2}$ and
$\textsf{Act3}$.  $\textsf{Act2}$ provides the appropriate transition
for such a located process, but its behaviour is based on that of the
unlocated process, which in this case is $\tau.\overline{out}.Spec$.
Thus, $\textsf{Act3}$ is used to yield:

\begin{equation}
\tau.\overline{out}.Spec \derives{\tau} \overline{out}.Spec
\end{equation}

\noindent which is then applied as the precondition for
$\textsf{Act2}$ to give:

\begin{equation}
l::\tau.\overline{out}.Spec \locderives{l}{\tau} l::\overline{out}.Spec
\end{equation}

\noindent As $u$ is effectively the empty string, $\epsilon$, in this
case, due to the $\tau$ transition being unlocated, the result of the
concatenation, $ul$, is simply $l$.

The final derivation again combines the use of $\textsf{Act2}$ with another
rule.  This time, the action is a member of $\overline{\mathcal{N}}$,
so $\textsf{Act1}$ is used to give the derivation of the unlocated
variant, $\overline{out}.Spec$:

\begin{equation}
\overline{out}.Spec \locderives{k}{\overline{out}} k::Spec
\end{equation}

\noindent where $k$ is again an arbitrary location assigned to the new
visible action.  Merging this with the main process using
$\textsf{Act2}$ gives:

\begin{equation}
l::\overline{out}.Spec \locderives{lk}{\overline{out}} l::k::Spec
\end{equation}

\noindent resulting in a final process with a causal path of two
locations, $l$ and $k$.

But how does this help distinguish the specification from its dual
process implementation shown previously?  First, it is necessary to
extend the definition of bisimulation given in \ref{pivariants} to
incorporate the localised transitions of LCCS.  Recall that a
\emph{bisimulation} is a symmetric binary relation between two
processes, which exists if each process can simulate the behaviour of
the other.  $R \subseteq LCCS \times LCCS$ is a \emph{dynamic location
bisimulation} relation iff, $\forall (p,q) \in R \wedge a \in \mathcal{N}
\cup \overline{\mathcal{N}} \wedge u \in Loc$:

\begin{enumerate}
\item $P \locderives{u}{a} P^\prime \implies \exists Q^\prime\ such\
  that\ Q \locderives{u}{a} Q^\prime\ and\ (P^\prime,Q^\prime) \in R$
\item $Q \locderives{u}{a} Q^\prime \implies \exists P^\prime\ such\
  that\ P \locderives{u}{a} P^\prime\ and\ (P^\prime,Q^\prime) \in R$
\item $P \derives{\tau} P^\prime \implies \exists Q^\prime\ such\
  that\ Q \derives{\tau} Q^\prime\ and\ (P^\prime,Q^\prime) \in R$
\item $Q \derives{\tau} Q^\prime \implies \exists P^\prime\ such\
  that\ P \derives{\tau} P^\prime\ and\ (P^\prime,Q^\prime) \in R$
\end{enumerate}

\noindent This is the strong variant that observes $\tau$ transitions.
A localised version of weak bisimulation merely requires satisfying the
first two conditions.  As the earlier comparison between the two
processes was made using weak bisimulation, it is this weak variant of
dynamic location bisimulation that will be used here.

The implementation with two processes, shown in \ref{lccs2proc}, had the
following transitions using plain CCS:

\begin{equation}
  \lderives{in} \lderives{\tau} \lderives{\tau} \lderives{\overline{out}}
\end{equation}

\noindent whereas the specification exhibits the following behaviour in
LCCS:

\begin{equation}
  \locderives{l}{in} \locderives{l}{\tau} \locderives{l}{\tau} \locderives{lk}{\overline{out}}
\end{equation}

\noindent To compare the two, it is necessary to give a similar
localised treatment to the transitions for the implementation.  Clearly,
the $\tau$ transitions will be relatively unaffected, and, under a weak
form of bisimulation, are irrelevant anyway.  Essentially, the two
sequences being compared are:

\begin{align}
& \locderives{l}{in} \locderives{lk}{\overline{out}} \tag{Specification
(Localised)} \\
& \lderives{in} \lderives{\overline{out}} \tag{Implementation}
\end{align}

\noindent when the $\tau$ transitions are ignored.  To localise the
latter of these, it is necessary to look back to the original two
processes from which these transitions are derived.  The first,
$\lderives{in}$, arises from the $Receiver$ as follows:

\begin{equation}
in.\overline{a}.Receiver \derives{in} \overline{a}.Receiver
\end{equation}

\noindent which, when localised, becomes:

\begin{equation}
in.\overline{a}.Receiver \locderives{l}{in} l::\overline{a}.Receiver
\end{equation}

\noindent So, the first of the two transitions should be
$\locderives{l}{in}$ when LCCS is used.

However, the use of $a$ makes things a little complicated.  It appears
in both the $Receiver$ (as just shown) and the $Sender$ as a visible
action ($a$ and $\overline{a}$ respectively), but these combine to
become a $\tau$ action when the two are run in parallel.  The above
makes it appear that the $Receiver$ will evolve to $l::k::Receiver$, by
assigning a further location to $a$, but this doesn't match with the
higher-level behaviour of the composed processes.  Thus, to make
assigning locations easier, it is better to look instead at the
sequences of transitions from each process, rather than their explicit
definitions:

\begin{align}
& \lderives{in} \lderives{\tau} \tag{Receiver} \\
& \lderives{\tau} \lderives{\tau} \lderives{\overline{out}} \tag{Sender}
\end{align}

\noindent where the $\tau$ transition arising from the synchronisation
is given for both.  From this, it is a simple matter of assigning a
location to each observable action:

\begin{align}
& \locderives{l}{in} \lderives{\tau} \tag{Localised Receiver} \\
& \lderives{\tau} \lderives{\tau} \locderives{l}{\overline{out}}
\tag{Localised Sender}
\end{align}

\noindent and merging the two to give a localised version of both the
specification and its implementation:

\begin{align}
& \locderives{l}{in} \locderives{lk}{\overline{out}} \tag{Specification
(Localised)} \\
& \locderives{l}{in} \locderives{l}{\overline{out}} \tag{Implementation (Localised)}
\end{align}

\noindent which illustrates a clear difference between the two.

For the first transition, the two can match each other, as both are
capable of performing $\locderives{l}{in}$.  However, the relation
breaks down on the second transition which compares $\locderives{lk}{\overline{out}}$
with $\locderives{l}{\overline{out}}$.  Under a normal weak
bisimulation, these two transitions would be judged equivalent, as only
the action is available for comparison; both perform an
$\overline{out}$.  However, a localised bisimulation requires the
locations to also match, which fails here.  The specification has a
longer causal path, as its single process has performed two visible
actions.  In contrast, the two processes involved in the implementation
have performed one action each, resulting in two separate paths with a
length of one.

This shows that localities can be used to provide a stronger equivalence
theory; a dynamic location bisimulation can distinguish more processes
than a standard bisimulation.  As stated earlier, localities are now
more commonly used in calculi which exhibit mobility in the form of
\emph{migration}, where they are used to group arbitrary numbers of
processes.  The locality gives the grouping a context, which may change
during execution of the system, via the movement of the locality or its
constituent processes.  What follows is a further examination of such
distributed calculi, including those which have arisen from existing
non-distributed formalisms, such as the Join calculus.

%\subsubsection{The Distributed $\pi$ Calculus}

%The distributed $\pi$ calculus, or D$\pi$ \cite{hennessy:dpi98},

\subsubsection{The Distributed Join Calculus}

By adding localities, \cite{djoin} defines a distributed variant of the
Join calculus shown in \ref{join}.  The extended syntax is as follows:

\begin{align}
\label{djoinsyntax}
  P, Q\ & ::=\ 
  0\ |\ 
  \mathtt{def}\ D\ \mathtt{in}\ P\ |\
  (P\;|\;Q)\ |\ 
  x\langle \tilde{v} \rangle \pc
  go \langle b, \kappa \rangle \\
  D, E\ & ::=\
  J \rhd P\ |\
  D \wedge E\ |\ 
  \mathbf{T} \pc 
 a[D : P]
 \\
  J,J^\prime\ & ::=\ 
  x\langle \tilde{v} \rangle\ |\
  (J\;|\;J^\prime)
\end{align}

\noindent with the additional syntax of $a[D : P]$ representing input
channels located at $a$, the name of the locality. $P$ is used to
`initialise' the locality.  The names are globally scoped and unique
to a particular definition, so:

\begin{equation}
\mathtt{def}\ a[D:P] \wedge a[D':Q] \rhd R\ \mathtt{in}\ S
\end{equation}

\noindent is disallowed.  The syntax allows localities to be nested to
form a hierarchical structure, with each node in the tree
corresponding to a different location.  All receptors for a channel
must occur in the same location.  The following is disallowed,

\begin{equation}
  \mathtt{def}\ a[x \langle y \rangle \rhd P : S] \wedge b[x \langle z \rangle \rhd Q : R]\ \mathtt{in}\ T
\end{equation}

\noindent as one receptor for $x$, $P$, is defined in location $a$ and
the other in location $b$.  Instead, 

\begin{equation}
  \mathtt{def}\ a[x \langle y \rangle \rhd P \wedge x \langle z \rangle \rhd Q : R]\ \mathtt{in}\ T
\end{equation}

\noindent may be used, where both $P$ and $Q$ are in location $a$.

Migration may occur using the new process construct, $go \langle b,
\kappa \rangle$.  Rather than the process itself migrating, this
operator causes the surrounding location to migrate and become an
immediate sub-location of $b$.  Upon completion of the migration, an
empty message is emitted on $\kappa$.  This allows other processes to
block until the migration is complete, by waiting for receipt of this
completion message.  For example,

\begin{equation}
\mathtt{def}\ a[D:(P \pc go \langle b, \kappa \rangle)]\ \mathtt{in}\ S
\pc \mathtt{def}\ b[E : Q] \mathtt{in}\ T
\end{equation}

\noindent reduces to:

\begin{equation}
\mathtt{def}\ b[E : Q \pc (\mathtt{def}\ a[D:(P \pc k\langle \rangle)]\ \mathtt{in}\ S)] \mathtt{in}\ T
\end{equation}

\noindent when $go \langle b, \kappa \rangle$ is expanded, with $a$
now a sub-location of $b$.

The distributed join calculus is an interesting example of how an
existing calculus (the $\pi$ calculus in this case) can be both
adapted to suit a different purpose or remove perceived deficiencies
(as shown in \ref{join}) and then later extended to incorporate
mobility via distribution, via the simple addition of localities and a
migration primitive.  The advantage of this is that the new calculus
can build on the established theory of the original calculus, instead
of having to start from scratch.  This differs from the approach taken
by the ambient calculus, which instead begins again from first
principles, in an attempt to formalise this more spatial form of
mobility in a minimal fashion.

%\subsubsection{Nomadic Pict}

%Pawel Wojciechowski defines, in his PhD thesis \cite{wojciechowski:phd},
%an extension to PICT \cite{daveturner:phd} which incorporates
%distribution.

\subsubsection{The Ambient Calculus}
\label{ambientcalculus}

The ambients within the ambient calculus \cite{amb} are a form of
locality.  Each ambient can contain processes and other ambients,
allowing a nested structure of ambients to be formed.  This topology
is dynamic; new ambients may be created and existing ones moved or
destroyed during execution.  Within the formal syntax of the calculus,

\begin{equation}
\label{ambsyntax}
  E, F\ ::=\ 
  0\ |\ 
  M.E\ |\ 
  (\nu n)E\ |\ 
  (E\ |\ F)\ |\ 
  n[E]\ |\ 
  !E
\end{equation}

\noindent the ambients are represented by the term $n[E]$, where $n$
is an ambient name.  In comparing this with the syntax given for CCS
in Eqn. \ref{ccssyntax} and that of the $\pi$ calculus from Eqn.
\ref{pisyntax}, some apparent similarities can be seen, especially
with regard to the latter.  The same nil process, $0$, is present, as
is parallel composition and replication.  $(\nu n)E$ looks similar to
restriction\footnote{This is the syntax used in versions of the $\pi$
  calculus later than \cite{funcproc}.}.  Continuing on this
presumption, $M.E$ may be considered to be the prefixing already seen
in CCS and the $\pi$ calculus.  However, the syntax for $M$ is

\begin{equation}
\label{ambsyntaxcap}
  M\ ::=\ 
  in\ n\ |\
  out\ n\ | \
  open\ n
\end{equation}

\noindent which is quite different from that of action prefixing.  The
ambient calculus has no concept of channels; the only names present
refer to ambients (so $(\nu n)E$ restricts these).  What $M$ provides
is a set of mobility primitives, known as \emph{capabilities}.
Processes emit these in order to alter the structuring of the
ambients, and thus perform the physical migration of ambients and the
processes within them.

Perhaps the most confusing aspect of capabilities is that they are
emitted by the process, but it is the ambient that actually moves.
For example, if process $P$ is defined as $in\ n.0$, then performing
this action has the effect of moving the \emph{ambient} in which $P$
resides inside $n$, rather than just $P$.  Likewise, $out\ n$ is the
converse and moves the surrounding ambient outside $n$.

Such behaviour is best illustrated by an example. Suppose the process,
$in\ n.out\ n.P$ begins its life in the ambient $m$
(Fig. \ref{fig:ambient1}).  Performing the first action, $in\ n$,
moves its surrounding ambient, $m$, inside $n$
(Fig. \ref{fig:ambient2}).  The converse, $out\ n$, then moves $m$
back outside $n$, resulting in a return to the original ambient
structure (Fig. \ref{fig:ambient3}), but with the process having
evolved into $P$.

\begin{figure}  
  \centering
  \includegraphics[scale=0.3]{ambient1}
  \caption{Spatial diagram of $m[\ambin{n}.\ambout{n}.P] \mid n[]$}
  \label{fig:ambient1}
\end{figure}

\begin{figure}  
  \centering
  \includegraphics[scale=0.3]{ambient2}
  \caption{Spatial diagram of $n[m[\ambout{n}.P]]$}
  \label{fig:ambient2}
\end{figure}

\begin{figure}  
  \centering
  \includegraphics[scale=0.3]{ambient3}
  \caption{Spatial diagram of $m[P] \mid n[]$}
  \label{fig:ambient3}
\end{figure}

$open\ n$ is quite different.  It alters the structure, just as $in$ and
$out$ do, but rather than moving ambients, it destroys them.  It is also
applied to a child ambient rather than to the surrounding ambient, so
$open\ m.P\ |\ m[Q]$ (as in \cite{amb}) reduces to $P\ |\ Q$.

There are also issues with regard to the applicability of capabilities
and the use of the names.  A capability may only cause movement to occur
when at least one applicable ambient is available.  As such, movement
is heavily dependent on context, and specifically the availability of an
appropriately named ambient.  Applicability is dependent upon the
capability involved:

\begin{itemize}
\item For $\ambin{m}$, there must be a sibling of the surrounding
ambient named $m$.
\item For $\ambout{m}$, the parent of the surrounding ambient must be
named $m$.
\item For $\ambopen{m}$, there must be a child of the surrounding
  ambient named $m$.
\end{itemize}

All three capabilities are non-deterministic.  The same ambient name may
occur more than once, and each occurrence is regarded as being distinct.
As a result, the reduction of a capability includes a choice if there is
more than one applicable ambient present.  For example, $open\ m.P\ |\
m[Q]\ |\ m[R]$ has two possible derivations,

\begin{enumerate}
\item $open\ m.P\ |\ m[Q]\ |\ m[R] \rightarrow P\ |\ Q\ |\ m[R]$
\item $open\ m.P\ |\ m[Q]\ |\ m[R] \rightarrow P\ |\ m[Q]\ |\ R$
\end{enumerate}

The issue of non-determinism illustrates the behaviour that occurs when
there is more than one applicable ambient.  What about when there are
none?  The process stalls, and can not move on until such an ambient
becomes available.  This is akin to the situation in channel-based
calculi, such as CCS or the $\pi$ calculus, where a name is restricted,
but the appropriate co-name is not available to provide synchronisation.
For example,

\begin{equation}
(a.P) \backslash a 
\end{equation}

\noindent may never progress to become $P$ as there is no $\overline{a}$
for $a$ to synchronize with.  This behaviour is particularly relevant
with respect to $\ambout{m}$, where the sole use of the name is to stop
the surrounding ambient leaving its parent if the names don't match.

The restriction of ambient names, via $(\nu n) E$, combined with
mobility means that scope extrusion is also present in the calculus.
Just as the transmission of a name outside its scope causes extrusion
in the $\pi$ calculus, the restriction of ambient names may float
outward as necessary.  Scope intrusion is also possible in both
calculi, as demonstrated by the presence of the structural congruence
rule,

\begin{equation}
(\nu n)(P \pc Q) \equiv P \pc (\nu n) Q\ \text{if}\ n \not \in fn(P) \tag{Struct Res Par}
\end{equation}

\noindent which allows the restriction of $n$ to be removed from $P$
if the name doesn't occur free within its body.

\subsubsection{Variants of the Ambient Calculus}
\label{ambvariants}

A general problem within concurrency is the possibility of
\emph{interference}.  This was touched on briefly in the introduction
to this review, where the value of $x$ differed due to a race
condition.  In the ambient calculus, \emph{redex interference}
\cite{sangiorgi:mobsafeambients} is an issue, and is related to the
non-determinism mentioned above.

Take the example process from \cite{sangiorgi:mobsafeambients}.

\begin{equation}
n[\ambin{m}.P] \pc m[Q] \pc m[R]
\end{equation}

\noindent It is unclear what the environment of $P$ will be, following
the reduction of the capability, $\ambin{m}$.  There are two
alternatives,

\begin{enumerate}
\item $n[\ambin{m}.P] \pc m[Q] \pc m[R] \rightarrow m[n[P] \pc Q] \pc m[R]$
\item $n[\ambin{m}.P] \pc m[Q] \pc m[R] \rightarrow m[Q] \pc m[n[P] \pc [R]]$
\end{enumerate}

\noindent resulting from the two redexes formed between
$n[\ambin{m}.P]$ and $m[Q]$, and $n[\ambin{m}.P]$ and $m[R]$.  If one
contracts, resulting in a reduction, the other is no longer possible.
However, in this case, all three processes, $P$, $Q$ and $R$, can
still interact following either reduction.

In another example from the same paper,

\begin{equation}
\ambopen{n}.P \pc \ambopen{n}.Q \pc n[R]
\end{equation}

\noindent again with two possible interactions

\begin{enumerate}
\item $\ambopen{n}.P \pc \ambopen{n}.Q \pc n[R] \rightarrow P \pc \ambopen{n}.Q \pc R$
\item $\ambopen{n}.P \pc \ambopen{n}.Q \pc n[R] \rightarrow
  \ambopen{n}.P \pc Q \pc R$
\end{enumerate}


\noindent the resulting process includes a process, either $\ambopen{n}.Q$
or $\ambopen{n}.P$, which is stuck until such a time as another ambient
named $n$ appears as a parent.  This may never occur.  These kinds of
interference, referred to in \cite{sangiorgi:mobsafeambients} as
\emph{plain interferences}, may occur in other calculi.  The equivalent
in the $\pi$ calculus would be:

\begin{equation}
\overline{x}z.P \pc x(y).Q \pc x(y).R
\end{equation}

\noindent where again a reduction will occur between one of the two:

\begin{enumerate}
\item $\overline{x}z.P \pc x(y).Q \pc x(y).R \rightarrow P \pc Q\{z/y\}
  \pc x(y).R$
\item $\overline{x}z.P \pc x(y).Q \pc x(y).R \rightarrow P \pc x(y).Q
  \pc R\{z/y\}$
\end{enumerate}

\noindent and the remaining process, either $x(y).Q$ or $x(y).R$, will
be blocked.

Another more serious form of interference may occur in the ambient
calculus, due to the provision of differing interactions ($\ambin{m},
\ambout{m}$ and $\ambopen{m}$).  These \emph{grave interferences} occur
when an ambient is involved in two reductions occurring as the result
of different types of capability.  Take the example process,

\begin{equation}
\ambopen{n}.\nil \pc n[\ambin{m}.P] \pc m[Q]
\end{equation}

\noindent in which two reductions can occur that are logically
different.  While the interferences described above are a
representation of the kind of race conditions and non-determinism that
would be expected in any concurrent model, for example, to represent
competition for resources, grave interferences are usually unexpected
and typically represent errors in the model.  This process may perform
two radically different reductions,

\begin{enumerate}
\item $\ambopen{n}.\nil \pc n[\ambin{m}.P] \pc m[Q] \rightarrow \nil \pc
\ambin{m}.P \pc m[Q]$
\item $\ambopen{n}.\nil \pc n[\ambin{m}.P] \pc m[Q] \rightarrow
\ambopen{n}.\nil \pc m[n[P] \pc Q]$
\end{enumerate}

\noindent where either $n$ is destroyed, thus preventing the latter
movement of $P$ in to $m$ as it has no surrounding ambient, or $n$ moves
inside $m$ and is no longer available to be destroyed by
$\ambopen{n}.\nil$.  Clearly, only one of these reductions is likely to be
intentional.  

Levi and Sangiorgi's calculus of Mobile Safe Ambients \cite{safeamb00,
sangiorgi:mobsafeambients} presents a solution to this.  It introduces a
notion of co-capabilities, which enforce a pairing of mobility
primitives before a reduction can be made.  The result of this is that
the ambient being entered, exited or opened is aware of what is taking
place, and may react accordingly.

With these co-capabilities in place, the reduction rules for the
calculus run as follows:

\begin{align}
 n[\ambin{m}.P_1 \pc P_2] \pc m[\sambin{m}.Q_1 \pc Q2]
\rightarrow
m[n[P_1 \pc P_2] \pc Q_1 \pc Q_2] \tag{SafeIn}\\
 m[n[\ambout{m}.P_1 \pc P_2] \pc \sambout{m}.Q_2 \pc Q2]
\rightarrow
n[P_1 \pc P_2] \pc m[Q_1 \pc Q_2] \tag{SafeOut}\\
 \ambopen{n}.P \pc n[\sambopen{n}.Q_1 \pc Q_2]
\rightarrow
P \pc Q_1 \pc Q_2 \tag{SafeOpen} 
\end{align}

\noindent where, in each case, the capability must be able to
synchronize with a co-capability in the relevant ambient for the
reduction to take place.  For example, in SafeIn, $\ambin{m}.P_1$ must
pair up with $\sambin{m}.Q_1$ in the ambient $m$.  As a result, $Q_1$
can react appropriately to the change in structure, based on the fact
that it knows the movement has occurred.

The changes in the calculus of safe ambients, though simple, have a
dramatic effect on the ability to construct an algebraic theory for
the calculus and prove properties, especially when coupled with an
appropriate type system\footnote{In this case, the type system ensures
  single-threadedness, where only one process within an ambient may
  exercise a capability.}.  Essentially, they represent a move from
asynchronous to synchronous mobility primitives.  The calculus of
controlled ambients \cite{controlledamb02} restricts behaviour
further, by requiring that a co-capability must appear in both the
source and the destination.  Thus, an $\ambin{m}$ capability requires
permission both to leave its current location and to enter the
destination ambient. This is useful for the specific application of
the calculus, controlling resources, but is excessive in most
circumstances.

A further variant of the ambient calculus is the calculus of boxed
ambients \cite{boxedamb01}.  This removes the $open$ capability
altogether, replacing it with a form of directed communication inspired
by \cite{seal}.  Processes remain within their initial ambient
permanently (hence the term `boxed') and only the structure of the
ambient topology changes via the $in$ and $out$ capabilities.  Messages
may be sent locally, upwards or downwards, but not to siblings.

An example process from \cite{boxedamb01} is:

\begin{equation}
n[(x)^pP \pc p[\langle M \rangle \pc (x)Q \pc q[\langle N
\rangle^\uparrow ]]]
\end{equation}

\noindent where $n$, $p$ and $q$ are ambients, $(x)$ is an input and
$\langle M \rangle$ and $\langle N \rangle$ represent outputs.  $(x)Q$
may synchronize with either $\langle M \rangle$ locally or the upward
communication from $\langle N \rangle$.  $(x)^pP$ must synchronize with
$\langle M \rangle$, as the only output in $p$.

The ideas behind the boxed ambients calculus result in a formalism which
is more suited to communication-focused modelling, where the destruction
of locations would be unnatural.  Both it and the original ambient
calculus have their own particular niche, being suited to particular
applications.  In contrast, the latter is clearly more suited to
situations where the removal of a locality corresponds to a similar
event in the real-world situation being modelled.

\subsubsection{Advantages and Disadvantages of the Ambient Calculus}

The most interesting aspect of the ambient calculus is that, while it
includes no communication primitives, it can encode the asynchronous
$\pi$ calculus (see \ref{pivariants}).  This seems to imply that it is
possible to model mobility in a more natural way without losing much of
the expressivity of the $\pi$ calculus.  On consideration , this seems a
little less surprising as ambient names exhibit the same scope extrusion
seen with channel names in the $\pi$ calculus.  With this in mind, it is
not too difficult to see that ambient names could be used to mimic
channel names, with synchronisation being emulated by two processes
performing some kind of interaction within the same ambient.

However, the representation of synchronisation illustrated in \cite{amb}
seems to suggest that the ambient calculus may still have problems
dealing with the kind of global synchronisation needed for the
compositional broadcast agent considered in \ref{ccslimit}.  The
operation is performed by destroying and recreating ambients, as a
signal to the other process involved in the synchronisation.  Extending
this would seem to require using more ambients, which again leads to the
problem of enumerating the number of entities who wish to synchronize.
As before, this is possible but not compositional; every time
synchronisation is performed with a different number of agents, the
semantics of the process must be recreated.

Thus, the ambient calculus and the $\pi$ calculus have more in common
than is initially apparent, and the choice between the two seems to be
largely based on the most natural formalism for a particular task.

%\subsubsection{The Seal Calculus}

%The seal calculus \cite{seal}

\subsubsection{P Systems}
\label{psystems}

While providing a way of modelling concurrent spatially-oriented
systems, P Systems \cite{paun:98membranes} arise from the area of formal
language theory and re-writing rules rather than process
calculi.  They are considered here, as there exist a number of
similarities between them and, for example, the ambient calculus both in
providing a distributed model of computation and in finding applications
in the area of biological modelling.

A P system or \emph{transition super-cell system}
\cite{paun:98membranes} of degree $n$, where $n \ge 1$ is represented
as:

\begin{equation}
\Pi = (V, \mu, M_1, \dots, M_n,(R_1,\rho_1), \dots, (R_n,\rho_n),i_0)
\end{equation}

\noindent where:

\begin{itemize}
\item $V$ is an alphabet of objects.
\item $\mu$ is the membrane structure, containing $n$ membranes.
\item $M_i$, where $1 \le i \le n$, is a multiset of objects from $V$
      which are contained in membrane $i$.
\item $R_i$, where $1 \le i \le n$ is an evolution rule associated with
      one of the membranes, $i$. The corresponding $\rho_i$ is a
      partial-order relation which determines the priority of the rule.
      The rules are rewriting rules of the form $a \rightarrow v$, which
      causes $a$ to be replaced by $v$.
\item $i_0$ is a number between 1 and $n$ which specifies the \emph{output
      membrane} where the result of the computation should be found.
\end{itemize}

\noindent Any of the multisets, rules or priority relations may be
empty.  Evolution occurs in parallel, in a synchronous fashion involving
all membranes (referred to as \emph{maximal parallelism}).  A universal
clock is assumed to exist, which breaks the evolution of the system into
cycles.  Objects may move between membranes and membranes may be broken,
causing their objects to flood into the membrane above and their rules to
disappear.  Such behaviour has echoes of the ambient calculus described
in \ref{ambientcalculus}, where ambients may be destroyed by the $open$
primitive and processes may move around the ambient hierarchy (but only
within an ambient).  The notion of synchronous clock cycles also recalls
the discrete timed calculi of \ref{timing}, where evolution can also be
bounded by clock cycles in a synchronous fashion.  An interesting
distinction is commonly made in P systems; the outer membrane or
\emph{skin membrane} is assumed to be special.  For example, at least in
a biological context, the system is assumed to terminate if the outer
membrane is destroyed (biologically, the external membrane has been
broken and thus the organism falls apart).

Consider the following example P system (Fig. \ref{fig:psystem}),
\begin{align*}
\Pi_1 & = (V, \mu, M_1, M_2, M_3, M_4, (R_1, \rho_1), (R_2, \rho_2),
 (R_3, \rho_3), (R_4, \rho_4),4) \\
V & = \{a,b,b',c,f\} \\
\mu & = [_1[_2[_3]_3[_4]_4]_2]_1 \\
M_1 & = \emptyset, 
R_1 = \emptyset,
\rho_1 = \emptyset \\
M_2 & = \emptyset, 
R_2 = \{b' \rightarrow b, b \rightarrow b(c, in_4), r_1 : ff
 \rightarrow af, r_2 : f \rightarrow a\delta\},
\rho_2 = \{r_1 > r_2\} \\
M_3 & = \{af\},
R_3 = \{a \rightarrow ab', a \rightarrow b'\delta, f \rightarrow ff\},
\rho_3 = \emptyset \\
M_4 & = \emptyset,
R_4 = \emptyset,
\rho_4 = \emptyset
\end{align*}

\begin{figure}  
  \centering
  \includegraphics[scale=0.3]{psystem}
  \caption{Example P System}
  \label{fig:psystem}
\end{figure}

\noindent where the only membrane that initially contains any objects is
$M_3$.  In $M_3$ are two objects, $a$ and $f$.  $f$ only matches one
rule, $f \rightarrow ff$, which causes the number of $f$s to double on
each evolution.  For $a$, there are two rules and one is chosen
non-deterministically.  If the first, $a \rightarrow ab'$, is applied,
then an additional object $b'$ appears, and the rule may be applied
again as an $a$ is still present.  If $a \rightarrow ab'$ and $f
\rightarrow ff$ are applied for $n$ steps, then $n$ instances of $b'$
and $2^n$ occurrences of $f$ are present.

If the second $a$ rule $a \rightarrow b'\delta$, is applied, the
$\delta$ causes the membrane, $M_3$, to be dissolved.  At this point,
there will be one extra $b'$ and one extra $f$ resulting from the
application of this rule and $f \rightarrow ff$, respectively, and no
$a$.  This changes the configuration of the system to become:
\begin{align*}
\mu & = [_1[_2[_4]_4]_2]_1 \\
M_1 & = \emptyset, 
R_1 = \emptyset,
\rho_1 = \emptyset \\
M_2 & = \{b'^{n+1}, f^{2n+1}\} \\ 
R_2 & = \{b' \rightarrow b, b \rightarrow b(c, in_4), r_1 : ff
 \rightarrow af, r_2 : f \rightarrow a\delta\},
\rho_2 = \{r_1 > r_2\} \\
M_4 & = \emptyset,
R_4 = \emptyset,
\rho_4 = \emptyset
\end{align*}

\noindent The three rules that were present in $M_3$ are lost, while the
objects float into the membrane above, $M_2$.  In this configuration,
$n$ represents the number of times the pair of rules $a \rightarrow ab'$
and $f \rightarrow ff$ were applied prior to this, and is greater than
or equal to zero.

In $M_2$, a priority relation exists that forces $ff \rightarrow af$ to
be given precedence over $f \rightarrow a\delta$.  As a result, whenever
it is possible to apply $ff \rightarrow af$ (i.e. there are two $f$
objects), it will be applied instead of $f \rightarrow a\delta$.  The
other two rules manipulate the $b'$ objects. First, they are all
converted in to $b$ objects.  This will always occur, as there are at
least two $f$ objects in $M_2$ to begin with, which means $ff
\rightarrow af$ will be applied rather than $f \rightarrow a\delta$
which destroys $M_2$.  Each time $ff \rightarrow af$ is applied, the
number of $f$ objects halves.

The remaining rule, $b \rightarrow b(c, in_4)$, will evolve once for
each occurrence of $ff$, of which there are $n$.  $M_2$ contains $n + 1$
$b$ objects, all converted from the $b'$ objects that were in $M_3$.  As
long as there is an even number of $f$ objects, the two rules $b
\rightarrow b(c, in_4)$ and $ff \rightarrow af$ will be applied, halving
the number of $f$ objects and creating $n + 1$ c objects in $M_4$ (via
$(c, in_4)$), while the number of $b$ objects remains the same.

When only one $f$ object is left, $f \rightarrow a\delta$ will be
applied, resulting in $M_2$ being destroyed and the following
configuration:
\begin{align*}
\mu & = [_1[_4]_4]_1 \\
M_1 & = \{a^{2n+1},b^{n+1}\}, 
R_1 = \emptyset,
\rho_1 = \emptyset \\
M_4 & = \{c^{(n+1)^2}\},
R_4 = \emptyset,
\rho_4 = \emptyset
\end{align*}

\noindent No further evolution is possible, as there are no more rules.
$c^{(n+1)^2}$ is the final output, as $M_4$ is the output membrane.

Further variants of P systems exist.  Tissue P systems use a graph-based
structure rather than the tree shown here, while population P systems
also incorporate an environment.  At present, the main flaw with
modelling concurrent systems using this formalism is that the underlying
theory is not as advanced as for those of process calculi,
such as the $\pi$ calculus.  So far, P systems research has focused on
their power of expressivity, and their application within the field of
biological modelling.  In the latter, they provide a much more natural
perspective than the channel-based operations of the $\pi$ calculus, and
this is something that will be considered further in section \ref{bioapps}.

\section{Applying Mobile Calculi to Biology}
\label{bioapps}

Biological systems are inherently concurrent, being focused on the
behaviour of multiple entities from low-level molecules, through
bacteria and other bodies, to full cellular structures and beyond.
Models which incorporate spatial distribution, such as the ambient
calculus (\ref{ambientcalculus}) and P systems (\ref{psystems}) are
especially useful for representing the structure of real-world
biological entities.

Such modelling is becoming common place within the
literature\cite{biospi, cardelli:bioambients, fran}, where concurrent
models represent an alternative to the use of ordinary differential
equations (ODEs).  The usual approach is to create a model of the
system within the formalism and then perform simulations.  Such
simulations rely on reducing the non-determinism within the model by
introducing a stochastic semantics.  In each of the biochemical
stochastic $\pi$ calculus \cite{biospi}, the BioAmbient variant
\cite{cardelli:bioambients} and P systems \cite{fran}, these are based
on Gillespie's algorithm \cite{gillespie}.

The algorithm selects which reaction occurs next and the necessary
advancement of the system's `clock' (a real time value in this context,
rather than some discrete notion).  A probability is associated with
each reaction, so that the algorithm basically runs as follows:

\begin{enumerate}
\item $a_0$ is calculated as the sum of the probabilities.
\item Two random numbers, $r_1$ and $r_2$, are generated from a uniform
      distribution over the unit interval 0 to 1.
\item Calculate the waiting time for the next reaction, $\tau_i =
      \frac{1}{a_0} \ln (\frac{1}{r_1})$
\item Take the index, $j$, of the reaction such that
      $\displaystyle\sum_{k=1}^{j-1} p_k < r_2 a_0 \le
      \displaystyle\sum_{k=1}^jp_k$ where $p_k$ is the $k$th probability.
\item Return the pair $(\tau_i, j)$
\end{enumerate}

\noindent determining which one occurs.  Slight alterations are made in
distributed models to handle the rules arising from different localities.
For example, the P systems model \cite{fran} adapts the algorithm to
form a multi-compartmental variant, which treats each membrane
separately, to a degree, while also taking into account that activity
in one membrane may affect others.

Clearly, different formalisms offer different approaches.  In the
original $\pi$ calculus approach of \cite{biospi}, the focus was solely
on communication with biological compartments abstracted as private
channels.  The model given for BioAmbients \cite{cardelli:bioambients}
is more natural due to the explicit realisation of these compartments.

Take the following example from \cite{cardelli:bioambients},

\begin{equation}
\begin{aligned}
\mathtt{System} & ::= \mathtt{molecule[Mol] \pc \dots \pc molecule[Mol] \pc
 cell[Porin]} \\
\mathtt{Mol} & ::= \mathtt{enter\ cell1.Mol + exit\ cell2 . Mol} \\
\mathtt{Porin} & ::= \mathtt{accept\ cell1.Porin + expel\ cell2.Porin}
\end{aligned}
\end{equation}

\noindent which demonstrates a membranal pore, which molecules use to
pass through a membrane.  Both the cell and the molecules are
represented by ambients.  Each molecule is controlled by a process,
$Mol$, which, at any time, has the option of performing either an
\texttt{enter} or an \texttt{exit}.  Similarly, the $Porin$ process,
which represents the membranal pore, may \texttt{accept} or
\texttt{expel}.

Within the BioAmbient calculus, movement is synchronous and takes place
by the pairing of an \texttt{enter} and \texttt{accept} action (the
equivalent of $in$) or an \texttt{exit} and \texttt{expel} action
(equivalent to $out$).  The first action in each case is used by the
moving process.  Both must also mention the same channel name
(\texttt{cell1} and \texttt{cell2} here).  In the case of the system
shown above, both \texttt{Mol} and \texttt{Porin} permanently offer
their halves of this pairing.  However, the spatial context makes one of
them inapplicable.  Initially, \texttt{exit} and \texttt{expel} won't
synchronize, as \texttt{Mol} is not inside the ambient from which it is
being expelled.  Likewise, once it has entered, it can't do so again,
even though the actions make this possible.

Models such as this seem a little unnatural as molecules are modelled as
both an ambient and a process.  This is because only ambients may move
but only processes can emit the necessary mobility primitives to do so.
The notions of mobility present in the ambient calculus, including this
idea, have been carried across, even though it doesn't directly adopt
the primitives of the ambient calculus; the style is still more akin to
the $\pi$ calculus.

In contrast, \cite{fran} takes a different approach using P systems,
representing signals and proteins directly as objects in the membranes.
One particular application of this technique is \emph{quorum sensing}.
This is a gene regulation system where a population of bacterial cells
communicate in order to regulate the expression of certain genes in a
co-ordinated way which is dependent on the size of the population.
\cite{fran} presents a model of this phenomenon in \emph{vibrio
fischeri}, a marine bacterium, using a P system\footnote{This method of
defining the configuration differs slightly from that in \ref{psystems},
as it also includes a set of labels, rather than assuming that the
natural numbers are used.}:
\begin{align*}
  \Pi_{vf} & = (O, \{e,b\}, \mu, (w_1, e), (w_2, b), \dots,
  (w_{n+1},b),\mathcal{R}_b, \mathcal{R}_e) \\
  O & = \{OHHL, LuxR, LuxR\text{-}OHHL, LuxBox, LuxBox\text{-}LuxR\text{-}OHHL\} \\
  w_1 & = \emptyset \\
  w_i & = \{LuxBox\}\ \text{where}\ 2 \le i \le n + 1 \\
\end{align*}

\noindent where each bacteria is represented as a membrane, $b$, within
an environment membrane, $e$.  The alphabet, $O$, contains the signal,
$OHHL$, the protein, $LuxR$ and the regulatory region, $LuxBox$, in
addition to the protein-signal complex ($LuxR-OHHL$) formed and its
regulatory region, $LuxBox-LuxR-OHHL$.  The initial configuration shown
above leaves the environment empty and places just the genome, $LuxBox$,
inside each bacteria membrane to start production of the signal and the
protein.  $\mathcal{R}_b$ and $\mathcal{R}_e$ contain the rules which
affect the bacteria and the environment respectively.  The reader is
referred to the full paper for full details of these.

This model seems much more natural and has a clearer correspondence with
the real-world representation.  The main issue, as noted earlier in
\ref{psystems}, is that the theory of P systems is not as well developed
as that of the $\pi$ calculus (upon which the BioAmbients calculus is
essentially based).  This can prove problematic, especially when model
checking such models.

\subsubsection{Bigraphs}
\label{bigraphs}

Bigraphs \cite{bigraph1, bigraph2} are an attempt at providing a
unifying framework, able to represent both spatial relationships
(\emph{locality}) in the style of the ambient calculus (see
\ref{ambientcalculus}) and link-based-relationships
(\emph{connectivity}) seen in the $\pi$ calculus (see \ref{picalculus}).
Their particular application area is within pervasive computing, where a
mixture of both concepts is needed to represent both movement through
space and the change in relationships between agents.

The nodes in a bigraph support a dual structure, hence the name.  On one
level, there are nodes nested within nodes, representing locality.  This
is called the \emph{place graph}.  These nodes have \emph{ports} which
are connected via links to form a \emph{link graph}.  Each node has a
\emph{control} with an arity that defines the number of ports.  The two
graphs share nodes, but are otherwise independent.  Nesting can only
occur in nodes with a \emph{non-atomic} control.  These can also be
\emph{active} or \emph{passive}.  The former allows reactions to occur
within the node.  \emph{Holes} may occur in bigraphs, where other
bigraphs can appear.

Within this model, it is possible to encode both the $\pi$ calculus and
the ambient calculus.  Take the following rule from the asynchronous
$\pi$ calculus without summation,

\begin{equation}
\overline{x}y \pc x(z).P \rightarrow P\{y/z\}
\end{equation}

\noindent which represents synchronisation.  In \cite{bigraph1}, Milner
encodes this as a bigraph with two controls, $send$ and $get$, both of
which have an arity of two.  To represent the fact that the output
prefix has no continuation in the asynchronous $\pi$ calculus, $send$ is
declared atomic.  $get$ is non-atomic but inactive.

The node $get$ includes a nested hole with the port $z$.  This
represents the continuation $P$, with $z$ being the name bound on
input.  The port $z$ is linked from the hole to $get$ itself.  $send$
has two ports: $x$, which is also connected to $get$, and $y$.  With
these concepts in place, the reaction may be represented as:

\begin{equation}
send_{xy} \pc get_{x(z)} \square \rightarrow x \pc y/(z) \square
\end{equation}

\noindent the $send$ node disappearing afterwards, leaving $y$ connected
to $z$ and $x$ unused.

Similarly, \cite{bigraph1} shows how the $in$ capability from the
ambient calculus:

\begin{equation}
 n[\ambin{m}.P_1 \pc P_2] \pc m[Q]
  \rightarrow
  m[n[P_1 \pc P_2] \pc Q] \\
\end{equation}

\noindent may be encoded using two controls, $amb$ and $in$, both with
an arity of one.  The two ambients involved are represented by
instances of $amb$, while $in$ is an atomic control representing the
process that emits the capability.  The $amb$ control is non-atomic
and active, each ambient containing a hole which represents their
continued behaviour,

The ambient names are represented as the node's single port.  In the
case of the ambient named in the capability, this is also linked to the
$in$ instance.  To model the reaction above, $n$ is connected to the
port of one $amb$, while $m$ is connected to both the other $amb$ and
$in$.  The reaction is then encoded as:

\begin{equation}
amb_n (in_m \pc \square_0) \pc amb_m \square_1
\rightarrow
amb_m (amb_n \square_0 \pc \square_1)
\end{equation}

\noindent where the similarities between the two are clear.

Bigraphs provide an interesting framework for unifying the two disparate
concepts outlined above in \ref{scopemobility} and \ref{migration}.  It
will be exciting to see how this theory develops, and whether it can
also be used to encode the discrete time notions described in \ref{timing}.

\section{Conclusion}

The $\pi$ calculus seems to provide the best of both worlds, being able
to model concurrent systems and still retain the expressiveness of the
$\lambda$ calculus.  However, a key limitation was identified which
reinforced the idea that expressivity only makes a model capable, and
not suitable, for simulating any recursive function: modelling global
synchronisation via a broadcasting agent.  This limitation seems to hold
for both CCS and the $\pi$ calculus, and it is also likely that it
applies to many other process calculi, such as the ambient calculus, a
formalism that provides a more natural form of mobility via structural
changes.

Biology was also considered briefly (see \ref{bioapps}), as a
potential application area.  P systems seem the most natural
formalism, but they lack some of the proven theoretical aspects of
process calculi.
